{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4643151f-8b64-40cb-9b36-758e369e6407",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20bc9df7-5328-4945-a1bd-30f823fc8b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super_admin/hasindu/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import wandb\n",
    "import evaluate  # Hugging Face's evaluate library\n",
    "import numpy as np\n",
    "import torch\n",
    "from bert_score import BERTScorer, score as bert_score\n",
    "\n",
    "from utils import tokenize_dataset_for_domain_bound_qna\n",
    "# from prompt_templates import domain_bound_qna_prompt_template as prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458060f-cfcf-4f51-a6aa-506cef1dc7c1",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb057781-55b7-494f-bcb4-dd7d00ba4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/phi_domain_bound_qna_finetuned_attempt_6/final\"\n",
    "\n",
    "data_path = \"../data/domain_bound_data/v4/\"\n",
    "test_data_path = data_path + \"test.csv\"\n",
    "\n",
    "model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "base_model_path = \"../models/phi_qna_finetuned_attempt_3/final_pretrained_2\"\n",
    "\n",
    "max_len = 512\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38a3746-3c13-42cc-920a-2614eeede918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhasindumadushan325\u001b[0m (\u001b[33mhasindumadushan325-university-of-peradeniya\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/super_admin/hasindu/src/wandb/run-20250517_142314-tfdqmz5e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/domain_bound_qna_finetune-evaluation/runs/tfdqmz5e' target=\"_blank\">attempt_6</a></strong> to <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/domain_bound_qna_finetune-evaluation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/domain_bound_qna_finetune-evaluation' target=\"_blank\">https://wandb.ai/hasindumadushan325-university-of-peradeniya/domain_bound_qna_finetune-evaluation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/domain_bound_qna_finetune-evaluation/runs/tfdqmz5e' target=\"_blank\">https://wandb.ai/hasindumadushan325-university-of-peradeniya/domain_bound_qna_finetune-evaluation/runs/tfdqmz5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hasindumadushan325-university-of-peradeniya/domain_bound_qna_finetune-evaluation/runs/tfdqmz5e?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f6f3573caf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"domain_bound_qna_finetune-evaluation\", name=\"attempt_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f573ca-73a8-4ef6-af46-ecd2d84e6bad",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15035ec-6729-4cf4-a946-1b1f2a700e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Instruction:\n",
      "Assume you are an excellent doctor. Using your knowledge, answer the question given below.\n",
      "\n",
      "# Question: {question}\n",
      "\n",
      "# Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "# Instruction:\n",
    "Assume you are an excellent doctor. Using your knowledge, answer the question given below.\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Answer: \"\"\"\n",
    "prompt_template = prompt_template.strip()\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8443f97-858d-4220-b3d2-8f728fa48020",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac602a77-0a3a-4d89-915a-b102625ebce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1034.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_data_path)\n",
    "test_set = tokenize_dataset_for_domain_bound_qna(tokenizer, test_df, prompt_template, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a851ebd-aff1-4e44-ad09-93151119bcb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60d571-b664-464b-a3b5-7ba96d0e8e22",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4bbed1-cead-4893-980c-ab4269030a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../models/phi_qna_finetuned_attempt_3/final_pretrained_2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65cb96b6-5bd7-44d1-bd84-c6a107afddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# === Quantized model loading ===\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6afd719f-5e81-4335-82ad-c4346f9d2b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../models/phi_domain_bound_qna_finetuned_attempt_6/final'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0b63bf7-fe1b-401f-b8c1-a394a65ed24d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3Attention(\n",
       "              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "              (qkv_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "              (activation_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): Phi3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(model, model_path)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0fd68d-392c-49c4-bc85-81c05bb4a07b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ce6b4-018f-422f-a85e-285f9038dbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b39ed-ea6c-426c-a76c-63ac9283a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded167b3-4fc9-439c-8c3c-238d193c5a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a3294-c17a-403d-9fe7-626608f6f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./eval_output_base\",\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     do_eval=True,\n",
    "#     report_to=\"none\"\n",
    "# )\n",
    "\n",
    "# base_model_trainer = Trainer(\n",
    "#     model=base_model,\n",
    "#     args=training_args,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "# )\n",
    "\n",
    "# base_model_eval_result = base_model_trainer.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6480b4-2485-422f-a289-898f0ba4dbe9",
   "metadata": {},
   "source": [
    "# Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f852223-0fa4-49c8-9f81-e600cef32e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./eval_output\",\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",\n",
    "    eval_accumulation_steps=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d93a44e-cf37-4dc0-8171-3bde13eaca09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86415/875759355.py:97: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize metrics ONCE (reuse them)\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bert_scorer = BERTScorer(\n",
    "    lang=\"en\",\n",
    "    model_type=\"bert-base-uncased\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    idf=False,  # Disable IDF to save memory\n",
    "    rescale_with_baseline=True  # Better score normalization\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\" Metric computation \"\"\"\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_preds\n",
    "        \n",
    "        # Convert to numpy (move to CPU first if needed)\n",
    "        if torch.is_tensor(logits):\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "        if torch.is_tensor(labels):\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "        \n",
    "        # Get predicted tokens (shape: [batch_size, seq_length])\n",
    "        pred_ids = np.argmax(logits, axis=-1)\n",
    "        \n",
    "        # Decode in batches to avoid memory spikes\n",
    "        batch_size = 8  # Adjust based on your GPU memory\n",
    "        pred_str, label_str = [], []\n",
    "        \n",
    "        for i in range(0, len(pred_ids), batch_size):\n",
    "            # Decode predictions\n",
    "            batch_preds = pred_ids[i:i+batch_size]\n",
    "            pred_str.extend(tokenizer.batch_decode(\n",
    "                batch_preds, \n",
    "                skip_special_tokens=True\n",
    "            ))\n",
    "            \n",
    "            # Decode labels (replace -100 with pad_token_id)\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "            batch_labels = np.where(\n",
    "                batch_labels != -100, \n",
    "                batch_labels, \n",
    "                tokenizer.pad_token_id\n",
    "            )\n",
    "            label_str.extend(tokenizer.batch_decode(\n",
    "                batch_labels, \n",
    "                skip_special_tokens=True\n",
    "            ))\n",
    "        \n",
    "        # Skip if empty (avoid errors)\n",
    "        if not pred_str or not label_str:\n",
    "            return {\n",
    "                'bleu': 0.0,\n",
    "                'rouge1': 0.0,\n",
    "                'rouge2': 0.0,\n",
    "                'rougeL': 0.0,\n",
    "                'bertscore_f1': 0.0\n",
    "            }\n",
    "        \n",
    "        # Compute BLEU (handle edge cases)\n",
    "        try:\n",
    "            bleu_score = bleu_metric.compute(\n",
    "                predictions=pred_str,\n",
    "                references=[[ref] for ref in label_str]\n",
    "            )['bleu']\n",
    "        except:\n",
    "            bleu_score = 0.0\n",
    "        \n",
    "        # Compute ROUGE\n",
    "        rouge_scores = rouge_metric.compute(\n",
    "            predictions=pred_str,\n",
    "            references=label_str,\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        \n",
    "        # Compute BERTScore in batches\n",
    "        P, R, F1 = bert_scorer.score(\n",
    "            pred_str, \n",
    "            label_str,\n",
    "            batch_size=4  # Small batch for BERTScore\n",
    "        )\n",
    "        \n",
    "        metrics = {\n",
    "            'bleu': bleu_score,\n",
    "            'rouge1': rouge_scores['rouge1'],\n",
    "            'rouge2': rouge_scores['rouge2'],\n",
    "            'rougeL': rouge_scores['rougeL'],\n",
    "            'bertscore_precision': P.mean().item(),\n",
    "            'bertscore_recall': R.mean().item(),\n",
    "            'bertscore_f1': F1.mean().item(),\n",
    "        }\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        return metrics\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer, padding=False),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9d3989f-7cbb-47aa-8197-c2dab6692fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 08:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Evaluate perplexity ===\n",
    "eval_result = trainer.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ddcdbf5-5d13-49ac-b3a3-86e2986139ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "Loss: 1.6728\n",
      "Perplexity: 5.33\n",
      "BLEU: 0.0476\n",
      "ROUGE-1: 0.2373\n",
      "ROUGE-2: 0.1100\n",
      "ROUGE-L: 0.1902\n",
      "BERTscore precision: -0.0313\n",
      "BERTscore recall: 0.3024\n",
      "BERTscore f1: 0.0862\n"
     ]
    }
   ],
   "source": [
    "# Print all results\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Loss: {eval_result['eval_loss']:.4f}\")\n",
    "print(f\"Perplexity: {torch.exp(torch.tensor(eval_result['eval_loss'])):.2f}\")\n",
    "print(f\"BLEU: {eval_result['eval_bleu']:.4f}\")\n",
    "print(f\"ROUGE-1: {eval_result['eval_rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {eval_result['eval_rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {eval_result['eval_rougeL']:.4f}\")\n",
    "print(f\"BERTscore precision: {eval_result['eval_bertscore_precision']:.4f}\")\n",
    "print(f\"BERTscore recall: {eval_result['eval_bertscore_recall']:.4f}\")\n",
    "print(f\"BERTscore f1: {eval_result['eval_bertscore_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92be58f3-423a-42b3-9bc4-ae8a46dc9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\n",
    "    \"eval_loss\": eval_result['eval_loss'], \n",
    "    \"perplexity\": torch.exp(torch.tensor(eval_result['eval_loss'])),\n",
    "    \"BLUE\": eval_result['eval_bleu'],\n",
    "    \"ROUGE_1\": eval_result['eval_rouge1'],\n",
    "    \"ROUGE_2\": eval_result['eval_rouge2'],\n",
    "    \"ROUGE_L\": eval_result['eval_rougeL'],\n",
    "    \"BERTscore_precision\": eval_result['eval_bertscore_precision'],\n",
    "    \"BERTscore recall\": eval_result['eval_bertscore_recall'],\n",
    "    \"BERTscore f1\": eval_result['eval_bertscore_f1']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b6324-721a-40fd-94e8-260ae1f37420",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74594108-635c-4a63-ac8f-074d13a890cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = test_set.select(range(433, 439))  # First 5 examples\n",
    "input_ids = torch.tensor(samples[\"input_ids\"]).to(model.device)\n",
    "attention_mask = torch.tensor(samples[\"attention_mask\"]).to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "# Log predictions to W&B\n",
    "wandb_table = wandb.Table(columns=[\"Title\", \"Actual Abstract\", \"Generated Text\"])\n",
    "for i, gen in enumerate(generated_texts):\n",
    "    title = samples[i][\"title\"]\n",
    "    actual = samples[i][\"abstract\"]\n",
    "    print(f\"\\nActual: {title}\\n{actual}\\n---\\nGenerated: {gen}\\n\")\n",
    "    wandb_table.add_data(title, actual, gen)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f60d22-2210-4463-bee0-814c84f988dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"generated_examples\": wandb_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14185bf3-943b-4a05-b9e0-649841a4723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, text, max_new_tokens=128):\n",
    "    sample = tokenizer(text + tokenizer.eos_token, truncation=True, padding=\"max_length\", max_length=max_len, return_attention_mask=True)\n",
    "    input_ids = torch.tensor([sample[\"input_ids\"]]).to(model.device)\n",
    "    attention_mask = torch.tensor([sample[\"attention_mask\"]]).to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "        use_cache=False\n",
    "    )\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return generated_texts[0]\n",
    "    # # Log predictions to W&B\n",
    "    # for i, gen in enumerate(generated_texts):\n",
    "    #     title = samples[i][\"title\"]\n",
    "    #     actual = samples[i][\"abstract\"]\n",
    "    #     print(f\"\\nTitle: {title}\\n---\\nActual Abstract: {actual}\\n---\\nGenerated: {gen}\\n\")\n",
    "    #     wandb_table.add_data(title, actual, gen)\n",
    "    \n",
    "    \n",
    "    # wandb.log({\"generated_examples\": wandb_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47b38672-f3a9-481e-a6e3-b5fdb059ff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_generate(model, tokenizer, text, max_new_tokens=300):\n",
    "    model.eval()\n",
    "    sample = tokenizer(\n",
    "        text + tokenizer.eos_token,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    ).to(model.device)\n",
    "\n",
    "    input_ids = sample[\"input_ids\"]\n",
    "    generated = input_ids.clone()\n",
    "    past_key_values = None\n",
    "    position_ids = torch.arange(0, input_ids.shape[1], device=model.device).unsqueeze(0)\n",
    "\n",
    "    prev_decoded = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "    for i in range(max_new_tokens):\n",
    "        if i == 0:\n",
    "            input_token = input_ids\n",
    "        else:\n",
    "            input_token = next_token_id\n",
    "            position_ids = torch.tensor([[generated.shape[1] - 1]], device=model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_token,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "                position_ids=position_ids\n",
    "            )\n",
    "\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        next_token_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        generated = torch.cat((generated, next_token_id), dim=1)\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # Decode full sequence and compute the diff\n",
    "        decoded = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        new_text = decoded[len(prev_decoded):]\n",
    "        prev_decoded = decoded\n",
    "\n",
    "        yield new_text\n",
    "\n",
    "        if next_token_id.squeeze().item() == tokenizer.eos_token_id:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "760d9d2e-4b96-438a-83d9-3f0a5cfcf894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering. \"\"\"\n",
    "    logits = logits.clone()\n",
    "\n",
    "    # Top-K filtering\n",
    "    if top_k > 0:\n",
    "        top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    # Top-P (nucleus) filtering\n",
    "    if top_p > 0.0 and top_p < 1.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above top_p\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the mask to keep at least one token\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[..., indices_to_remove] = filter_value\n",
    "\n",
    "    return logits\n",
    "\n",
    "def stream_generate(model, tokenizer, text, max_new_tokens=300, temperature=1.0, top_k=0, top_p=0.0, max_len=512):\n",
    "    model.eval()\n",
    "    sample = tokenizer(\n",
    "        text + tokenizer.eos_token,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    ).to(model.device)\n",
    "\n",
    "    input_ids = sample[\"input_ids\"]\n",
    "    generated = input_ids.clone()\n",
    "    past_key_values = None\n",
    "    position_ids = torch.arange(0, input_ids.shape[1], device=model.device).unsqueeze(0)\n",
    "\n",
    "    prev_decoded = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "    for i in range(max_new_tokens):\n",
    "        if i == 0:\n",
    "            input_token = input_ids\n",
    "        else:\n",
    "            input_token = next_token_id\n",
    "            position_ids = torch.tensor([[generated.shape[1] - 1]], device=model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_token,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "                position_ids=position_ids\n",
    "            )\n",
    "\n",
    "        logits = outputs.logits[:, -1, :] / temperature\n",
    "        logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "        generated = torch.cat((generated, next_token_id), dim=1)\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # Decode full sequence and compute the diff\n",
    "        decoded = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        new_text = decoded[len(prev_decoded):]\n",
    "        prev_decoded = decoded\n",
    "\n",
    "        yield new_text\n",
    "\n",
    "        if next_token_id.squeeze().item() == tokenizer.eos_token_id:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a803fb8f-23cb-4456-bc26-3e586d8b28fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "# Instruction:\n",
    "Assume you are an excellent doctor. Using your knowledge, answer the question given below.\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Answer: \"\"\"\n",
    "prompt_template = prompt_template.strip()\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51c50739-9375-48b4-b882-d90e32d26f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"What is Glaucoma ?\",\n",
    "    \"What are the symptoms of Glaucoma ??\",\n",
    "    \"My sister is on Xanax, feyntnol patch and a pain medicine for cancer.  She has been on 25 of fentynol and within 6 days she has been bumped up to 100 now she is almost lethargic and breathing is really labored and right arm is twitching.. She was carrying on conversation Sunday and Monday patch was put on Tuesday and now cant even sit up..no one seems worried but me.. Just wondering what I could do\",\n",
    "    \"I was playing basketball the other night and went up to block a shot and flipped over the guy and landed on my side/back. Since then the lower left side of back/side have been sore, hurts when I take deep breaths and when I lay on my back, any chance of a bruised kidney or any serious injury I could have?\",\n",
    "    \"What are the treatments for High Blood Pressure ?\",\n",
    "    \"What is (are) Urinary Tract Infections ?\",\n",
    "    \"Create a C++ function that computes the Fast Fourier Transform (FFT) of a signal\",\n",
    "    \"When did Beyonce start becoming popular?\",\n",
    "    \"What are the symptoms of diabetes?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38c2f42d-1e94-44f8-aaf1-a09d08ce088c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'microsoft/Phi-3.5-mini-instruct'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fb30c34-0b0e-48e3-865d-d8d06af7b5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:24<00:00, 12.14s/it]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    "    cache_dir=\"../model_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c318105-cdd1-4c96-b36b-ee00642da94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0098416e-e9f9-4b19-867c-362d87938d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Instruction:\\nAssume you are an excellent doctor. Using your knowledge, answer the question given below.\\n\\n# Question: What are the treatments for High Blood Pressure ?\\n\\n# Answer:'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.format(question=examples[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f386f3f-2af8-4b44-a35b-903df2ffe216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The treatment of hypertension depends on the underlying cause. Treatment may include lifestyle changes, medications, or surgery."
     ]
    }
   ],
   "source": [
    "for token in stream_generate(model, tokenizer, prompt_template.format(question=examples[4])):\n",
    "    print(token, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e5e997f-875b-4518-b4d0-5da482dd35e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <reponame>ChatDoctor/ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-ChatDoctor-"
     ]
    }
   ],
   "source": [
    "for token in stream_generate(model, tokenizer, prompt_template.format(question=examples[-3]), temperature=0.2, top_k=50, top_p=0.9):\n",
    "    print(token, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adae0bb-3035-4e52-8bdd-fc4b0eecaa21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9063faa6-b569-4dfc-ba82-27f879bd23a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Glaucoma ? \n",
      "  <reponame>ChatGPT/OpenAI/ChatGPT-India-En-IN-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1\n",
      "What are the symptoms of Glaucoma ?? \n",
      "  The following are symptoms of glaucoma:\n",
      "\n",
      "- Loss of peripheral vision\n",
      "- Blurred vision\n",
      "- Halos around lights\n",
      "- Eye pain\n",
      "- Nausea and vomiting\n",
      "- Redness of the eye\n",
      "\n",
      "If you have any of these symptoms, see your eye doctor right away.\n",
      "My sister is on Xanax, feyntnol patch and a pain medicine for cancer.  She has been on 25 of fentynol and within 6 days she has been bumped up to 100 now she is almost lethargic and breathing is really labored and right arm is twitching.. She was carrying on conversation Sunday and Monday patch was put on Tuesday and now cant even sit up..no one seems worried but me.. Just wondering what I could do \n",
      "  The symptoms you describe are consistent with a possible stroke. I would recommend that you take her to the emergency room immediately.\n",
      "I was playing basketball the other night and went up to block a shot and flipped over the guy and landed on my side/back. Since then the lower left side of back/side have been sore, hurts when I take deep breaths and when I lay on my back, any chance of a bruised kidney or any serious injury I could have? \n",
      "  The pain you are experiencing is most likely due to a muscle strain. I would recommend taking ibuprofen or acetaminophen for the pain. If the pain persists, you may want to consider seeing a doctor.\n",
      "What are the treatments for High Blood Pressure ? \n",
      "  The treatment of hypertension depends on the underlying cause. Treatment may include lifestyle changes, medications, or surgery.\n",
      "What is (are) Urinary Tract Infections ? \n",
      "  <reponame>Matthew-Hughes/Matthew-Hughes.com\n",
      "Matthew Hughes is a professional musician and composer. He has written music for a number of films, television shows, and video games. He has also written music for a number of commercials. He has also written music for a number of commercials. He has also written music for a number of commercials. He has also written music for a number of commercials. He has also written music for a number of commercials. He has also written music for a number of commercials. He has also written music for a number of commercials. He has also written music for a number of commercials. He has also written music for a number of commercials. He has also written music for a number of commercials. He has also written music for a number of commercials. He has also written music for a number of commercials. He has also written music for a number of commercials. He has also written music for a number of commercials. He has also written music for a number of commercials. He has also written music for a number of commercials. He has also written music for a number of commercials. He has also written music for a number of commercials. He has also written music for a number of commercials.\n",
      "Create a C++ function that computes the Fast Fourier Transform (FFT) of a signal \n",
      "  <reponame>ChatGPT/OpenAI/ChatGPT-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-Inc-Bot-India-\n",
      "When did Beyonce start becoming popular? \n",
      "  The model is not designed to answer questions about health.\n"
     ]
    }
   ],
   "source": [
    "wandb_table = wandb.Table(columns=[\"Question\", \"Generated answer\"])\n",
    "\n",
    "for example in examples:\n",
    "    generated_answer = \"\"\n",
    "    for token in stream_generate(model, tokenizer, prompt_template.format(question=example)):\n",
    "        generated_answer += token\n",
    "    wandb_table.add_data(example, generated_answer)\n",
    "    print(example, \"\\n\", generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c63b455a-4561-4c4a-aa26-95a3f8d8c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"generated_examples\": wandb_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e169dd6-f8b8-4d5e-927e-56b9c3fefa9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a172452-8905-4dc8-9175-30af9e5162b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "30be9b16-e523-40fb-a7b3-4449bf87af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/pubmed_baseline/pubmed_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ae677cea-200d-4613-b51a-4e36df4579a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2712356</td>\n",
       "      <td>Membrane relationships in murine Meissner corp...</td>\n",
       "      <td>Mechanoreceptive sensory corpuscles (murine Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8979397</td>\n",
       "      <td>Two-dimensional protein patterns of Arabidopsi...</td>\n",
       "      <td>In order to detect gene products involved in A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1462207</td>\n",
       "      <td>Pathoanatomy of lumbar disc herniation as demo...</td>\n",
       "      <td>Computed tomography/discography was performed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1807731</td>\n",
       "      <td>An innovative method of teaching Advanced Card...</td>\n",
       "      <td>A demonstration and discussion of the effectiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3207648</td>\n",
       "      <td>The distribution of CA 125 in the reproductive...</td>\n",
       "      <td>Investigation of serum and tissue homogenates ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pmid                                              title  \\\n",
       "0  2712356  Membrane relationships in murine Meissner corp...   \n",
       "1  8979397  Two-dimensional protein patterns of Arabidopsi...   \n",
       "2  1462207  Pathoanatomy of lumbar disc herniation as demo...   \n",
       "3  1807731  An innovative method of teaching Advanced Card...   \n",
       "4  3207648  The distribution of CA 125 in the reproductive...   \n",
       "\n",
       "                                            abstract  \n",
       "0  Mechanoreceptive sensory corpuscles (murine Me...  \n",
       "1  In order to detect gene products involved in A...  \n",
       "2  Computed tomography/discography was performed ...  \n",
       "3  A demonstration and discussion of the effectiv...  \n",
       "4  Investigation of serum and tissue homogenates ...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "12e6e86c-15d2-4133-95b5-5497cb054dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         None\n",
       "1         None\n",
       "2         None\n",
       "3         None\n",
       "4         None\n",
       "          ... \n",
       "999995    None\n",
       "999996    None\n",
       "999997    None\n",
       "999998    None\n",
       "999999    None\n",
       "Name: abstract, Length: 1000000, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    if not isinstance(x, str):\n",
    "        print(x)\n",
    "        return\n",
    "    if \"\" in x: \n",
    "        print(x)\n",
    "train_df[\"abstract\"].apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d918f711-af37-4177-83c6-251184875be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_layer',\n",
       " 'default',\n",
       " 'down_proj',\n",
       " 'gate_up_proj',\n",
       " 'lora_A',\n",
       " 'lora_B',\n",
       " 'lora_dropout',\n",
       " 'lora_embedding_A',\n",
       " 'lora_embedding_B',\n",
       " 'lora_magnitude_vector',\n",
       " 'o_proj',\n",
       " 'qkv_proj',\n",
       " 'resid_attn_dropout',\n",
       " 'self_attn'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules = set()\n",
    "for name, module in model.named_modules():\n",
    "    if \"attn\" in name or \"proj\" in name:\n",
    "        modules.add(name.split(\".\")[-1])\n",
    "modules"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
