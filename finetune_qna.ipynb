{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026f8cee-8ece-469f-90ab-a3252e726ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flash-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a40d16-2fc5-47a7-8023-633fb24547f0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31bb4a93-2d17-4994-a440-23107a47e0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super_admin/hasindu/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict, load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import PeftModel, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from utils import tokenize_dataset_for_qna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a1366-410f-4d33-9226-b1c6cebdacd6",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b247f35-dfcd-4b94-bd81-95bc57d829e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/qna/\"\n",
    "train_data_path = data_path + \"train.csv\"\n",
    "val_data_path = data_path + \"val.csv\"\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "base_model_path = \"../models/phi_pubmed_pretrained_attempt_3/final_pretrained\"\n",
    "\n",
    "model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "model_output_dir = \"../models/phi_qna_finetuned_attempt_4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eceb44-4e47-418a-b767-c443edf822ff",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "082ee172-0d21-43db-96f7-9ba72c4baf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r = 32\n",
    "lora_alpha = 64\n",
    "lora_target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "batch_size = 32\n",
    "quantization = None\n",
    "lora_dropout = 0.05\n",
    "epochs = 5\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e8dde-e44f-4718-8e99-f5fcb9b66cd0",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e2fda0-49ac-41b0-a2a8-03fbba8db2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Instruction:\n",
      "Assume you are an excellent doctor. Using your knowledge, answer the question given below.\n",
      "\n",
      "# Question: {question}\n",
      "\n",
      "# Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "# Instruction:\n",
    "Assume you are an excellent doctor. Using your knowledge, answer the question given below.\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Answer: \"\"\"\n",
    "prompt_template = prompt_template.strip()\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4358e35f-0f55-4458-ba55-014c149fddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6adcda-05e5-4c38-a565-2cab23a9493a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac21737f-73e8-48e3-95b1-faf83a5f074a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:00<00:00, 784.34 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 110000/110000 [02:17<00:00, 801.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_csv(val_data_path)\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "\n",
    "val_set = tokenize_dataset_for_qna(tokenizer, val_df, prompt_template, max_len)\n",
    "train_set = tokenize_dataset_for_qna(tokenizer, train_df, prompt_template, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a201991-e1ec-4695-b68c-4504b6fb3d1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'I have had a cyst now for 6 months.6weeks ago it became bigger and I smashed it alittle and alot of thick clear stcky stuff came out of it.Now theres a huge hole...like a sack.it will not heal or fill in and really needs stitches but if you have it fixed...the cyst will come back.Its not painful in the least.(its in between my vag. and anus )Its just a hole.my dad told me when he was younger this same thing happen to him 4 times until he had the sack removed.What is this???I was very scared of cancer but my dad said it will go away when the sack is removed.Im 48 yrs old.Its on my scar where they cut me so I could have my children.thank you Tina Leatherwood',\n",
       " 'answer': 'Welcome to Chat Doctor It needs to be examined to know whether it is just and abscess or a fistula.  In case of fistula it needs to be Chat Doctor.  After that pain medication to reduce the pain and antibiotic to prevent and check the infection. In case assess if it is small there is no need of stitches, the pain is usually well managed by painkillers, you need not worry about the pain. In case of fistula surgery can be performed at the same time as abscess surgery. However, fistulas often develop four to six weeks after an abscess is',\n",
       " 'input_ids': [32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  396,\n",
       "  2799,\n",
       "  4080,\n",
       "  29901,\n",
       "  13,\n",
       "  7900,\n",
       "  2017,\n",
       "  366,\n",
       "  526,\n",
       "  385,\n",
       "  15129,\n",
       "  11619,\n",
       "  29889,\n",
       "  5293,\n",
       "  596,\n",
       "  7134,\n",
       "  29892,\n",
       "  1234,\n",
       "  278,\n",
       "  1139,\n",
       "  2183,\n",
       "  2400,\n",
       "  29889,\n",
       "  13,\n",
       "  13,\n",
       "  29937,\n",
       "  894,\n",
       "  29901,\n",
       "  306,\n",
       "  505,\n",
       "  750,\n",
       "  263,\n",
       "  274,\n",
       "  858,\n",
       "  1286,\n",
       "  363,\n",
       "  29871,\n",
       "  29953,\n",
       "  7378,\n",
       "  29889,\n",
       "  29953,\n",
       "  705,\n",
       "  14541,\n",
       "  8020,\n",
       "  372,\n",
       "  3897,\n",
       "  16600,\n",
       "  322,\n",
       "  306,\n",
       "  1560,\n",
       "  25936,\n",
       "  372,\n",
       "  394,\n",
       "  1992,\n",
       "  322,\n",
       "  394,\n",
       "  327,\n",
       "  310,\n",
       "  12003,\n",
       "  2821,\n",
       "  380,\n",
       "  384,\n",
       "  29891,\n",
       "  6433,\n",
       "  2996,\n",
       "  714,\n",
       "  310,\n",
       "  372,\n",
       "  29889,\n",
       "  10454,\n",
       "  266,\n",
       "  11175,\n",
       "  263,\n",
       "  12176,\n",
       "  16188,\n",
       "  856,\n",
       "  4561,\n",
       "  263,\n",
       "  269,\n",
       "  547,\n",
       "  29889,\n",
       "  277,\n",
       "  674,\n",
       "  451,\n",
       "  540,\n",
       "  284,\n",
       "  470,\n",
       "  5445,\n",
       "  297,\n",
       "  322,\n",
       "  2289,\n",
       "  4225,\n",
       "  380,\n",
       "  2335,\n",
       "  267,\n",
       "  541,\n",
       "  565,\n",
       "  366,\n",
       "  505,\n",
       "  372,\n",
       "  4343,\n",
       "  856,\n",
       "  1552,\n",
       "  274,\n",
       "  858,\n",
       "  674,\n",
       "  2041,\n",
       "  1250,\n",
       "  29889,\n",
       "  29902,\n",
       "  1372,\n",
       "  451,\n",
       "  6788,\n",
       "  1319,\n",
       "  297,\n",
       "  278,\n",
       "  3203,\n",
       "  14030,\n",
       "  1169,\n",
       "  297,\n",
       "  1546,\n",
       "  590,\n",
       "  325,\n",
       "  351,\n",
       "  29889,\n",
       "  322,\n",
       "  385,\n",
       "  375,\n",
       "  1723,\n",
       "  29902,\n",
       "  1372,\n",
       "  925,\n",
       "  263,\n",
       "  16188,\n",
       "  29889,\n",
       "  1357,\n",
       "  270,\n",
       "  328,\n",
       "  5429,\n",
       "  592,\n",
       "  746,\n",
       "  540,\n",
       "  471,\n",
       "  20023,\n",
       "  445,\n",
       "  1021,\n",
       "  2655,\n",
       "  3799,\n",
       "  304,\n",
       "  1075,\n",
       "  29871,\n",
       "  29946,\n",
       "  3064,\n",
       "  2745,\n",
       "  540,\n",
       "  750,\n",
       "  278,\n",
       "  269,\n",
       "  547,\n",
       "  6206,\n",
       "  29889,\n",
       "  5618,\n",
       "  338,\n",
       "  445,\n",
       "  28772,\n",
       "  29902,\n",
       "  471,\n",
       "  1407,\n",
       "  885,\n",
       "  1965,\n",
       "  310,\n",
       "  23900,\n",
       "  541,\n",
       "  590,\n",
       "  270,\n",
       "  328,\n",
       "  1497,\n",
       "  372,\n",
       "  674,\n",
       "  748,\n",
       "  3448,\n",
       "  746,\n",
       "  278,\n",
       "  269,\n",
       "  547,\n",
       "  338,\n",
       "  6206,\n",
       "  29889,\n",
       "  1888,\n",
       "  29871,\n",
       "  29946,\n",
       "  29947,\n",
       "  343,\n",
       "  2288,\n",
       "  2030,\n",
       "  29889,\n",
       "  29902,\n",
       "  1372,\n",
       "  373,\n",
       "  590,\n",
       "  21990,\n",
       "  988,\n",
       "  896,\n",
       "  5700,\n",
       "  592,\n",
       "  577,\n",
       "  306,\n",
       "  1033,\n",
       "  505,\n",
       "  590,\n",
       "  4344,\n",
       "  29889,\n",
       "  386,\n",
       "  804,\n",
       "  366,\n",
       "  323,\n",
       "  1099,\n",
       "  951,\n",
       "  1624,\n",
       "  6115,\n",
       "  13,\n",
       "  13,\n",
       "  29937,\n",
       "  673,\n",
       "  29901,\n",
       "  28862,\n",
       "  2763,\n",
       "  304,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  739,\n",
       "  4225,\n",
       "  304,\n",
       "  367,\n",
       "  4392,\n",
       "  1312,\n",
       "  304,\n",
       "  1073,\n",
       "  3692,\n",
       "  372,\n",
       "  338,\n",
       "  925,\n",
       "  322,\n",
       "  6425,\n",
       "  985,\n",
       "  470,\n",
       "  263,\n",
       "  285,\n",
       "  391,\n",
       "  2497,\n",
       "  29889,\n",
       "  29871,\n",
       "  512,\n",
       "  1206,\n",
       "  310,\n",
       "  285,\n",
       "  391,\n",
       "  2497,\n",
       "  372,\n",
       "  4225,\n",
       "  304,\n",
       "  367,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  29889,\n",
       "  29871,\n",
       "  2860,\n",
       "  393,\n",
       "  6788,\n",
       "  13589,\n",
       "  362,\n",
       "  304,\n",
       "  10032,\n",
       "  278,\n",
       "  6788,\n",
       "  322,\n",
       "  3677,\n",
       "  747,\n",
       "  29875,\n",
       "  13574,\n",
       "  304,\n",
       "  5557,\n",
       "  322,\n",
       "  1423,\n",
       "  278,\n",
       "  297,\n",
       "  20309,\n",
       "  29889,\n",
       "  512,\n",
       "  1206,\n",
       "  24809,\n",
       "  565,\n",
       "  372,\n",
       "  338,\n",
       "  2319,\n",
       "  727,\n",
       "  338,\n",
       "  694,\n",
       "  817,\n",
       "  310,\n",
       "  380,\n",
       "  2335,\n",
       "  267,\n",
       "  29892,\n",
       "  278,\n",
       "  6788,\n",
       "  338,\n",
       "  5491,\n",
       "  1532,\n",
       "  8745,\n",
       "  491,\n",
       "  6788,\n",
       "  21174,\n",
       "  414,\n",
       "  29892,\n",
       "  366,\n",
       "  817,\n",
       "  451,\n",
       "  15982,\n",
       "  1048,\n",
       "  278,\n",
       "  6788,\n",
       "  29889,\n",
       "  512,\n",
       "  1206,\n",
       "  310,\n",
       "  285,\n",
       "  391,\n",
       "  2497,\n",
       "  25300,\n",
       "  708,\n",
       "  508,\n",
       "  367,\n",
       "  8560,\n",
       "  472,\n",
       "  278,\n",
       "  1021,\n",
       "  931,\n",
       "  408,\n",
       "  6425,\n",
       "  985,\n",
       "  25300,\n",
       "  708,\n",
       "  29889,\n",
       "  2398,\n",
       "  29892,\n",
       "  285,\n",
       "  391,\n",
       "  15173,\n",
       "  4049,\n",
       "  2693,\n",
       "  3023,\n",
       "  304,\n",
       "  4832,\n",
       "  11405,\n",
       "  1156,\n",
       "  385,\n",
       "  6425,\n",
       "  985,\n",
       "  338,\n",
       "  32000],\n",
       " 'attention_mask': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  2763,\n",
       "  304,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  739,\n",
       "  4225,\n",
       "  304,\n",
       "  367,\n",
       "  4392,\n",
       "  1312,\n",
       "  304,\n",
       "  1073,\n",
       "  3692,\n",
       "  372,\n",
       "  338,\n",
       "  925,\n",
       "  322,\n",
       "  6425,\n",
       "  985,\n",
       "  470,\n",
       "  263,\n",
       "  285,\n",
       "  391,\n",
       "  2497,\n",
       "  29889,\n",
       "  29871,\n",
       "  512,\n",
       "  1206,\n",
       "  310,\n",
       "  285,\n",
       "  391,\n",
       "  2497,\n",
       "  372,\n",
       "  4225,\n",
       "  304,\n",
       "  367,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  29889,\n",
       "  29871,\n",
       "  2860,\n",
       "  393,\n",
       "  6788,\n",
       "  13589,\n",
       "  362,\n",
       "  304,\n",
       "  10032,\n",
       "  278,\n",
       "  6788,\n",
       "  322,\n",
       "  3677,\n",
       "  747,\n",
       "  29875,\n",
       "  13574,\n",
       "  304,\n",
       "  5557,\n",
       "  322,\n",
       "  1423,\n",
       "  278,\n",
       "  297,\n",
       "  20309,\n",
       "  29889,\n",
       "  512,\n",
       "  1206,\n",
       "  24809,\n",
       "  565,\n",
       "  372,\n",
       "  338,\n",
       "  2319,\n",
       "  727,\n",
       "  338,\n",
       "  694,\n",
       "  817,\n",
       "  310,\n",
       "  380,\n",
       "  2335,\n",
       "  267,\n",
       "  29892,\n",
       "  278,\n",
       "  6788,\n",
       "  338,\n",
       "  5491,\n",
       "  1532,\n",
       "  8745,\n",
       "  491,\n",
       "  6788,\n",
       "  21174,\n",
       "  414,\n",
       "  29892,\n",
       "  366,\n",
       "  817,\n",
       "  451,\n",
       "  15982,\n",
       "  1048,\n",
       "  278,\n",
       "  6788,\n",
       "  29889,\n",
       "  512,\n",
       "  1206,\n",
       "  310,\n",
       "  285,\n",
       "  391,\n",
       "  2497,\n",
       "  25300,\n",
       "  708,\n",
       "  508,\n",
       "  367,\n",
       "  8560,\n",
       "  472,\n",
       "  278,\n",
       "  1021,\n",
       "  931,\n",
       "  408,\n",
       "  6425,\n",
       "  985,\n",
       "  25300,\n",
       "  708,\n",
       "  29889,\n",
       "  2398,\n",
       "  29892,\n",
       "  285,\n",
       "  391,\n",
       "  15173,\n",
       "  4049,\n",
       "  2693,\n",
       "  3023,\n",
       "  304,\n",
       "  4832,\n",
       "  11405,\n",
       "  1156,\n",
       "  385,\n",
       "  6425,\n",
       "  985,\n",
       "  338,\n",
       "  32000]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f233469a-c003-4ea1-b113-2dc640ebf09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (2/2 shards): 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 110000/110000 [00:01<00:00, 104560.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_set.save_to_disk(data_path + \"tokenized\")\n",
    "# train_set = load_from_disk(data_path + \"tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e4e383a-b0dd-4430-84e6-a72cca745d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhasindumadushan325\u001b[0m (\u001b[33mhasindumadushan325-university-of-peradeniya\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/super_admin/hasindu/src/wandb/run-20250516_042341-75mi5uni</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/med-qna-finetune/runs/75mi5uni' target=\"_blank\">attempt_4</a></strong> to <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/med-qna-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/med-qna-finetune' target=\"_blank\">https://wandb.ai/hasindumadushan325-university-of-peradeniya/med-qna-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/med-qna-finetune/runs/75mi5uni' target=\"_blank\">https://wandb.ai/hasindumadushan325-university-of-peradeniya/med-qna-finetune/runs/75mi5uni</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hasindumadushan325-university-of-peradeniya/med-qna-finetune/runs/75mi5uni?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x783b5ce00f70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"med-qna-finetune\",\n",
    "    name=\"attempt_4\",\n",
    "    config={\n",
    "        \"model\": model_id,\n",
    "        \"lora_r\": lora_r,\n",
    "        \"lora_alpha\": lora_alpha,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"quantization\": quantization,\n",
    "        \"lora_target_modules\": lora_target_modules\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4341cf2-34e2-444b-bbf5-710281ad62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=quantization==\"4bit\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89f63831-2d8c-4824-973e-3b5f90e3ffbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f42d169c-852d-489e-9a1c-b6f9f6469d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa8c15c0-f5f0-44dc-900f-292689b2c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b5c62-72a2-4e23-871d-3a2233ec127d",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "318af7f8-35ce-4311-819b-c278c6e93471",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    eval_strategy=\"epoch\",  # ✅ eval at each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"attempt_4\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b462a5f-3e8a-4897-bc0d-3fc498778fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61905/1780078394.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer, padding=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0cea96-6f0c-477f-8d91-79b91e000ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6029' max='17190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6029/17190 5:58:50 < 11:04:30, 0.28 it/s, Epoch 1.75/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.748200</td>\n",
       "      <td>1.726235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model(model_output_dir + \"/final\")\n",
    "tokenizer.save_pretrained(model_output_dir + \"/final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcab8a00-a380-493d-93af-c66f77ef94c6",
   "metadata": {},
   "source": [
    "## Merge model with lora weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ad3e24-e26f-46eb-b85e-56341f359a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, output_model_dir + \"/final\")\n",
    "merged_model = finetuned_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5c5339-4f30-4dc8-a062-91c8b0ecc14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25254130-380f-4a0c-b23f-90a7b4801bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(output_model_dir +  \"/final_pretrained\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
