{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "026f8cee-8ece-469f-90ab-a3252e726ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attention\n",
      "  Downloading flash_attention-1.0.0-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: flash-attention\n",
      "Successfully installed flash-attention-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install flash-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a40d16-2fc5-47a7-8023-633fb24547f0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31bb4a93-2d17-4994-a440-23107a47e0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super_admin/hasindu/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict, load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import PeftModel, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from utils import tokenize_dataset_for_qna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a1366-410f-4d33-9226-b1c6cebdacd6",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b247f35-dfcd-4b94-bd81-95bc57d829e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/qna/\"\n",
    "train_data_path = data_path + \"train.csv\"\n",
    "val_data_path = data_path + \"val.csv\"\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "base_model_path = \"../models/phi_pubmed_pretrained_attempt_3/final_pretrained\"\n",
    "\n",
    "model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "model_output_dir = \"../models/phi_qna_finetuned_attempt_3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eceb44-4e47-418a-b767-c443edf822ff",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "082ee172-0d21-43db-96f7-9ba72c4baf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r = 24\n",
    "lora_alpha = 48\n",
    "lora_target_modules = [\"q_proj\", \"v_proj\", \"o_proj\"]\n",
    "batch_size = 32\n",
    "quantization = None\n",
    "lora_dropout = 0.05\n",
    "epochs = 6\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5e8dde-e44f-4718-8e99-f5fcb9b66cd0",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e2fda0-49ac-41b0-a2a8-03fbba8db2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Instruction:\n",
      "Assume you are an excellent doctor. Using your knowledge, answer the question given below.\n",
      "\n",
      "# Question: {question}\n",
      "\n",
      "# Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "# Instruction:\n",
    "Assume you are an excellent doctor. Using your knowledge, answer the question given below.\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Answer: \"\"\"\n",
    "prompt_template = prompt_template.strip()\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4358e35f-0f55-4458-ba55-014c149fddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb6adcda-05e5-4c38-a565-2cab23a9493a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac21737f-73e8-48e3-95b1-faf83a5f074a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|                                                                                                                                                                                                    | 0/300 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'paddint_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m val_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(val_data_path)\n\u001b[1;32m      2\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(train_data_path)\n\u001b[0;32m----> 4\u001b[0m val_set \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_dataset_for_qna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m train_set \u001b[38;5;241m=\u001b[39m tokenize_dataset_for_qna(tokenizer, train_df, prompt_template, max_len)\n",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m, in \u001b[0;36mtokenize_dataset_for_qna\u001b[0;34m(tokenizer, data_df, prompt_template, max_len)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenize_dataset_for_qna\u001b[39m(tokenizer, data_df, prompt_template, max_len):\n\u001b[1;32m      6\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(data_df)\n\u001b[0;32m----> 7\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize_for_qna\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/hasindu/myenv/lib/python3.10/site-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/hasindu/myenv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3079\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3075\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3076\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3077\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3078\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3080\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3081\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/hasindu/myenv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3501\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[1;32m   3499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[1;32m   3500\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 3501\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m iter_outputs(shard_iterable):\n\u001b[1;32m   3502\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3503\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/hasindu/myenv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3475\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.iter_outputs\u001b[0;34m(shard_iterable)\u001b[0m\n\u001b[1;32m   3473\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3474\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3475\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/hasindu/myenv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3398\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function\u001b[0;34m(pa_inputs, indices, offset)\u001b[0m\n\u001b[1;32m   3396\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[1;32m   3397\u001b[0m inputs, fn_args, additional_args, fn_kwargs \u001b[38;5;241m=\u001b[39m prepare_inputs(pa_inputs, indices, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[0;32m-> 3398\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m, in \u001b[0;36mtokenize_dataset_for_qna.<locals>.<lambda>\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenize_dataset_for_qna\u001b[39m(tokenizer, data_df, prompt_template, max_len):\n\u001b[1;32m      6\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(data_df)\n\u001b[0;32m----> 7\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m sample: \u001b[43mtokenize_for_qna\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "Cell \u001b[0;32mIn[12], line 46\u001b[0m, in \u001b[0;36mtokenize_for_qna\u001b[0;34m(example, tokenizer, prompt_template, max_len)\u001b[0m\n\u001b[1;32m     44\u001b[0m padding_len \u001b[38;5;241m=\u001b[39m max_len \u001b[38;5;241m-\u001b[39m full_len\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Mask the prompt tokens\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m labels[\u001b[43mpaddint_len\u001b[49m:padding_len \u001b[38;5;241m+\u001b[39m prompt_len] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Update the tokenized dict with numpy arrays\u001b[39;00m\n\u001b[1;32m     49\u001b[0m tokenized[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'paddint_len' is not defined"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_csv(val_data_path)\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "\n",
    "val_set = tokenize_dataset_for_qna(tokenizer, val_df, prompt_template, max_len)\n",
    "train_set = tokenize_dataset_for_qna(tokenizer, train_df, prompt_template, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f233469a-c003-4ea1-b113-2dc640ebf09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set.save_to_disk(data_path + \"tokenized\")\n",
    "train_set = load_from_disk(data_path + \"tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f26f4aae-0ad3-4f67-9e46-23b2ebe4cc14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'I have had a cyst now for 6 months.6weeks ago it became bigger and I smashed it alittle and alot of thick clear stcky stuff came out of it.Now theres a huge hole...like a sack.it will not heal or fill in and really needs stitches but if you have it fixed...the cyst will come back.Its not painful in the least.(its in between my vag. and anus )Its just a hole.my dad told me when he was younger this same thing happen to him 4 times until he had the sack removed.What is this???I was very scared of cancer but my dad said it will go away when the sack is removed.Im 48 yrs old.Its on my scar where they cut me so I could have my children.thank you Tina Leatherwood',\n",
       " 'answer': 'Welcome to Chat Doctor It needs to be examined to know whether it is just and abscess or a fistula.  In case of fistula it needs to be Chat Doctor.  After that pain medication to reduce the pain and antibiotic to prevent and check the infection. In case assess if it is small there is no need of stitches, the pain is usually well managed by painkillers, you need not worry about the pain. In case of fistula surgery can be performed at the same time as abscess surgery. However, fistulas often develop four to six weeks after an abscess is',\n",
       " 'input_ids': [32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  396,\n",
       "  2799,\n",
       "  4080,\n",
       "  29901,\n",
       "  13,\n",
       "  7900,\n",
       "  2017,\n",
       "  366,\n",
       "  526,\n",
       "  385,\n",
       "  15129,\n",
       "  11619,\n",
       "  29889,\n",
       "  5293,\n",
       "  596,\n",
       "  7134,\n",
       "  29892,\n",
       "  1234,\n",
       "  278,\n",
       "  1139,\n",
       "  2183,\n",
       "  2400,\n",
       "  29889,\n",
       "  13,\n",
       "  13,\n",
       "  29937,\n",
       "  894,\n",
       "  29901,\n",
       "  306,\n",
       "  505,\n",
       "  750,\n",
       "  263,\n",
       "  274,\n",
       "  858,\n",
       "  1286,\n",
       "  363,\n",
       "  29871,\n",
       "  29953,\n",
       "  7378,\n",
       "  29889,\n",
       "  29953,\n",
       "  705,\n",
       "  14541,\n",
       "  8020,\n",
       "  372,\n",
       "  3897,\n",
       "  16600,\n",
       "  322,\n",
       "  306,\n",
       "  1560,\n",
       "  25936,\n",
       "  372,\n",
       "  394,\n",
       "  1992,\n",
       "  322,\n",
       "  394,\n",
       "  327,\n",
       "  310,\n",
       "  12003,\n",
       "  2821,\n",
       "  380,\n",
       "  384,\n",
       "  29891,\n",
       "  6433,\n",
       "  2996,\n",
       "  714,\n",
       "  310,\n",
       "  372,\n",
       "  29889,\n",
       "  10454,\n",
       "  266,\n",
       "  11175,\n",
       "  263,\n",
       "  12176,\n",
       "  16188,\n",
       "  856,\n",
       "  4561,\n",
       "  263,\n",
       "  269,\n",
       "  547,\n",
       "  29889,\n",
       "  277,\n",
       "  674,\n",
       "  451,\n",
       "  540,\n",
       "  284,\n",
       "  470,\n",
       "  5445,\n",
       "  297,\n",
       "  322,\n",
       "  2289,\n",
       "  4225,\n",
       "  380,\n",
       "  2335,\n",
       "  267,\n",
       "  541,\n",
       "  565,\n",
       "  366,\n",
       "  505,\n",
       "  372,\n",
       "  4343,\n",
       "  856,\n",
       "  1552,\n",
       "  274,\n",
       "  858,\n",
       "  674,\n",
       "  2041,\n",
       "  1250,\n",
       "  29889,\n",
       "  29902,\n",
       "  1372,\n",
       "  451,\n",
       "  6788,\n",
       "  1319,\n",
       "  297,\n",
       "  278,\n",
       "  3203,\n",
       "  14030,\n",
       "  1169,\n",
       "  297,\n",
       "  1546,\n",
       "  590,\n",
       "  325,\n",
       "  351,\n",
       "  29889,\n",
       "  322,\n",
       "  385,\n",
       "  375,\n",
       "  1723,\n",
       "  29902,\n",
       "  1372,\n",
       "  925,\n",
       "  263,\n",
       "  16188,\n",
       "  29889,\n",
       "  1357,\n",
       "  270,\n",
       "  328,\n",
       "  5429,\n",
       "  592,\n",
       "  746,\n",
       "  540,\n",
       "  471,\n",
       "  20023,\n",
       "  445,\n",
       "  1021,\n",
       "  2655,\n",
       "  3799,\n",
       "  304,\n",
       "  1075,\n",
       "  29871,\n",
       "  29946,\n",
       "  3064,\n",
       "  2745,\n",
       "  540,\n",
       "  750,\n",
       "  278,\n",
       "  269,\n",
       "  547,\n",
       "  6206,\n",
       "  29889,\n",
       "  5618,\n",
       "  338,\n",
       "  445,\n",
       "  28772,\n",
       "  29902,\n",
       "  471,\n",
       "  1407,\n",
       "  885,\n",
       "  1965,\n",
       "  310,\n",
       "  23900,\n",
       "  541,\n",
       "  590,\n",
       "  270,\n",
       "  328,\n",
       "  1497,\n",
       "  372,\n",
       "  674,\n",
       "  748,\n",
       "  3448,\n",
       "  746,\n",
       "  278,\n",
       "  269,\n",
       "  547,\n",
       "  338,\n",
       "  6206,\n",
       "  29889,\n",
       "  1888,\n",
       "  29871,\n",
       "  29946,\n",
       "  29947,\n",
       "  343,\n",
       "  2288,\n",
       "  2030,\n",
       "  29889,\n",
       "  29902,\n",
       "  1372,\n",
       "  373,\n",
       "  590,\n",
       "  21990,\n",
       "  988,\n",
       "  896,\n",
       "  5700,\n",
       "  592,\n",
       "  577,\n",
       "  306,\n",
       "  1033,\n",
       "  505,\n",
       "  590,\n",
       "  4344,\n",
       "  29889,\n",
       "  386,\n",
       "  804,\n",
       "  366,\n",
       "  323,\n",
       "  1099,\n",
       "  951,\n",
       "  1624,\n",
       "  6115,\n",
       "  13,\n",
       "  13,\n",
       "  29937,\n",
       "  673,\n",
       "  29901,\n",
       "  28862,\n",
       "  2763,\n",
       "  304,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  739,\n",
       "  4225,\n",
       "  304,\n",
       "  367,\n",
       "  4392,\n",
       "  1312,\n",
       "  304,\n",
       "  1073,\n",
       "  3692,\n",
       "  372,\n",
       "  338,\n",
       "  925,\n",
       "  322,\n",
       "  6425,\n",
       "  985,\n",
       "  470,\n",
       "  263,\n",
       "  285,\n",
       "  391,\n",
       "  2497,\n",
       "  29889,\n",
       "  29871,\n",
       "  512,\n",
       "  1206,\n",
       "  310,\n",
       "  285,\n",
       "  391,\n",
       "  2497,\n",
       "  372,\n",
       "  4225,\n",
       "  304,\n",
       "  367,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  29889,\n",
       "  29871,\n",
       "  2860,\n",
       "  393,\n",
       "  6788,\n",
       "  13589,\n",
       "  362,\n",
       "  304,\n",
       "  10032,\n",
       "  278,\n",
       "  6788,\n",
       "  322,\n",
       "  3677,\n",
       "  747,\n",
       "  29875,\n",
       "  13574,\n",
       "  304,\n",
       "  5557,\n",
       "  322,\n",
       "  1423,\n",
       "  278,\n",
       "  297,\n",
       "  20309,\n",
       "  29889,\n",
       "  512,\n",
       "  1206,\n",
       "  24809,\n",
       "  565,\n",
       "  372,\n",
       "  338,\n",
       "  2319,\n",
       "  727,\n",
       "  338,\n",
       "  694,\n",
       "  817,\n",
       "  310,\n",
       "  380,\n",
       "  2335,\n",
       "  267,\n",
       "  29892,\n",
       "  278,\n",
       "  6788,\n",
       "  338,\n",
       "  5491,\n",
       "  1532,\n",
       "  8745,\n",
       "  491,\n",
       "  6788,\n",
       "  21174,\n",
       "  414,\n",
       "  29892,\n",
       "  366,\n",
       "  817,\n",
       "  451,\n",
       "  15982,\n",
       "  1048,\n",
       "  278,\n",
       "  6788,\n",
       "  29889,\n",
       "  512,\n",
       "  1206,\n",
       "  310,\n",
       "  285,\n",
       "  391,\n",
       "  2497,\n",
       "  25300,\n",
       "  708,\n",
       "  508,\n",
       "  367,\n",
       "  8560,\n",
       "  472,\n",
       "  278,\n",
       "  1021,\n",
       "  931,\n",
       "  408,\n",
       "  6425,\n",
       "  985,\n",
       "  25300,\n",
       "  708,\n",
       "  29889,\n",
       "  2398,\n",
       "  29892,\n",
       "  285,\n",
       "  391,\n",
       "  15173,\n",
       "  4049,\n",
       "  2693,\n",
       "  3023,\n",
       "  304,\n",
       "  4832,\n",
       "  11405,\n",
       "  1156,\n",
       "  385,\n",
       "  6425,\n",
       "  985,\n",
       "  338,\n",
       "  32000],\n",
       " 'attention_mask': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  2763,\n",
       "  304,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  739,\n",
       "  4225,\n",
       "  304,\n",
       "  367,\n",
       "  4392,\n",
       "  1312,\n",
       "  304,\n",
       "  1073,\n",
       "  3692,\n",
       "  372,\n",
       "  338,\n",
       "  925,\n",
       "  322,\n",
       "  6425,\n",
       "  985,\n",
       "  470,\n",
       "  263,\n",
       "  285,\n",
       "  391,\n",
       "  2497,\n",
       "  29889,\n",
       "  29871,\n",
       "  512,\n",
       "  1206,\n",
       "  310,\n",
       "  285,\n",
       "  391,\n",
       "  2497,\n",
       "  372,\n",
       "  4225,\n",
       "  304,\n",
       "  367,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  29889,\n",
       "  29871,\n",
       "  2860,\n",
       "  393,\n",
       "  6788,\n",
       "  13589,\n",
       "  362,\n",
       "  304,\n",
       "  10032,\n",
       "  278,\n",
       "  6788,\n",
       "  322,\n",
       "  3677,\n",
       "  747,\n",
       "  29875,\n",
       "  13574,\n",
       "  304,\n",
       "  5557,\n",
       "  322,\n",
       "  1423,\n",
       "  278,\n",
       "  297,\n",
       "  20309,\n",
       "  29889,\n",
       "  512,\n",
       "  1206,\n",
       "  24809,\n",
       "  565,\n",
       "  372,\n",
       "  338,\n",
       "  2319,\n",
       "  727,\n",
       "  338,\n",
       "  694,\n",
       "  817,\n",
       "  310,\n",
       "  380,\n",
       "  2335,\n",
       "  267,\n",
       "  29892,\n",
       "  278,\n",
       "  6788,\n",
       "  338,\n",
       "  5491,\n",
       "  1532,\n",
       "  8745,\n",
       "  491,\n",
       "  6788,\n",
       "  21174,\n",
       "  414,\n",
       "  29892,\n",
       "  366,\n",
       "  817,\n",
       "  451,\n",
       "  15982,\n",
       "  1048,\n",
       "  278,\n",
       "  6788,\n",
       "  29889,\n",
       "  512,\n",
       "  1206,\n",
       "  310,\n",
       "  285,\n",
       "  391,\n",
       "  2497,\n",
       "  25300,\n",
       "  708,\n",
       "  508,\n",
       "  367,\n",
       "  8560,\n",
       "  472,\n",
       "  278,\n",
       "  1021,\n",
       "  931,\n",
       "  408,\n",
       "  6425,\n",
       "  985,\n",
       "  25300,\n",
       "  708,\n",
       "  29889,\n",
       "  2398,\n",
       "  29892,\n",
       "  285,\n",
       "  391,\n",
       "  15173,\n",
       "  4049,\n",
       "  2693,\n",
       "  3023,\n",
       "  304,\n",
       "  4832,\n",
       "  11405,\n",
       "  1156,\n",
       "  385,\n",
       "  6425,\n",
       "  985,\n",
       "  338,\n",
       "  32000]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e4e383a-b0dd-4430-84e6-a72cca745d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhasindumadushan325\u001b[0m (\u001b[33mhasindumadushan325-university-of-peradeniya\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/super_admin/hasindu/src/wandb/run-20250515_042541-td127c8r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/med-qna-finetune/runs/td127c8r' target=\"_blank\">attempt_3</a></strong> to <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/med-qna-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/med-qna-finetune' target=\"_blank\">https://wandb.ai/hasindumadushan325-university-of-peradeniya/med-qna-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/med-qna-finetune/runs/td127c8r' target=\"_blank\">https://wandb.ai/hasindumadushan325-university-of-peradeniya/med-qna-finetune/runs/td127c8r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hasindumadushan325-university-of-peradeniya/med-qna-finetune/runs/td127c8r?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x71e2ab3c9030>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"med-qna-finetune\",\n",
    "    name=\"attempt_3\",\n",
    "    config={\n",
    "        \"model\": model_id,\n",
    "        \"lora_r\": lora_r,\n",
    "        \"lora_alpha\": lora_alpha,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"quantization\": quantization,\n",
    "        \"lora_target_modules\": lora_target_modules\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4341cf2-34e2-444b-bbf5-710281ad62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=quantization==\"4bit\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89f63831-2d8c-4824-973e-3b5f90e3ffbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.00it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f42d169c-852d-489e-9a1c-b6f9f6469d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa8c15c0-f5f0-44dc-900f-292689b2c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=lora_target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33b5c62-72a2-4e23-871d-3a2233ec127d",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "318af7f8-35ce-4311-819b-c278c6e93471",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    eval_strategy=\"epoch\",  # ✅ eval at each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"attempt_3\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b462a5f-3e8a-4897-bc0d-3fc498778fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51829/1780078394.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer, padding=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0cea96-6f0c-477f-8d91-79b91e000ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53' max='20628' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   53/20628 02:59 < 20:08:13, 0.28 it/s, Epoch 0.02/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model(model_output_dir + \"/final\")\n",
    "tokenizer.save_pretrained(model_output_dir + \"/final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcab8a00-a380-493d-93af-c66f77ef94c6",
   "metadata": {},
   "source": [
    "## Merge model with lora weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ad3e24-e26f-46eb-b85e-56341f359a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, output_model_dir + \"/final\")\n",
    "merged_model = finetuned_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5c5339-4f30-4dc8-a062-91c8b0ecc14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25254130-380f-4a0c-b23f-90a7b4801bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(output_model_dir +  \"/final_pretrained\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
