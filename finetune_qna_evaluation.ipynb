{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd5ae7-e718-4db9-9b4a-ff81be0e2457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install absl-py rouge-score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034bab24-dc8a-4b83-a570-29fcf3bdeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd47c21-5017-47b8-ac58-df9338ef163a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4643151f-8b64-40cb-9b36-758e369e6407",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20bc9df7-5328-4945-a1bd-30f823fc8b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super_admin/hasindu/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import wandb\n",
    "import evaluate  # Hugging Face's evaluate library\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from utils import tokenize_dataset_for_qna\n",
    "from prompt_templates import qna_prompt_template as prompt_template\n",
    "from generate import generate, stream_generate\n",
    "from evaluation_metrics import compute_metrics_for_qna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458060f-cfcf-4f51-a6aa-506cef1dc7c1",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb057781-55b7-494f-bcb4-dd7d00ba4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/phi_qna_finetuned_attempt_5/final\"\n",
    "\n",
    "data_path = \"../data/qna/\"\n",
    "test_data_path = data_path + \"test.csv\"\n",
    "\n",
    "model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "base_model_path = \"../models/phi_pubmed_pretrained_attempt_5/final_merged\"\n",
    "\n",
    "max_len = 512\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38a3746-3c13-42cc-920a-2614eeede918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhasindumadushan325\u001b[0m (\u001b[33mhasindumadushan325-university-of-peradeniya\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/super_admin/hasindu/src/wandb/run-20250518_161643-vona95c1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/qna_finetune-evaluation/runs/vona95c1' target=\"_blank\">attempt_5</a></strong> to <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/qna_finetune-evaluation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/qna_finetune-evaluation' target=\"_blank\">https://wandb.ai/hasindumadushan325-university-of-peradeniya/qna_finetune-evaluation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/qna_finetune-evaluation/runs/vona95c1' target=\"_blank\">https://wandb.ai/hasindumadushan325-university-of-peradeniya/qna_finetune-evaluation/runs/vona95c1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hasindumadushan325-university-of-peradeniya/qna_finetune-evaluation/runs/vona95c1?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7e45e8934490>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"qna_finetune-evaluation\", name=\"attempt_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f573ca-73a8-4ef6-af46-ecd2d84e6bad",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15035ec-6729-4cf4-a946-1b1f2a700e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Instruction:\n",
      "Assume you are an excellent doctor. Using your knowledge, answer the question given below.\n",
      "\n",
      "# Question: {question}\n",
      "\n",
      "# Answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8443f97-858d-4220-b3d2-8f728fa48020",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac602a77-0a3a-4d89-915a-b102625ebce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [00:01<00:00, 849.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_data_path)\n",
    "test_set = tokenize_dataset_for_qna(tokenizer, test_df.iloc[:1500, :], prompt_template, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a851ebd-aff1-4e44-ad09-93151119bcb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Ive had sleep apnea for approximately 8 years now. I also have polycythemia vera which started around the same time. I was constantly having to get plebotomies every six months when I wasnt wearing my cpap all of the time. But for the last few years now, Ive been real diligent about wearing it and I havent had to get any phlebotomies. Im now thinking that my polycythemia vera was because I was not getting enough oxygen? Am I on the right track? I would appreciate any help you could offer. Thanks.',\n",
       " 'answer': 'Hello, Thank you for contacting ChatDoctorI understand your concern, I am Chat Doctor, Infectious Disease Specialist answering your query. Yes you are on right track. This can happen to you for low oxygen tension. As apnea occurs during sleep time there is adaptation by mean of increase RBC which leads you towards the poly Bohemia. I advise you to use the CPAP during sleep otherwise the polycythemia will increase. I advise you to take the phlebotomies done and once the hemoglobin level is under control you can start using he CPAP. You can ask for further queries here on bit.ly/ Chat Doctor. Thank you, ChatDoctorInfectious Disease specialist wish you the best health at Chat Doctor.',\n",
       " 'input_ids': [32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  396,\n",
       "  2799,\n",
       "  4080,\n",
       "  29901,\n",
       "  13,\n",
       "  7900,\n",
       "  2017,\n",
       "  366,\n",
       "  526,\n",
       "  385,\n",
       "  15129,\n",
       "  11619,\n",
       "  29889,\n",
       "  5293,\n",
       "  596,\n",
       "  7134,\n",
       "  29892,\n",
       "  1234,\n",
       "  278,\n",
       "  1139,\n",
       "  2183,\n",
       "  2400,\n",
       "  29889,\n",
       "  13,\n",
       "  13,\n",
       "  29937,\n",
       "  894,\n",
       "  29901,\n",
       "  306,\n",
       "  345,\n",
       "  750,\n",
       "  8709,\n",
       "  3095,\n",
       "  14011,\n",
       "  363,\n",
       "  14235,\n",
       "  29871,\n",
       "  29947,\n",
       "  2440,\n",
       "  1286,\n",
       "  29889,\n",
       "  306,\n",
       "  884,\n",
       "  505,\n",
       "  15680,\n",
       "  1270,\n",
       "  386,\n",
       "  29747,\n",
       "  1147,\n",
       "  29874,\n",
       "  607,\n",
       "  4687,\n",
       "  2820,\n",
       "  278,\n",
       "  1021,\n",
       "  931,\n",
       "  29889,\n",
       "  306,\n",
       "  471,\n",
       "  21003,\n",
       "  2534,\n",
       "  304,\n",
       "  679,\n",
       "  5644,\n",
       "  7451,\n",
       "  290,\n",
       "  583,\n",
       "  1432,\n",
       "  4832,\n",
       "  7378,\n",
       "  746,\n",
       "  306,\n",
       "  471,\n",
       "  593,\n",
       "  591,\n",
       "  4362,\n",
       "  590,\n",
       "  21447,\n",
       "  481,\n",
       "  599,\n",
       "  310,\n",
       "  278,\n",
       "  931,\n",
       "  29889,\n",
       "  1205,\n",
       "  363,\n",
       "  278,\n",
       "  1833,\n",
       "  2846,\n",
       "  2440,\n",
       "  1286,\n",
       "  29892,\n",
       "  306,\n",
       "  345,\n",
       "  1063,\n",
       "  1855,\n",
       "  21749,\n",
       "  25692,\n",
       "  1048,\n",
       "  591,\n",
       "  4362,\n",
       "  372,\n",
       "  322,\n",
       "  306,\n",
       "  447,\n",
       "  794,\n",
       "  750,\n",
       "  304,\n",
       "  679,\n",
       "  738,\n",
       "  1374,\n",
       "  280,\n",
       "  7451,\n",
       "  290,\n",
       "  583,\n",
       "  29889,\n",
       "  1954,\n",
       "  1286,\n",
       "  7291,\n",
       "  393,\n",
       "  590,\n",
       "  15680,\n",
       "  1270,\n",
       "  386,\n",
       "  29747,\n",
       "  1147,\n",
       "  29874,\n",
       "  471,\n",
       "  1363,\n",
       "  306,\n",
       "  471,\n",
       "  451,\n",
       "  2805,\n",
       "  3307,\n",
       "  288,\n",
       "  28596,\n",
       "  29973,\n",
       "  1913,\n",
       "  306,\n",
       "  373,\n",
       "  278,\n",
       "  1492,\n",
       "  5702,\n",
       "  29973,\n",
       "  306,\n",
       "  723,\n",
       "  11188,\n",
       "  738,\n",
       "  1371,\n",
       "  366,\n",
       "  1033,\n",
       "  5957,\n",
       "  29889,\n",
       "  1834,\n",
       "  29889,\n",
       "  13,\n",
       "  13,\n",
       "  29937,\n",
       "  673,\n",
       "  29901,\n",
       "  10994,\n",
       "  29892,\n",
       "  3374,\n",
       "  366,\n",
       "  363,\n",
       "  6958,\n",
       "  292,\n",
       "  678,\n",
       "  271,\n",
       "  6132,\n",
       "  2801,\n",
       "  29902,\n",
       "  2274,\n",
       "  596,\n",
       "  5932,\n",
       "  29892,\n",
       "  306,\n",
       "  626,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  29892,\n",
       "  512,\n",
       "  3647,\n",
       "  2738,\n",
       "  360,\n",
       "  895,\n",
       "  559,\n",
       "  12630,\n",
       "  391,\n",
       "  22862,\n",
       "  596,\n",
       "  2346,\n",
       "  29889,\n",
       "  3869,\n",
       "  366,\n",
       "  526,\n",
       "  373,\n",
       "  1492,\n",
       "  5702,\n",
       "  29889,\n",
       "  910,\n",
       "  508,\n",
       "  3799,\n",
       "  304,\n",
       "  366,\n",
       "  363,\n",
       "  4482,\n",
       "  288,\n",
       "  28596,\n",
       "  260,\n",
       "  2673,\n",
       "  29889,\n",
       "  1094,\n",
       "  3095,\n",
       "  14011,\n",
       "  10008,\n",
       "  2645,\n",
       "  8709,\n",
       "  931,\n",
       "  727,\n",
       "  338,\n",
       "  28206,\n",
       "  491,\n",
       "  2099,\n",
       "  310,\n",
       "  7910,\n",
       "  390,\n",
       "  5371,\n",
       "  607,\n",
       "  11981,\n",
       "  366,\n",
       "  7113,\n",
       "  278,\n",
       "  15680,\n",
       "  17966,\n",
       "  29747,\n",
       "  29889,\n",
       "  306,\n",
       "  22939,\n",
       "  366,\n",
       "  304,\n",
       "  671,\n",
       "  278,\n",
       "  28505,\n",
       "  3301,\n",
       "  2645,\n",
       "  8709,\n",
       "  6467,\n",
       "  278,\n",
       "  15680,\n",
       "  1270,\n",
       "  386,\n",
       "  29747,\n",
       "  674,\n",
       "  7910,\n",
       "  29889,\n",
       "  306,\n",
       "  22939,\n",
       "  366,\n",
       "  304,\n",
       "  2125,\n",
       "  278,\n",
       "  1374,\n",
       "  280,\n",
       "  7451,\n",
       "  290,\n",
       "  583,\n",
       "  2309,\n",
       "  322,\n",
       "  2748,\n",
       "  278,\n",
       "  9736,\n",
       "  468,\n",
       "  417,\n",
       "  2109,\n",
       "  3233,\n",
       "  338,\n",
       "  1090,\n",
       "  2761,\n",
       "  366,\n",
       "  508,\n",
       "  1369,\n",
       "  773,\n",
       "  540,\n",
       "  28505,\n",
       "  3301,\n",
       "  29889,\n",
       "  887,\n",
       "  508,\n",
       "  2244,\n",
       "  363,\n",
       "  4340,\n",
       "  9365,\n",
       "  1244,\n",
       "  373,\n",
       "  2586,\n",
       "  29889,\n",
       "  368,\n",
       "  29914,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  29889,\n",
       "  3374,\n",
       "  366,\n",
       "  29892,\n",
       "  678,\n",
       "  271,\n",
       "  6132,\n",
       "  2801,\n",
       "  797,\n",
       "  3647,\n",
       "  2738,\n",
       "  360,\n",
       "  895,\n",
       "  559,\n",
       "  4266,\n",
       "  391,\n",
       "  6398,\n",
       "  366,\n",
       "  278,\n",
       "  1900,\n",
       "  9045,\n",
       "  472,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  29889,\n",
       "  32000],\n",
       " 'attention_mask': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  10994,\n",
       "  29892,\n",
       "  3374,\n",
       "  366,\n",
       "  363,\n",
       "  6958,\n",
       "  292,\n",
       "  678,\n",
       "  271,\n",
       "  6132,\n",
       "  2801,\n",
       "  29902,\n",
       "  2274,\n",
       "  596,\n",
       "  5932,\n",
       "  29892,\n",
       "  306,\n",
       "  626,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  29892,\n",
       "  512,\n",
       "  3647,\n",
       "  2738,\n",
       "  360,\n",
       "  895,\n",
       "  559,\n",
       "  12630,\n",
       "  391,\n",
       "  22862,\n",
       "  596,\n",
       "  2346,\n",
       "  29889,\n",
       "  3869,\n",
       "  366,\n",
       "  526,\n",
       "  373,\n",
       "  1492,\n",
       "  5702,\n",
       "  29889,\n",
       "  910,\n",
       "  508,\n",
       "  3799,\n",
       "  304,\n",
       "  366,\n",
       "  363,\n",
       "  4482,\n",
       "  288,\n",
       "  28596,\n",
       "  260,\n",
       "  2673,\n",
       "  29889,\n",
       "  1094,\n",
       "  3095,\n",
       "  14011,\n",
       "  10008,\n",
       "  2645,\n",
       "  8709,\n",
       "  931,\n",
       "  727,\n",
       "  338,\n",
       "  28206,\n",
       "  491,\n",
       "  2099,\n",
       "  310,\n",
       "  7910,\n",
       "  390,\n",
       "  5371,\n",
       "  607,\n",
       "  11981,\n",
       "  366,\n",
       "  7113,\n",
       "  278,\n",
       "  15680,\n",
       "  17966,\n",
       "  29747,\n",
       "  29889,\n",
       "  306,\n",
       "  22939,\n",
       "  366,\n",
       "  304,\n",
       "  671,\n",
       "  278,\n",
       "  28505,\n",
       "  3301,\n",
       "  2645,\n",
       "  8709,\n",
       "  6467,\n",
       "  278,\n",
       "  15680,\n",
       "  1270,\n",
       "  386,\n",
       "  29747,\n",
       "  674,\n",
       "  7910,\n",
       "  29889,\n",
       "  306,\n",
       "  22939,\n",
       "  366,\n",
       "  304,\n",
       "  2125,\n",
       "  278,\n",
       "  1374,\n",
       "  280,\n",
       "  7451,\n",
       "  290,\n",
       "  583,\n",
       "  2309,\n",
       "  322,\n",
       "  2748,\n",
       "  278,\n",
       "  9736,\n",
       "  468,\n",
       "  417,\n",
       "  2109,\n",
       "  3233,\n",
       "  338,\n",
       "  1090,\n",
       "  2761,\n",
       "  366,\n",
       "  508,\n",
       "  1369,\n",
       "  773,\n",
       "  540,\n",
       "  28505,\n",
       "  3301,\n",
       "  29889,\n",
       "  887,\n",
       "  508,\n",
       "  2244,\n",
       "  363,\n",
       "  4340,\n",
       "  9365,\n",
       "  1244,\n",
       "  373,\n",
       "  2586,\n",
       "  29889,\n",
       "  368,\n",
       "  29914,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  29889,\n",
       "  3374,\n",
       "  366,\n",
       "  29892,\n",
       "  678,\n",
       "  271,\n",
       "  6132,\n",
       "  2801,\n",
       "  797,\n",
       "  3647,\n",
       "  2738,\n",
       "  360,\n",
       "  895,\n",
       "  559,\n",
       "  4266,\n",
       "  391,\n",
       "  6398,\n",
       "  366,\n",
       "  278,\n",
       "  1900,\n",
       "  9045,\n",
       "  472,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  29889,\n",
       "  32000]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60d571-b664-464b-a3b5-7ba96d0e8e22",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65cb96b6-5bd7-44d1-bd84-c6a107afddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# === Quantized model loading ===\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0b63bf7-fe1b-401f-b8c1-a394a65ed24d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3Attention(\n",
       "              (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "              (qkv_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=24, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=24, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "              (activation_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
       "        (rotary_emb): Phi3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(model, model_path)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0fd68d-392c-49c4-bc85-81c05bb4a07b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ce6b4-018f-422f-a85e-285f9038dbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b39ed-ea6c-426c-a76c-63ac9283a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded167b3-4fc9-439c-8c3c-238d193c5a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a3294-c17a-403d-9fe7-626608f6f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./eval_output_base\",\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     do_eval=True,\n",
    "#     report_to=\"none\"\n",
    "# )\n",
    "\n",
    "# base_model_trainer = Trainer(\n",
    "#     model=base_model,\n",
    "#     args=training_args,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "# )\n",
    "\n",
    "# base_model_eval_result = base_model_trainer.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6480b4-2485-422f-a289-898f0ba4dbe9",
   "metadata": {},
   "source": [
    "# Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f852223-0fa4-49c8-9f81-e600cef32e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./eval_output\",\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",\n",
    "    eval_accumulation_steps=2,\n",
    "    label_names=[\"labels\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d93a44e-cf37-4dc0-8171-3bde13eaca09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98616/1623222695.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer, padding=False),\n",
    "    compute_metrics=compute_metrics_for_qna\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d3989f-7cbb-47aa-8197-c2dab6692fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='186' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [186/188 15:14 < 00:09, 0.20 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Evaluate perplexity ===\n",
    "eval_result = trainer.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcdbf5-5d13-49ac-b3a3-86e2986139ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print all results\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Loss: {eval_result['eval_loss']:.4f}\")\n",
    "print(f\"Perplexity: {torch.exp(torch.tensor(eval_result['eval_loss'])):.2f}\")\n",
    "print(f\"BLEU: {eval_result['eval_bleu']:.4f}\")\n",
    "print(f\"ROUGE-1: {eval_result['eval_rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {eval_result['eval_rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {eval_result['eval_rougeL']:.4f}\")\n",
    "print(f\"BERTscore precision: {eval_result['eval_bertscore_precision']:.4f}\")\n",
    "print(f\"BERTscore recall: {eval_result['eval_bertscore_recall']:.4f}\")\n",
    "print(f\"BERTscore f1: {eval_result['eval_bertscore_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be58f3-423a-42b3-9bc4-ae8a46dc9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\n",
    "    \"eval_loss\": eval_result['eval_loss'], \n",
    "    \"perplexity\": torch.exp(torch.tensor(eval_result['eval_loss'])),\n",
    "    \"BLUE\": eval_result['eval_bleu'],\n",
    "    \"ROUGE_1\": eval_result['eval_rouge1'],\n",
    "    \"ROUGE_2\": eval_result['eval_rouge2'],\n",
    "    \"ROUGE_L\": eval_result['eval_rougeL'],\n",
    "    \"BERTscore_precision\": eval_result['eval_bertscore_precision'],\n",
    "    \"BERTscore recall\": eval_result['eval_bertscore_recall'],\n",
    "    \"BERTscore f1\": eval_result['eval_bertscore_f1']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b6324-721a-40fd-94e8-260ae1f37420",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cef6f295-aa3e-4543-b18d-4233329c74ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"What is Glaucoma ?\",\n",
    "    \"What are the symptoms of Glaucoma ??\",\n",
    "    \"My sister is on Xanax, feyntnol patch and a pain medicine for cancer.  She has been on 25 of fentynol and within 6 days she has been bumped up to 100 now she is almost lethargic and breathing is really labored and right arm is twitching.. She was carrying on conversation Sunday and Monday patch was put on Tuesday and now cant even sit up..no one seems worried but me.. Just wondering what I could do\",\n",
    "    \"I was playing basketball the other night and went up to block a shot and flipped over the guy and landed on my side/back. Since then the lower left side of back/side have been sore, hurts when I take deep breaths and when I lay on my back, any chance of a bruised kidney or any serious injury I could have?\",\n",
    "    \"What are the treatments for High Blood Pressure ?\",\n",
    "    \"What is (are) Urinary Tract Infections ?\",\n",
    "    \"What are the symptoms of diabetes?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f748a1-4da5-439e-911d-0c76f22bbedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate(model, tokenizer, prompt_template.format(question=examples[3]), max_new_tokens=200)\t\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867378a-e7b3-4674-ba2a-0adccf82ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"example_2\": generated_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf3492-8ff3-4ad8-93a7-7dc58381f852",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726ad10-f44e-4381-92ea-1705858dab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(model, \"# The relationship between diabetes and blood pressure\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b38672-f3a9-481e-a6e3-b5fdb059ff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_generate(model, tokenizer, text, max_new_tokens=300):\n",
    "    model.eval()\n",
    "    sample = tokenizer(\n",
    "        text + tokenizer.eos_token,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    ).to(model.device)\n",
    "\n",
    "    input_ids = sample[\"input_ids\"]\n",
    "    generated = input_ids.clone()\n",
    "    past_key_values = None\n",
    "    position_ids = torch.arange(0, input_ids.shape[1], device=model.device).unsqueeze(0)\n",
    "\n",
    "    prev_decoded = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "    for i in range(max_new_tokens):\n",
    "        if i == 0:\n",
    "            input_token = input_ids\n",
    "        else:\n",
    "            input_token = next_token_id\n",
    "            position_ids = torch.tensor([[generated.shape[1] - 1]], device=model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_token,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "                position_ids=position_ids\n",
    "            )\n",
    "\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        next_token_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        generated = torch.cat((generated, next_token_id), dim=1)\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # Decode full sequence and compute the diff\n",
    "        decoded = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        new_text = decoded[len(prev_decoded):]\n",
    "        prev_decoded = decoded\n",
    "\n",
    "        yield new_text\n",
    "\n",
    "        if next_token_id.squeeze().item() == tokenizer.eos_token_id:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c46300-d4ee-41f4-9690-ffb0b7d0d086",
   "metadata": {},
   "source": [
    "\"What are the symptoms of Glaucoma ??\"\n",
    "2. \"Explain Management of advanced and extragonadal germ-cell tumors.\"\n",
    "3. \"what is the role of hns in the virulence phenotype of pathogenic salmonellae?\"\n",
    "4. \"what are drinking patterns of high-risk drivers?\"\n",
    "5. \"My sister is on Xanax, feyntnol patch and a pain medicine for cancer.  She has been on 25 of fentynol and within 6 days she has been bumped up to 100 now she is almost lethargic and breathing is really labored and right arm is twitching.. She was carrying on conversation Sunday and Monday patch was put on Tuesday and now cant even sit up..no one seems worried but me.. Just wondering what I could do\"\n",
    "6. \"I was playing basketball the other night and went up to block a shot and flipped over the guy and landed on my side/back. Since then the lower left side of back/side have been sore, hurts when I take deep breaths and when I lay on my back, any chance of a bruised kidney or any serious injury I could have?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb433ff-575c-44cf-b3fe-5235e1941ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in stream_generate(model, tokenizer, \"The symptoms of Glaucoma\\n\"):\n",
    "    print(token, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e997f-875b-4518-b4d0-5da482dd35e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in stream_generate(model, tokenizer, prompt_template.format(question=\"I was playing basketball the other night and went up to block a shot and flipped over the guy and landed on my side/back. Since then the lower left side of back/side have been sore, hurts when I take deep breaths and when I lay on my back, any chance of a bruised kidney or any serious injury I could have?\",\n",
    ")):\n",
    "    print(token, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c50739-9375-48b4-b882-d90e32d26f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"What are the symptoms of Glaucoma ??\",\n",
    "    \"My sister is on Xanax, feyntnol patch and a pain medicine for cancer.  She has been on 25 of fentynol and within 6 days she has been bumped up to 100 now she is almost lethargic and breathing is really labored and right arm is twitching.. She was carrying on conversation Sunday and Monday patch was put on Tuesday and now cant even sit up..no one seems worried but me.. Just wondering what I could do\",\n",
    "    \"I was playing basketball the other night and went up to block a shot and flipped over the guy and landed on my side/back. Since then the lower left side of back/side have been sore, hurts when I take deep breaths and when I lay on my back, any chance of a bruised kidney or any serious injury I could have?\",\n",
    "    \"What are the treatments for High Blood Pressure ?\",\n",
    "    \"What is (are) Urinary Tract Infections ?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063faa6-b569-4dfc-ba82-27f879bd23a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_table = wandb.Table(columns=[\"Question\", \"Generated answer\"])\n",
    "\n",
    "for example in examples:\n",
    "    generated_answer = \"\"\n",
    "    for token in stream_generate(model, tokenizer, prompt_template.format(question=example)):\n",
    "        generated_answer += token\n",
    "    wandb_table.add_data(example, generated_answer)\n",
    "    print(example, \"\\n\", generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b455a-4561-4c4a-aa26-95a3f8d8c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"generated_examples\": wandb_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422f3aee-e5c6-4ae9-b8f4-e5f5bb4506d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58027e8f-7d5b-44f5-a919-86735ba77857",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../models/phi_qna_finetuned_attempt_3/final_pretrained_2\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7435bc-f3f0-4bb2-b7a5-70a8e78120a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../models/phi_qna_finetuned_attempt_3/final_pretrained_2\",    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False\n",
    ")\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65dba5a1-b45d-4414-828f-2a226bb517a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, Thanks for writing in. I am Chat Doctor, infectious disease specialist, answering your query. I understand your concern. I would suggest you to get an ultrasound of the abdomen and kidneys to rule out any injury to the kidneys. If the ultrasound is normal, then you can take a course of antibiotics for 5 days. I hope this information helps you. Thank you for choosing Chat Doctor. I wish you a quick recovery. Best, Chans Doctor.<|endoftext|>"
     ]
    }
   ],
   "source": [
    "for token in stream_generate(model, tokenizer, prompt_template.format(question=examples[3]), do_sample=True, temperature=0.1, top_k=20, max_new_tokens=512):\n",
    "    print(token, end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
