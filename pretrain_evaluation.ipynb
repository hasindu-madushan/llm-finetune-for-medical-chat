{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4643151f-8b64-40cb-9b36-758e369e6407",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd5ae7-e718-4db9-9b4a-ff81be0e2457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install absl-py rouge-score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034bab24-dc8a-4b83-a570-29fcf3bdeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20bc9df7-5328-4945-a1bd-30f823fc8b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super_admin/hasindu/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import wandb\n",
    "import evaluate  # Hugging Face's evaluate library\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458060f-cfcf-4f51-a6aa-506cef1dc7c1",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb057781-55b7-494f-bcb4-dd7d00ba4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/phi_pubmed_pretrained_attempt_3/final\"\n",
    "\n",
    "data_path = \"../data/pubmed_baseline/\"\n",
    "test_data_path = data_path + \"pubmed_test.csv\"\n",
    "\n",
    "model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "max_len = 300\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38a3746-3c13-42cc-920a-2614eeede918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhasindumadushan325\u001b[0m (\u001b[33mhasindumadushan325-university-of-peradeniya\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/super_admin/hasindu/src/wandb/run-20250513_050625-88uu6er5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/pubmed-pretrain-evaluation/runs/88uu6er5' target=\"_blank\">base_model</a></strong> to <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/pubmed-pretrain-evaluation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/pubmed-pretrain-evaluation' target=\"_blank\">https://wandb.ai/hasindumadushan325-university-of-peradeniya/pubmed-pretrain-evaluation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/pubmed-pretrain-evaluation/runs/88uu6er5' target=\"_blank\">https://wandb.ai/hasindumadushan325-university-of-peradeniya/pubmed-pretrain-evaluation/runs/88uu6er5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hasindumadushan325-university-of-peradeniya/pubmed-pretrain-evaluation/runs/88uu6er5?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x77dd6008afb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"pubmed-pretrain-evaluation\", name=\"base_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f573ca-73a8-4ef6-af46-ecd2d84e6bad",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8443f97-858d-4220-b3d2-8f728fa48020",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c9145ea-6249-4c60-b086-14025a1ee4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(tokenizer, data_df):\n",
    "    dataset = Dataset.from_pandas(data_df)\n",
    "    def tokenize(example):\n",
    "        text = f\"<s>{example['title']}\\n{example['abstract']}</s>\"\n",
    "        return tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_len, return_attention_mask=True)\n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac602a77-0a3a-4d89-915a-b102625ebce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:01<00:00, 1337.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "test_set = tokenize_dataset(tokenizer, test_df.iloc[:2000, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60d571-b664-464b-a3b5-7ba96d0e8e22",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65cb96b6-5bd7-44d1-bd84-c6a107afddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# === Quantized model loading ===\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0b63bf7-fe1b-401f-b8c1-a394a65ed24d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(model, model_path)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0fd68d-392c-49c4-bc85-81c05bb4a07b",
   "metadata": {},
   "source": [
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ded167b3-4fc9-439c-8c3c-238d193c5a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a3294-c17a-403d-9fe7-626608f6f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./eval_output_base\",\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     do_eval=True,\n",
    "#     report_to=\"none\"\n",
    "# )\n",
    "\n",
    "# base_model_trainer = Trainer(\n",
    "#     model=base_model,\n",
    "#     args=training_args,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "# )\n",
    "\n",
    "# base_model_eval_result = base_model_trainer.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6480b4-2485-422f-a289-898f0ba4dbe9",
   "metadata": {},
   "source": [
    "# Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f852223-0fa4-49c8-9f81-e600cef32e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./eval_output\",\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",\n",
    "    eval_accumulation_steps=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d93a44e-cf37-4dc0-8171-3bde13eaca09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32551/2880446313.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load metrics\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\" Calculate bleu and rouge scores \"\"\"\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_preds\n",
    "        \n",
    "        logits = logits.cpu().numpy() if torch.is_tensor(logits) else logits\n",
    "        \n",
    "        labels = labels.cpu().numpy() if torch.is_tensor(labels) else labels\n",
    "        \n",
    "        # Get predicted token IDs (argmax of logits)\n",
    "        pred_ids = np.argmax(logits, axis=-1)  # Shape: (batch_size, seq_length)\n",
    "        \n",
    "        # Decode predictions and labels\n",
    "        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Replace -100 with pad_token_id in labels\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        references = [[ref] for ref in label_str]\n",
    "\n",
    "        \n",
    "        # Compute BLEU\n",
    "        bleu_result = bleu_metric.compute(\n",
    "            predictions=pred_str,\n",
    "            references=references\n",
    "        )\n",
    "        \n",
    "        # Compute ROUGE\n",
    "        rouge_result = rouge_metric.compute(\n",
    "            predictions=pred_str,\n",
    "            references=label_str,\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        \n",
    "        # Extract main scores\n",
    "        metrics = {\n",
    "            'bleu': bleu_result['bleu'],\n",
    "            'rouge1': rouge_result['rouge1'],\n",
    "            'rouge2': rouge_result['rouge2'],\n",
    "            'rougeL': rouge_result['rougeL'],\n",
    "        }\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Then when you evaluate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9d3989f-7cbb-47aa-8197-c2dab6692fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 14:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Evaluate perplexity ===\n",
    "eval_result = trainer.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ddcdbf5-5d13-49ac-b3a3-86e2986139ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "Loss: 1.8662\n",
      "Perplexity: 6.46\n",
      "BLEU: 0.1632\n",
      "ROUGE-1: 0.5243\n",
      "ROUGE-2: 0.2098\n",
      "ROUGE-L: 0.4334\n"
     ]
    }
   ],
   "source": [
    "# Print all results\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Loss: {eval_result['eval_loss']:.4f}\")\n",
    "print(f\"Perplexity: {torch.exp(torch.tensor(eval_result['eval_loss'])):.2f}\")\n",
    "print(f\"BLEU: {eval_result['eval_bleu']:.4f}\")\n",
    "print(f\"ROUGE-1: {eval_result['eval_rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {eval_result['eval_rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {eval_result['eval_rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92be58f3-423a-42b3-9bc4-ae8a46dc9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\n",
    "    \"eval_loss\": eval_result['eval_loss'], \n",
    "    \"perplexity\": torch.exp(torch.tensor(eval_result['eval_loss'])),\n",
    "    \"BLUE\": eval_result['eval_bleu'],\n",
    "    \"ROUGE_1\": eval_result['eval_rouge1'],\n",
    "    \"ROUGE_2\": eval_result['eval_rouge2'],\n",
    "    \"ROUGE_L\": eval_result['eval_rougeL']    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b6324-721a-40fd-94e8-260ae1f37420",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74594108-635c-4a63-ac8f-074d13a890cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actual: The elastase activity of alveolar macrophages: measurements using synthetic substrates and elastin.\n",
      "Hamster, rat, guinea pig, and rabbit alveolar macrophage extracts were tested for elastase activity using elastin suspended in agar and two synthetic substrates, p-nitrophenyl N-tert-butyloxycarbonyl L-alaninate (NBA) and succinyl-L-alanyl-L-alanyl-L-alanine-p-nitroanilide (SLAPN). Activity against NBA was easily detectable, but there was no activity against SLAPN or against elastin-agar, although the assay procedures employing these substrates measured as little as 50 and 2 ng of pancreatic elastase, respectively. We concluded that unstimulated alveolar macrophages from these animals do not contain elastase, and that NBA activity is misleading as an indicator of elastolytic activity in crude alveolar macrophage extracts from these species.\n",
      "---\n",
      "Generated: The elastase activity of alveolar macrophages: measurements using synthetic substrates and elastin.\n",
      "Hamster, rat, guinea pig, and rabbit alveolar macrophage extracts were tested for elastase activity using elastin suspended in agar and two synthetic substrates, p-nitrophenyl N-tert-butyloxycarbonyl L-alaninate (NBA) and succinyl-L-alanyl-L-alanyl-L-alanine-p-nitroanilide (SLAPN). Activity against NBA was easily detectable, but there was no activity against SLAPN or against elastin-agar, although the assay procedures employing these substrates measured as little as 50 and 2 ng of pancreatic elastase, respectively. We concluded that unstimulated alveolar macrophages from these animals do not contain elastase, and that NBA activity is misleading as an indicator of elastolytic activity in crude alveolar macrophage extracts from these species.</s>\n",
      "\n",
      "\n",
      "Actual: Hepatitis C virus infection in two groups of paediatric patients: one maintained on haemodialysis and the other on continuous ambulatory peritoneal dialysis.\n",
      "Seropositivity to hepatitis C virus (HCV) was evaluated in three groups of Saudi children. One group (n = 18) was maintained on haemodialysis and another group (n = 21) on continuous ambulatory peritoneal dialysis (CAPD). The third group were community-based normal controls. The prevalence of antibody to HCV (anti-HCV) in children on haemodialysis (11.2%) was significantly higher than that in the control group (n = 220) (1.4%; p = 0.056). There was no significant difference in the prevalence of anti-HCV between children on CAPD (4.8%) and the control group (1.4%; p = 0.244). Among ten children on haemodialysis who were anti-HCV-negative 4 years earlier, two seroconverted and the seroconversion was not due to transfused blood but was most likely due to environmental contamination. This is the first report on the prevalence of anti-HCV in children maintained on CAPD. The results of the study emphasize the need for separate dialysis machines for anti-HCV-positive patients. It seems that CAPD therapy might reduce transmission of HCV but a large number of CAPD patients will need to be studied to confirm superiority to haemodialysis in this respect.\n",
      "---\n",
      "Generated: Hepatitis C virus infection in two groups of paediatric patients: one maintained on haemodialysis and the other on continuous ambulatory peritoneal dialysis.\n",
      "Seropositivity to hepatitis C virus (HCV) was evaluated in three groups of Saudi children. One group (n = 18) was maintained on haemodialysis and another group (n = 21) on continuous ambulatory peritoneal dialysis (CAPD). The third group were community-based normal controls. The prevalence of antibody to HCV (anti-HCV) in children on haemodialysis (11.2%) was significantly higher than that in the control group (n = 220) (1.4%; p = 0.056). There was no significant difference in the prevalence of anti-HCV between children on CAPD (4.8%) and the control group (1.4%; p = 0.244). Among ten children on haemodialysis who were anti-HCV-negative 4 years earlier, two seroconverted and the seroconversion was not due to transfused blood but was most likely due to environmental contamination. This is the first report on the prevalence of anti-HCV in children maintained on CAPD.\n",
      "Keywords: Hepatitis C virus, haemodialysis, peritoneal dialysis, children\n",
      "Hepatitis C virus (HCV) infection is a major public health problem worldwide. The prevalence of HCV infection in the general population varies from 0.5% to 3.5% [1]. The prevalence of HCV infection in children is low, ranging from 0.01% to 0.05% [2]. The prevalence of HCV infection in children on haemodial\n",
      "\n",
      "\n",
      "Actual: Vitrectomy in uveitis associated with ankylosing spondylitis.\n",
      "Chronic recurrent iridocyclitis in three eyes of two patients with ankylosing spondylitis was associated with posterior spillover of inflammatory cells into the vitreous cavity. Continued inflammation resulted in significant vitreous opacification in all three eyes. After pars plana vitrectomy (two eyes) and cataract extraction with subtotal vitrectomy (one eye), visual acuity improved and stabilized in all three instances. Ocular inflammation was not appreciably exacerbated by surgical intervention. Vitreous opacification did not recur after vitrectomy, but visual improvement was limited because of chronic cystoid macular edema.\n",
      "---\n",
      "Generated: Vitrectomy in uveitis associated with ankylosing spondylitis.\n",
      "Chronic recurrent iridocyclitis in three eyes of two patients with ankylosing spondylitis was associated with posterior spillover of inflammatory cells into the vitreous cavity. Continued inflammation resulted in significant vitreous opacification in all three eyes. After pars plana vitrectomy (two eyes) and cataract extraction with subtotal vitrectomy (one eye), visual acuity improved and stabilized in all three instances. Ocular inflammation was not appreciably exacerbated by surgical intervention. Vitreous opacification did not recur after vitrectomy, but visual improvement was limited because of chronic cystoid macular edema.</s> **Question:** What was the outcome of vitrectomy in patients with uveitis associated with ankylosing spondylitis?\n",
      "\n",
      "**Answer:** Visual acuity improved and stabilized after vitrectomy, but chronic cystoid macular edema limited visual improvement.\n",
      "\n",
      "\n",
      "### Instruction \n",
      "\n",
      "In a study involving patients with ankylosing spondylitis, chronic recurrent iridocyclitis led to vitreous opacification. Following vitrectomy and cataract extraction, the patients experienced improved and stabilized visual acuity.\n",
      "\n",
      "\n",
      "Actual: Ductal carcinoma of the pancreas. Rationales for total pancreatectomy.\n",
      "In order to evaluate total pancreatectomy as a surgical procedure for ductal carcinoma of the pancreas, a histopathological analysis was made on 18 resected specimens with special regard to the pattern of cancer growth in the pancreatic tissue. In five of them there was no lymphatic involvement or extrapancreatic invasion, but cancer extended continuously to the tail along with the pancreatic ducts and reached the end of the ducts in three cases. All 11 patients treated with Whipple's procedure died of recurrence, while of four total pancreatectomized patients, one with continuously invasive cancer to the end of the pancreatic duct has been living more than eight years postoperatively. We believe that total pancreatectomy for this type of \"intraductal spreading cancer\" without invasion beyond the pancreas is indicated as a radical procedure.\n",
      "---\n",
      "Generated: Ductal carcinoma of the pancreas. Rationales for total pancreatectomy.\n",
      "In order to evaluate total pancreatectomy as a surgical procedure for ductal carcinoma of the pancreas, a histopathological analysis was made on 18 resected specimens with special regard to the pattern of cancer growth in the pancreatic tissue. In five of them there was no lymphatic involvement or extrapancreatic invasion, but cancer extended continuously to the tail along with the pancreatic ducts and reached the end of the ducts in three cases. All 11 patients treated with Whipple's procedure died of recurrence, while of four total pancreatectomized patients, one with continuously invasive cancer to the end of the pancreatic duct has been living more than eight years postoperatively. We believe that total pancreatectomy for this type of \"intraductal spreading cancer\" without invasion beyond the pancreas is indicated as a radical procedure.</s>\n",
      "\n",
      "\n",
      "Actual: Fine-needle aspiration of breast cancer. Relationship of clinical factors to cytology results in 689 primary malignancies.\n",
      "Between 1980 and 1983, 689 women with primary breast cancer at the Royal Infirmary of Edinburgh and associated Hospitals had fine-needle aspiration biopsies prior to definitive surgery. Clinical factors relating to the success of these aspirations were evaluated. The most significant factor was which physician performed the aspiration. Size of the lesion was also an important variable; however, size difference could not account for the marked variation between different individuals performing the aspiration. There was no difference between node-positive and node-negative patients when matched by tumor size. A significantly lower rate of positive aspirations occurred in the 45 to 55 year age group which could not be accounted for by tumor size. Location of the mass was not significant, although there was a persistent lower rate of positive aspirates from the upper inner quadrants. Aspiration cytology was positive or suspicious in 65 percent of patients with primary breast cancer who had clinical diagnoses of benign breast lesions. It is concluded that the most significant variable in the accuracy of breast aspiration biopsy is the size of the lesion and the proficiency of the individual performing the procedure. With a skilled physician, positive aspiration results were obtained in over 80% of breast cancers.\n",
      "---\n",
      "Generated: Fine-needle aspiration of breast cancer. Relationship of clinical factors to cytology results in 689 primary malignancies.\n",
      "Between 1980 and 1983, 689 women with primary breast cancer at the Royal Infirmary of Edinburgh and associated Hospitals had fine-needle aspiration biopsies prior to definitive surgery. Clinical factors relating to the success of these aspirations were evaluated. The most significant factor was which physician performed the aspiration. Size of the lesion was also an important variable; however, size difference could not account for the marked variation between different individuals performing the aspiration. There was no difference between node-positive and node-negative patients when matched by tumor size. A significantly lower rate of positive aspirations occurred in the 45 to 55 year age group which could not be accounted for by tumor size. Location of the mass was not significant, although there was a persistent lower rate of positive aspirates from the upper inner quadrants. Aspiration cytology was positive or suspicious in 65 percent of patients with primary breast cancer who had clinical diagnoses of benign breast lesions. It is concluded that the most significant variable in the accuracy of breast aspiration biopsy is the size of the lesion and the proficiency of the individual performing the aspiration.\n",
      "Citation: McLaughlin JA, McLaughlin JA, McLaughlin JA (1985) Fine-needle aspiration of breast cancer. Relationship of clinical factors to cytology results in 689 primary malignancies. J Clin Oncol 3: 1077-1083.\n",
      "https://doi.org/10.1200/JCO.1985.3.6.1077\n",
      "A cytologic diagnosis of breast\n",
      "\n",
      "\n",
      "Actual: Multicenter evaluation of a new commercial assay for detection of immunoglobulin M antibodies to Toxoplasma gondii. Multicenter Study Group.\n",
      "A new commercial assay for detection of IgM-specific antibodies to Toxoplasma gondii (IMx Toxo IgM, Abbott, USA), based on microparticle enzyme immunoassay technology, was evaluated at 15 clinical sites in Europe and the USA. Performance characteristics were established by testing clinical specimens collected randomly from pregnant women, blood donors, individuals with suspected Toxoplasma gondii infection and individuals confirmed HIV positive. Reference testing was performed using Toxo-M EIA (Abbott). Specimens evaluated at European sites yielding discordant results between the new assay and the reference EIA were further tested with an immunosorbent agglutination assay; at sites in the USA, discordant results were resolved using Platelia Toxo IgM (Sanofi, France) and Vidas Toxo IgM (bioMérieux, France) assays. In addition, matched plasma and serum, heat-treated and non-heat-treated specimens, and fresh and frozen specimens were evaluated at the USA sites. At European sites the new commercial assay had a sensitivity of 95.6% (196/205), a specificity of 99.8% (3,137/3,143) and an agreement of 99.6% (3,333/3,348) following resolution of discordant results; sensitivity in the USA was 97.4% (184/189), specificity was 99.8% (1,204/1,207) and agreement was 99.4% (1,388/1,396) following resolution. The new IMx Toxo IgM is a sensitive and specific assay for measurement of IgM antibodies to Toxoplasma gondii in human serum and plasma.\n",
      "---\n",
      "Generated: Multicenter evaluation of a new commercial assay for detection of immunoglobulin M antibodies to Toxoplasma gondii. Multicenter Study Group.\n",
      "A new commercial assay for detection of IgM-specific antibodies to Toxoplasma gondii (IMx Toxo IgM, Abbott, USA), based on microparticle enzyme immunoassay technology, was evaluated at 15 clinical sites in Europe and the USA. Performance characteristics were established by testing clinical specimens collected randomly from pregnant women, blood donors, individuals with suspected Toxoplasma gondii infection and individuals confirmed HIV positive. Reference testing was performed using Toxo-M EIA (Abbott). Specimens evaluated at European sites yielding discordant results between the new assay and the reference EIA were further tested with an immunosorbent agglutination assay; at sites in the USA, discordant results were resolved using Platelia Toxo IgM (Sanofi, France) and Vidas Toxo IgM (bioMérieux, France) assays. In addition, matched plasma and serum, heat-treated and non-heat-treated specimens, and fresh and frozen specimens were evaluated at the USA sites. At European sites, the new assay had a sensitivity of 97.5% (95% CI 95.6-98.4) and a specificity of 99.8% (95% CI 99.6-100) for detection of IgM-specific antibodies to Toxoplasma gondii. At USA sites, the sensitivity was 99.2% (95% CI 98.2-99.9) and the specificity was 99.9% (95% CI 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = test_set.select(range(433, 439))  # First 5 examples\n",
    "input_ids = torch.tensor(samples[\"input_ids\"]).to(model.device)\n",
    "attention_mask = torch.tensor(samples[\"attention_mask\"]).to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "# Log predictions to W&B\n",
    "wandb_table = wandb.Table(columns=[\"Title\", \"Actual Abstract\", \"Generated Text\"])\n",
    "for i, gen in enumerate(generated_texts):\n",
    "    title = samples[i][\"title\"]\n",
    "    actual = samples[i][\"abstract\"]\n",
    "    print(f\"\\nActual: {title}\\n{actual}\\n---\\nGenerated: {gen}\\n\")\n",
    "    wandb_table.add_data(title, actual, gen)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1035ed31-1dea-4631-8d66-797ce87c0a74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pmid': 1247212,\n",
       " 'title': 'The elastase activity of alveolar macrophages: measurements using synthetic substrates and elastin.',\n",
       " 'abstract': 'Hamster, rat, guinea pig, and rabbit alveolar macrophage extracts were tested for elastase activity using elastin suspended in agar and two synthetic substrates, p-nitrophenyl N-tert-butyloxycarbonyl L-alaninate (NBA) and succinyl-L-alanyl-L-alanyl-L-alanine-p-nitroanilide (SLAPN). Activity against NBA was easily detectable, but there was no activity against SLAPN or against elastin-agar, although the assay procedures employing these substrates measured as little as 50 and 2 ng of pancreatic elastase, respectively. We concluded that unstimulated alveolar macrophages from these animals do not contain elastase, and that NBA activity is misleading as an indicator of elastolytic activity in crude alveolar macrophage extracts from these species.',\n",
       " 'input_ids': [32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  1,\n",
       "  450,\n",
       "  560,\n",
       "  579,\n",
       "  559,\n",
       "  6354,\n",
       "  310,\n",
       "  394,\n",
       "  345,\n",
       "  10170,\n",
       "  11758,\n",
       "  561,\n",
       "  1179,\n",
       "  29901,\n",
       "  20398,\n",
       "  773,\n",
       "  14710,\n",
       "  7492,\n",
       "  25148,\n",
       "  1078,\n",
       "  322,\n",
       "  560,\n",
       "  579,\n",
       "  262,\n",
       "  29889,\n",
       "  13,\n",
       "  29950,\n",
       "  314,\n",
       "  2475,\n",
       "  29892,\n",
       "  7548,\n",
       "  29892,\n",
       "  1410,\n",
       "  16736,\n",
       "  282,\n",
       "  335,\n",
       "  29892,\n",
       "  322,\n",
       "  27127,\n",
       "  277,\n",
       "  394,\n",
       "  345,\n",
       "  10170,\n",
       "  11758,\n",
       "  561,\n",
       "  482,\n",
       "  6597,\n",
       "  29879,\n",
       "  892,\n",
       "  9528,\n",
       "  363,\n",
       "  560,\n",
       "  579,\n",
       "  559,\n",
       "  6354,\n",
       "  773,\n",
       "  560,\n",
       "  579,\n",
       "  262,\n",
       "  8872,\n",
       "  2760,\n",
       "  297,\n",
       "  946,\n",
       "  279,\n",
       "  322,\n",
       "  1023,\n",
       "  14710,\n",
       "  7492,\n",
       "  25148,\n",
       "  1078,\n",
       "  29892,\n",
       "  282,\n",
       "  29899,\n",
       "  26129,\n",
       "  307,\n",
       "  9789,\n",
       "  2904,\n",
       "  405,\n",
       "  29899,\n",
       "  357,\n",
       "  29873,\n",
       "  29899,\n",
       "  4187,\n",
       "  29891,\n",
       "  417,\n",
       "  3594,\n",
       "  4287,\n",
       "  29890,\n",
       "  2592,\n",
       "  29880,\n",
       "  365,\n",
       "  29899,\n",
       "  284,\n",
       "  273,\n",
       "  16976,\n",
       "  313,\n",
       "  29940,\n",
       "  5688,\n",
       "  29897,\n",
       "  322,\n",
       "  8348,\n",
       "  262,\n",
       "  2904,\n",
       "  29899,\n",
       "  29931,\n",
       "  29899,\n",
       "  284,\n",
       "  1384,\n",
       "  29880,\n",
       "  29899,\n",
       "  29931,\n",
       "  29899,\n",
       "  284,\n",
       "  1384,\n",
       "  29880,\n",
       "  29899,\n",
       "  29931,\n",
       "  29899,\n",
       "  284,\n",
       "  273,\n",
       "  457,\n",
       "  29899,\n",
       "  29886,\n",
       "  29899,\n",
       "  26129,\n",
       "  307,\n",
       "  273,\n",
       "  309,\n",
       "  680,\n",
       "  313,\n",
       "  12750,\n",
       "  3301,\n",
       "  29940,\n",
       "  467,\n",
       "  13414,\n",
       "  2750,\n",
       "  21517,\n",
       "  471,\n",
       "  5948,\n",
       "  6459,\n",
       "  519,\n",
       "  29892,\n",
       "  541,\n",
       "  727,\n",
       "  471,\n",
       "  694,\n",
       "  6354,\n",
       "  2750,\n",
       "  27146,\n",
       "  3301,\n",
       "  29940,\n",
       "  470,\n",
       "  2750,\n",
       "  560,\n",
       "  579,\n",
       "  262,\n",
       "  29899,\n",
       "  28641,\n",
       "  29892,\n",
       "  5998,\n",
       "  278,\n",
       "  1223,\n",
       "  388,\n",
       "  28648,\n",
       "  5703,\n",
       "  292,\n",
       "  1438,\n",
       "  25148,\n",
       "  1078,\n",
       "  17005,\n",
       "  408,\n",
       "  2217,\n",
       "  408,\n",
       "  29871,\n",
       "  29945,\n",
       "  29900,\n",
       "  322,\n",
       "  29871,\n",
       "  29906,\n",
       "  8736,\n",
       "  310,\n",
       "  7243,\n",
       "  1037,\n",
       "  2454,\n",
       "  560,\n",
       "  579,\n",
       "  559,\n",
       "  29892,\n",
       "  8307,\n",
       "  29889,\n",
       "  1334,\n",
       "  22834,\n",
       "  393,\n",
       "  443,\n",
       "  303,\n",
       "  326,\n",
       "  7964,\n",
       "  394,\n",
       "  345,\n",
       "  10170,\n",
       "  11758,\n",
       "  561,\n",
       "  1179,\n",
       "  515,\n",
       "  1438,\n",
       "  15006,\n",
       "  437,\n",
       "  451,\n",
       "  1712,\n",
       "  560,\n",
       "  579,\n",
       "  559,\n",
       "  29892,\n",
       "  322,\n",
       "  393,\n",
       "  21517,\n",
       "  6354,\n",
       "  338,\n",
       "  3984,\n",
       "  25369,\n",
       "  408,\n",
       "  385,\n",
       "  27717,\n",
       "  310,\n",
       "  560,\n",
       "  579,\n",
       "  324,\n",
       "  3637,\n",
       "  293,\n",
       "  6354,\n",
       "  297,\n",
       "  7618,\n",
       "  311,\n",
       "  394,\n",
       "  345,\n",
       "  10170,\n",
       "  11758,\n",
       "  561,\n",
       "  482,\n",
       "  6597,\n",
       "  29879,\n",
       "  515,\n",
       "  1438,\n",
       "  6606,\n",
       "  29889,\n",
       "  2],\n",
       " 'attention_mask': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7f60d22-2210-4463-bee0-814c84f988dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"generated_examples\": wandb_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14185bf3-943b-4a05-b9e0-649841a4723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, text, max_new_tokens=128):\n",
    "    sample = tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_len, return_attention_mask=True)\n",
    "    input_ids = torch.tensor([sample[\"input_ids\"]]).to(model.device)\n",
    "    attention_mask = torch.tensor([sample[\"attention_mask\"]]).to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        use_cache=False\n",
    "    )\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return generated_texts[0]\n",
    "    # # Log predictions to W&B\n",
    "    # for i, gen in enumerate(generated_texts):\n",
    "    #     title = samples[i][\"title\"]\n",
    "    #     actual = samples[i][\"abstract\"]\n",
    "    #     print(f\"\\nTitle: {title}\\n---\\nActual Abstract: {actual}\\n---\\nGenerated: {gen}\\n\")\n",
    "    #     wandb_table.add_data(title, actual, gen)\n",
    "    \n",
    "    \n",
    "    # wandb.log({\"generated_examples\": wandb_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2f748a1-4da5-439e-911d-0c76f22bbedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The relationship between diabetes and blood pressure\\n\\nDiabetes and high blood pressure often occur together, and managing both conditions is crucial for reducing the risk of complications. Here's how they are related:\\n\\n1. **Insulin Resistance**: Insulin resistance, a hallmark of type 2 diabetes, can lead to increased blood pressure. Insulin resistance can cause the body to retain sodium, which increases blood volume and, consequently, blood pressure.\\n\\n2. **Kidney Damage**: Diabetes can damage the blood vessels in the kidneys, impairing their ability\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text = generate(model, \"The relationship between diabetes and blood pressure\\n\")\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e867378a-e7b3-4674-ba2a-0adccf82ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"example_1\": generated_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726ad10-f44e-4381-92ea-1705858dab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(base_model, \"# The relationship between diabetes and blood pressure\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
