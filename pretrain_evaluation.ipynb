{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4643151f-8b64-40cb-9b36-758e369e6407",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34c2ec3-5430-4b61-b412-2c2e3ad2c057",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    Trainer, TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458060f-cfcf-4f51-a6aa-506cef1dc7c1",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb057781-55b7-494f-bcb4-dd7d00ba4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/phi_pubmed_pretrained_attempt_1/final\"\n",
    "\n",
    "data_path = \"../data/pubmed_baseline/\"\n",
    "test_data_path = data_path + \"pubmed_test.csv\"\n",
    "\n",
    "max_length = 512\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a3746-3c13-42cc-920a-2614eeede918",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"pubmed-pretrain-evaluation\", name=\"attempt_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f573ca-73a8-4ef6-af46-ecd2d84e6bad",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8443f97-858d-4220-b3d2-8f728fa48020",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9145ea-6249-4c60-b086-14025a1ee4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(tokenizer, data_df):\n",
    "    dataset = Dataset.from_pandas(data_df)\n",
    "    def tokenize(example):\n",
    "        text = f\"<s>#{example['title']}\\n{example['abstract']}</s>\"\n",
    "        return tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_len, return_attention_mask=True)\n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac602a77-0a3a-4d89-915a-b102625ebce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "test_set = tokenize_dataset(tokenizer, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6480b4-2485-422f-a289-898f0ba4dbe9",
   "metadata": {},
   "source": [
    "# Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f852223-0fa4-49c8-9f81-e600cef32e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./eval_output\",\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, padding=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d3989f-7cbb-47aa-8197-c2dab6692fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluate perplexity ===\n",
    "eval_result = trainer.evaluate(tokenized_test)\n",
    "loss = eval_result[\"eval_loss\"]\n",
    "perplexity = torch.exp(torch.tensor(loss))\n",
    "\n",
    "print(f\"\\nâœ… Evaluation Metrics:\")\n",
    "print(f\"Eval Loss     : {loss:.4f}\")\n",
    "print(f\"Eval Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be58f3-423a-42b3-9bc4-ae8a46dc9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"eval_loss\": loss, \"eval_perplexity\": perplexity.item()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b6324-721a-40fd-94e8-260ae1f37420",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74594108-635c-4a63-ac8f-074d13a890cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = tokenized_test.select(range(5))  # First 5 examples\n",
    "input_ids = torch.tensor(samples[\"input_ids\"]).to(model.device)\n",
    "attention_mask = torch.tensor(samples[\"attention_mask\"]).to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "# Log predictions to W&B\n",
    "wandb_table = wandb.Table(columns=[\"Title\", \"Actual Abstract\", \"Generated Text\"])\n",
    "for i, gen in enumerate(generated_texts):\n",
    "    title = samples[i][\"title\"]\n",
    "    actual = samples[i][\"abstract\"]\n",
    "    print(f\"\\nTitle: {title}\\n---\\nActual Abstract: {actual}\\n---\\nGenerated: {gen}\\n\")\n",
    "    if use_wandb:\n",
    "        wandb_table.add_data(title, actual, gen)\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.log({\"generated_examples\": wandb_table})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
