{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4643151f-8b64-40cb-9b36-758e369e6407",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20bc9df7-5328-4945-a1bd-30f823fc8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458060f-cfcf-4f51-a6aa-506cef1dc7c1",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb057781-55b7-494f-bcb4-dd7d00ba4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/phi_pubmed_pretrained_attempt_1/final\"\n",
    "\n",
    "data_path = \"../data/pubmed_baseline/\"\n",
    "test_data_path = data_path + \"pubmed_test.csv\"\n",
    "\n",
    "max_len = 300\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a38a3746-3c13-42cc-920a-2614eeede918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_perplexity</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_loss</td><td>1.63262</td></tr><tr><td>eval_perplexity</td><td>5.11726</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">attempt_2</strong> at: <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/pubmed-pretrain-evaluation/runs/xgdogy0f' target=\"_blank\">https://wandb.ai/hasindumadushan325-university-of-peradeniya/pubmed-pretrain-evaluation/runs/xgdogy0f</a><br> View project at: <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/pubmed-pretrain-evaluation' target=\"_blank\">https://wandb.ai/hasindumadushan325-university-of-peradeniya/pubmed-pretrain-evaluation</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250511_074844-xgdogy0f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/super_admin/hasindu/src/wandb/run-20250511_080031-yedfhxys</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/pubmed-pretrain-evaluation/runs/yedfhxys' target=\"_blank\">attempt_2</a></strong> to <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/pubmed-pretrain-evaluation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/pubmed-pretrain-evaluation' target=\"_blank\">https://wandb.ai/hasindumadushan325-university-of-peradeniya/pubmed-pretrain-evaluation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/pubmed-pretrain-evaluation/runs/yedfhxys' target=\"_blank\">https://wandb.ai/hasindumadushan325-university-of-peradeniya/pubmed-pretrain-evaluation/runs/yedfhxys</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hasindumadushan325-university-of-peradeniya/pubmed-pretrain-evaluation/runs/yedfhxys?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7c5778240310>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"pubmed-pretrain-evaluation\", name=\"attempt_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f573ca-73a8-4ef6-af46-ecd2d84e6bad",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8443f97-858d-4220-b3d2-8f728fa48020",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c9145ea-6249-4c60-b086-14025a1ee4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(tokenizer, data_df):\n",
    "    dataset = Dataset.from_pandas(data_df)\n",
    "    def tokenize(example):\n",
    "        text = f\"<s>#{example['title']}\\n{example['abstract']}</s>\"\n",
    "        return tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_len, return_attention_mask=True)\n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac602a77-0a3a-4d89-915a-b102625ebce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3000/3000 [00:02<00:00, 1279.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "test_set = tokenize_dataset(tokenizer, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60d571-b664-464b-a3b5-7ba96d0e8e22",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e73e1b6-b9b7-47b2-ab53-25f21c24d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = PeftConfig.from_pretrained(model_path)\n",
    "base_model_name = peft_config.base_model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4da75b58-9eeb-4020-936f-f1cf43d25c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'microsoft/Phi-3.5-mini-instruct'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65cb96b6-5bd7-44d1-bd84-c6a107afddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# === Quantized model loading ===\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0b63bf7-fe1b-401f-b8c1-a394a65ed24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3Attention(\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "              (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "              (activation_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm()\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_attention_layernorm): Phi3RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(model, model_path)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6480b4-2485-422f-a289-898f0ba4dbe9",
   "metadata": {},
   "source": [
    "# Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f852223-0fa4-49c8-9f81-e600cef32e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11644/2486791516.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./eval_output\",\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9d3989f-7cbb-47aa-8197-c2dab6692fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94/94 01:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Evaluate perplexity ===\n",
    "eval_result = trainer.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b6789b2-218d-4bb2-b521-fda9c0ca4a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Evaluation Metrics:\n",
      "Eval Loss     : 1.6326\n",
      "Eval Perplexity: 5.12\n"
     ]
    }
   ],
   "source": [
    "loss = eval_result[\"eval_loss\"]\n",
    "perplexity = torch.exp(torch.tensor(loss))\n",
    "\n",
    "print(f\"\\n✅ Evaluation Metrics:\")\n",
    "print(f\"Eval Loss     : {loss:.4f}\")\n",
    "print(f\"Eval Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92be58f3-423a-42b3-9bc4-ae8a46dc9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"eval_loss\": loss, \"eval_perplexity\": perplexity.item()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b6324-721a-40fd-94e8-260ae1f37420",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e55d60c7-275b-48f6-b682-b5f4b99783b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.51.3\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /home/super_admin/hasindu/myenv/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: peft\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a118bc30-9ed3-468e-8958-8cc9195b1bf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.40\n",
      "  Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: safetensors>=0.4.1 in /home/super_admin/hasindu/myenv/lib/python3.10/site-packages (from transformers==4.40) (0.5.3)\n",
      "Requirement already satisfied: requests in /home/super_admin/hasindu/myenv/lib/python3.10/site-packages (from transformers==4.40) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/super_admin/hasindu/myenv/lib/python3.10/site-packages (from transformers==4.40) (4.67.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/super_admin/hasindu/myenv/lib/python3.10/site-packages (from transformers==4.40) (2024.11.6)\n",
      "Requirement already satisfied: filelock in /home/super_admin/hasindu/myenv/lib/python3.10/site-packages (from transformers==4.40) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/super_admin/hasindu/myenv/lib/python3.10/site-packages (from transformers==4.40) (6.0.2)\n",
      "Collecting tokenizers<0.20,>=0.19\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/super_admin/hasindu/myenv/lib/python3.10/site-packages (from transformers==4.40) (25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/super_admin/hasindu/myenv/lib/python3.10/site-packages (from transformers==4.40) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/super_admin/hasindu/myenv/lib/python3.10/site-packages (from transformers==4.40) (0.31.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/super_admin/hasindu/myenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40) (4.13.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/super_admin/hasindu/myenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40) (2024.6.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /home/super_admin/hasindu/myenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40) (1.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/super_admin/hasindu/myenv/lib/python3.10/site-packages (from requests->transformers==4.40) (2025.4.26)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/super_admin/hasindu/myenv/lib/python3.10/site-packages (from requests->transformers==4.40) (2.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/super_admin/hasindu/myenv/lib/python3.10/site-packages (from requests->transformers==4.40) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/super_admin/hasindu/myenv/lib/python3.10/site-packages (from requests->transformers==4.40) (3.4.2)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.1\n",
      "    Uninstalling tokenizers-0.21.1:\n",
      "      Successfully uninstalled tokenizers-0.21.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.51.3\n",
      "    Uninstalling transformers-4.51.3:\n",
      "      Successfully uninstalled transformers-4.51.3\n",
      "Successfully installed tokenizers-0.19.1 transformers-4.40.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74594108-635c-4a63-ac8f-074d13a890cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title: Doxorubicin-vincristine therapy for Wilms' tumor: a pilot study.\n",
      "---\n",
      "Actual Abstract: Doxorubicin plus vincristine chemotherapy was given to 31 children following nephrectomy for Wilms' tumor. Radiation therapy was used as indicated. Disease-free survival by stage is: eight of nine patients (stage I), eight of nine (stage II), nine of ten (stage III), and two of three (stage IV). Median follow-up of survivors is 28 months (range, 2-67); for all but four patients, follow-up is greater than 12 months. Two of the three stage I-III failures occurred in children with unfavorable histologies; the third failure was due to fatal anthracycline cardiomyopathy. Lowering the maximal cumulative doxorubicin dose from 450 to 240 mg/m2 did not increase failures. Doxorubicin-vincristine appears to be effective chemotherapy for Wilms' tumor.\n",
      "---\n",
      "Generated: #Doxorubicin-vincristine therapy for Wilms' tumor: a pilot study.\n",
      "Doxorubicin plus vincristine chemotherapy was given to 31 children following nephrectomy for Wilms' tumor. Radiation therapy was used as indicated. Disease-free survival by stage is: eight of nine patients (stage I), eight of nine (stage II), nine of ten (stage III), and two of three (stage IV). Median follow-up of survivors is 28 months (range, 2-67); for all but four patients, follow-up is greater than 12 months. Two of the three stage I-III failures occurred in children with unfavorable histologies; the third failure was due to fatal anthracycline cardiomyopathy. Lowering the maximal cumulative doxorubicin dose from 450 to 240 mg/m2 did not increase failures. Doxorubicin-vincristine appears to be effective chemotherapy for Wilms' tumor.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "\n",
      "\n",
      "Title: Decision-making capacity for informed consent in the older population.\n",
      "---\n",
      "Actual Abstract: We discuss key concepts and review 12 published research studies relevant to informed consent and decision-making capacity in the older population. The literature suggests that aging is associated with impaired decision-making capacity; the following additional factors amplify the detrimental effect of aging: lower vocabulary level, lower educational level, chronic medical illness (as in nursing home residents), and acute medical illness. Aging may be associated particularly with impaired comprehension of consent forms. We discuss guidelines for clinicians and researchers for improving the process of obtaining a truly informed consent.\n",
      "---\n",
      "Generated: #Decision-making capacity for informed consent in the older population.\n",
      "We discuss key concepts and review 12 published research studies relevant to informed consent and decision-making capacity in the older population. The literature suggests that aging is associated with impaired decision-making capacity; the following additional factors amplify the detrimental effect of aging: lower vocabulary level, lower educational level, chronic medical illness (as in nursing home residents), and acute medical illness. Aging may be associated particularly with impaired comprehension of consent forms. We discuss guidelines for clinicians and researchers for improving the process of obtaining a truly informed consent.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "\n",
      "\n",
      "Title: Intrahepatic HBVDNA replication and gene products expression correlated with foci of hepatic necrosis.\n",
      "---\n",
      "Actual Abstract: Intrahepatic HBVDNA of patients with chronic hepatitis B was detected by in situ cytohybridization assay in combination with demonstration of HBsAg and HBcAg expression, and correlated with the foci of hepatic necrosis. It was found that HBsAg and HBcAg expressing sites appeared in the same areas where HBVDNA replicated; when HBsAg and HBcAg simultaneously expressed in the liver, one antigen usually dominated, the other declined. This unbalanced expression may be caused by the gene or transcripts variance which encodes the surface or core antigen, or by the activity of each replication. The other observation is that HBVDNA-positive hepatocytes outnumbered liver cells containing HBsAg and HBcAg in a few cases. It is supposed that a part of cytoplasmic HBVDNA was on replicative phase with gene production expression, on the contrary, another part of cytoplasmic HBVDNA may be out of replication with no gene product expression during chronic hepatitis B. Finally HBsAg and HBcAg expressing sites corresponding to HBVDNA-positive hepatocytes were closely attached to the foci of hepatic necrosis. There was no significant difference in the frequency of HBsAg and HBcAg expression correlating to hepatic necrosis.\n",
      "---\n",
      "Generated: #Intrahepatic HBVDNA replication and gene products expression correlated with foci of hepatic necrosis.\n",
      "Intrahepatic HBVDNA of patients with chronic hepatitis B was detected by in situ cytohybridization assay in combination with demonstration of HBsAg and HBcAg expression, and correlated with the foci of hepatic necrosis. It was found that HBsAg and HBcAg expressing sites appeared in the same areas where HBVDNA replicated; when HBsAg and HBcAg simultaneously expressed in the liver, one antigen usually dominated, the other declined. This unbalanced expression may be caused by the gene or transcripts variance which encodes the surface or core antigen, or by the activity of each replication. The other observation is that HBVDNA-positive hepatocytes outnumbered liver cells containing HBsAg and HBcAg in a few cases. It is supposed that a part of cytoplasmic HBVDNA was on replicative phase with gene production expression, on the contrary, another part of cytoplasmic HBVDNA may be out of replication with no gene product expression during chronic hepatitis B. Finally HBsAg and HBcAg expressing sites corresponding to HBVDNA-positive hepatocytes were found in the same areas of liver.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "\n",
      "\n",
      "Title: Protein synthesis in postnuclear supernatants from mengovirus-infected Ehrlich ascites tumor cells.\n",
      "---\n",
      "Actual Abstract: The effect of mengovirus infection on the protein synthetic capacity of Ehrlich ascites tumor cells cultured in vitro was studied in vivo and in vitro employing postnuclear supernatants prepared at various times post-infection in the absence and in the presence of 1% Triton X-100. The amino acid incorporating activities of extracts obtained in the presence of the detergent were reduced by about 30% compared with the capacities of the corresponding postnuclear supernatants prepared in the absence of Triton X-100; but the course of the activity vs. time curve was not influenced by the detergent. Under the conditions employed, the postnuclear supernatants were unable to reinitiate protein synthesis once elongation of nascent polypeptide chains concomitant with ribosome runoff was completed. After mengovirus infection, a gradual disappearance of polysomes from postnuclear supernatants and a simultaneous accumulation of monosomes was observed. The protein-synthesizing activities of normal and infected cells were inversely proportional to the monosome concentrations of their corresponding extracts. Qualitatively, protein synthesis in intact cells and in postnuclear supernatants responded similarly to mengovirus infection. In both cases an initial reduction of host-specific amino acid incorporation was followed by a burst of viral protein synthesis. However, the two activity vs. time curves showed the following significant differences: 1) The activities of extracts from control cells and from mengovirus-infected cells nearly in the infectious cycle were low compared with the activities observed in vivo. 2) In the middle of the infectious cycle, the peak of viral protein synthesis occurred later and the activity was higher in vitro. 3) Finally, in the late period of the infectious cycle the postnuclear supernatants had considerable protein synthesizing activity, at a time when protein synthesis in vivo was nil.\n",
      "---\n",
      "Generated: #Protein synthesis in postnuclear supernatants from mengovirus-infected Ehrlich ascites tumor cells.\n",
      "The effect of mengovirus infection on the protein synthetic capacity of Ehrlich ascites tumor cells cultured in vitro was studied in vivo and in vitro employing postnuclear supernatants prepared at various times post-infection in the absence and in the presence of 1% Triton X-100. The amino acid incorporating activities of extracts obtained in the presence of the detergent were reduced by about 30% compared with the capacities of the corresponding postnuclear supernatants prepared in the absence of Triton X-100; but the course of the activity vs. time curve was not influenced by the detergent. Under the conditions employed, the postnuclear supernatants were unable to reinitiate protein synthesis once elongation of nascent polypeptide chains concomitant with ribosome runoff was completed. After mengovirus infection, a gradual disappearance of polysomes from postnuclear supernatants and a simultaneous accumulation of monosomes was observed. The protein-synthesizing activities of normal and infected cells were inversely proportional to the monosome concentrations of their corresponding extracts. The results suggest that the mengovirus infection of Ehrlich ascites tumor cells results in a gradual loss of polysomes and a concomitant accumulation of monosomes in the cytoplasm.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "\n",
      "\n",
      "Title: Purification of an almond emulsin fucosidase on Cibacron blue-sepharose and demonstration of its activity toward fucose-containing glycoproteins.\n",
      "---\n",
      "Actual Abstract: The almond emulsin fucosidase that specifically hydrolyzes fucose in alpha (1-3) linkage to N-acetylglucosamine has been purified 1250-fold. The purification procedure includes ion exchange chromatography on sulfopropyl-Sephadex C-25, gel filtration on Sephacryl S-200, and affinity chromatography on Cibacron blue-Sepharose 4B-CL. The molecular weight of the fucosidase was estimated by gel filtration as approximately 73,000. Enzyme activity was maximal at pH 5.3 in acetate buffer and was dependent on ionic strength; at least 0.1 M NaCl was necessary for optimal activity. The purified enzyme was free of beta-galactosidase activity toward the glycoprotein substrate [3H]galactosyl-asialotransferrin and did not release fucose from substrates containing fucose in alpha (1-6) linkage, (bovine IgG glycopeptides) or in alpha (1-2) linkage, (2'-fucosyllactose). The fucosidase displayed activity toward two glycoprotein substrates known to contain fucose in alpha (1-3) linkage. Extensive incubations resulted in the release of 83% and 43% of the total fucose of asialoorosomucoid and lactoferrin, respectively. The fucosidase did not release fucose from either the \"slow\" or the \"fast\" form of alpha 2-macroglobulin, suggesting the absence of fucosyl alpha (1-3) linkages on that glycoprotein.\n",
      "---\n",
      "Generated: #Purification of an almond emulsin fucosidase on Cibacron blue-sepharose and demonstration of its activity toward fucose-containing glycoproteins.\n",
      "The almond emulsin fucosidase that specifically hydrolyzes fucose in alpha (1-3) linkage to N-acetylglucosamine has been purified 1250-fold. The purification procedure includes ion exchange chromatography on sulfopropyl-Sephadex C-25, gel filtration on Sephacryl S-200, and affinity chromatography on Cibacron blue-Sepharose 4B-CL. The molecular weight of the fucosidase was estimated by gel filtration as approximately 73,000. Enzyme activity was maximal at pH 5.3 in acetate buffer and was dependent on ionic strength; at least 0.1 M NaCl was necessary for optimal activity. The purified enzyme was free of beta-galactosidase activity toward the glycoprotein substrate [3H]galactosyl-asialotransferrin and did not release fucose from substrates containing fucose in alpha (1-6) linkage, (bovine serum albumin-fucose) or alpha (1-4) linkage, (bovine serum albumin-fucose). The enzyme was inhibited by 10 mM N-acetylglucosamine, 10 mM N-acetylgalactosamine, and 10 mM N-acetylglucosamine-6-phosphate. The enzyme was not inhibited by 10 mM N-acetylglucosamine-6-phosphate\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'use_wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTitle: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mActual Abstract: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactual\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgen\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m     wandb_table\u001b[38;5;241m.\u001b[39madd_data(title, actual, gen)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43muse_wandb\u001b[49m:\n\u001b[1;32m     24\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_examples\u001b[39m\u001b[38;5;124m\"\u001b[39m: wandb_table})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'use_wandb' is not defined"
     ]
    }
   ],
   "source": [
    "samples = test_set.select(range(5))  # First 5 examples\n",
    "input_ids = torch.tensor(samples[\"input_ids\"]).to(model.device)\n",
    "attention_mask = torch.tensor(samples[\"attention_mask\"]).to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "# Log predictions to W&B\n",
    "wandb_table = wandb.Table(columns=[\"Title\", \"Actual Abstract\", \"Generated Text\"])\n",
    "for i, gen in enumerate(generated_texts):\n",
    "    title = samples[i][\"title\"]\n",
    "    actual = samples[i][\"abstract\"]\n",
    "    print(f\"\\nTitle: {title}\\n---\\nActual Abstract: {actual}\\n---\\nGenerated: {gen}\\n\")\n",
    "    wandb_table.add_data(title, actual, gen)\n",
    "\n",
    "\n",
    "wandb.log({\"generated_examples\": wandb_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7f60d22-2210-4463-bee0-814c84f988dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"generated_examples\": wandb_table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0fd68d-392c-49c4-bc85-81c05bb4a07b",
   "metadata": {},
   "source": [
    "## Base model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ded167b3-4fc9-439c-8c3c-238d193c5a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b9a3294-c17a-403d-9fe7-626608f6f6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11644/1185074905.py:8: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  base_model_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='94' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [94/94 06:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./eval_output_base\",\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "base_model_trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "base_model_eval_result = base_model_trainer.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d050495-1043-4dc9-abdb-39a430aa626e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Evaluation Metrics:\n",
      "Eval Loss     : 1.8892\n",
      "Eval Perplexity: 6.61\n"
     ]
    }
   ],
   "source": [
    "loss = base_model_eval_result[\"eval_loss\"]\n",
    "perplexity = torch.exp(torch.tensor(loss))\n",
    "\n",
    "print(f\"\\n✅ Evaluation Metrics:\")\n",
    "print(f\"Eval Loss     : {loss:.4f}\")\n",
    "print(f\"Eval Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "717453ea-bff9-4d68-a805-fd1e95140d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"base_model_eval_loss\": loss, \"base_model_eval_perplexity\": perplexity.item()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "14185bf3-943b-4a05-b9e0-649841a4723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, text, max_new_tokens=128):\n",
    "    sample = tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_len, return_attention_mask=True)\n",
    "    input_ids = torch.tensor([sample[\"input_ids\"]]).to(model.device)\n",
    "    attention_mask = torch.tensor([sample[\"attention_mask\"]]).to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        use_cache=False\n",
    "    )\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return generated_texts[0]\n",
    "    # # Log predictions to W&B\n",
    "    # for i, gen in enumerate(generated_texts):\n",
    "    #     title = samples[i][\"title\"]\n",
    "    #     actual = samples[i][\"abstract\"]\n",
    "    #     print(f\"\\nTitle: {title}\\n---\\nActual Abstract: {actual}\\n---\\nGenerated: {gen}\\n\")\n",
    "    #     wandb_table.add_data(title, actual, gen)\n",
    "    \n",
    "    \n",
    "    # wandb.log({\"generated_examples\": wandb_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b2f748a1-4da5-439e-911d-0c76f22bbedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# The relationship between diabetes and blood pressure\\nDiabetes and hypertension are two of the most common chronic diseases in the world. The prevalence of diabetes is increasing rapidly, and the prevalence of hypertension is also increasing. The two diseases are closely related. The prevalence of hypertension is higher in patients with diabetes than in the general population. The prevalence of diabetes is higher in patients with hypertension than in the general population. The prevalence of diabetes is higher in patients with hypertension than in the general population. The prevalence of diabetes'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model, \"# The relationship between diabetes and blood pressure\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d726ad10-f44e-4381-92ea-1705858dab1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# The relationship between diabetes and blood pressure\\n\\n# Importing the libraries\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport pandas as pd\\n\\n# Importing the dataset\\ndataset = pd.read_csv('diabetes.csv')\\nX = dataset.iloc[:, :-1].values\\ny = dataset.iloc[:, -1].values\\n\\n# Splitting the dataset into the Training set and Test set\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y,\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(base_model, \"# The relationship between diabetes and blood pressure\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
