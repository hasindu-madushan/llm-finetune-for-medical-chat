{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df296c86-b127-495d-9731-15df211368f0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11296ccc-cb18-4748-9bbe-7869ddd756d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super_admin/hasindu/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4bd01-643d-42df-bfcc-501af14e26e7",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ec61d2-6470-4376-956e-ca7860ebeb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/pubmed_baseline/\"\n",
    "train_data_path = data_path + \"pubmed_train.csv\"\n",
    "val_data_path = data_path + \"pubmed_val.csv\"\n",
    "\n",
    "model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "max_len = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b778aa67-797f-4f89-a01f-03bf0c8bf831",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c824b99a-58ad-4709-a40c-5338c414bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r = 8\n",
    "lora_alpha = 16\n",
    "lora_target_modules = [\"q_proj\", \"v_proj\", \"o_proj\"]\n",
    "batch_size = 64\n",
    "quantization = \"4bit\"\n",
    "lora_dropout = 0.05\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7abbe6-fdb9-4b58-9e04-48d37a0b22e1",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a165fc8a-f0ad-4d60-9f06-9f4cee91f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(tokenizer, data_df):\n",
    "    dataset = Dataset.from_pandas(data_df)\n",
    "    def tokenize(example):\n",
    "        text = f\"<s>#{example['title']}\\n{example['abstract']}</s>\"\n",
    "        return tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_len, return_attention_mask=True)\n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "756460db-e534-440d-abe9-e4e9089fcc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9283ff09-467a-46cd-816b-6ef6f3faf0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1357.81 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000000/1000000 [11:54<00:00, 1399.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_csv(val_data_path)\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "\n",
    "val_set = tokenize_dataset(tokenizer, val_df)\n",
    "train_set = tokenize_dataset(tokenizer, train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d7c923-6704-49c1-aa29-276ab8a17ab4",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76d3f808-387d-4a57-8858-a00282fb2117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mentity\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mproject\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'StrPath | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnotes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Sequence[str] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dict[str, Any] | str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconfig_exclude_keys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'list[str] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconfig_include_keys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'list[str] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallow_val_change\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgroup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mjob_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Literal['online', 'offline', 'disabled'] | None\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mforce\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0manonymous\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Literal['never', 'allow', 'must'] | None\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreinit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"bool | Literal[None, 'default', 'return_previous', 'finish_previous', 'create_new']\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mresume\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"bool | Literal['allow', 'never', 'must', 'auto'] | None\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mresume_from\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfork_from\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msave_code\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtensorboard\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msync_tensorboard\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmonitor_gym\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'bool | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msettings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Settings | dict[str, Any] | None'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'Run'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Start a new run to track and log to W&B.\n",
       "\n",
       "In an ML training pipeline, you could add `wandb.init()` to the beginning of\n",
       "your training script as well as your evaluation script, and each piece would\n",
       "be tracked as a run in W&B.\n",
       "\n",
       "`wandb.init()` spawns a new background process to log data to a run, and it\n",
       "also syncs data to https://wandb.ai by default, so you can see your results\n",
       "in real-time.\n",
       "\n",
       "Call `wandb.init()` to start a run before logging data with `wandb.log()`.\n",
       "When you're done logging data, call `wandb.finish()` to end the run. If you\n",
       "don't call `wandb.finish()`, the run will end when your script exits.\n",
       "\n",
       "For more on using `wandb.init()`, including detailed examples, check out our\n",
       "[guide and FAQs](https://docs.wandb.ai/guides/track/launch).\n",
       "\n",
       "Examples:\n",
       "    ### Explicitly set the entity and project and choose a name for the run:\n",
       "\n",
       "    ```python\n",
       "    import wandb\n",
       "\n",
       "    run = wandb.init(\n",
       "        entity=\"geoff\",\n",
       "        project=\"capsules\",\n",
       "        name=\"experiment-2021-10-31\",\n",
       "    )\n",
       "\n",
       "    # ... your training code here ...\n",
       "\n",
       "    run.finish()\n",
       "    ```\n",
       "\n",
       "    ### Add metadata about the run using the `config` argument:\n",
       "\n",
       "    ```python\n",
       "    import wandb\n",
       "\n",
       "    config = {\"lr\": 0.01, \"batch_size\": 32}\n",
       "    with wandb.init(config=config) as run:\n",
       "        run.config.update({\"architecture\": \"resnet\", \"depth\": 34})\n",
       "\n",
       "        # ... your training code here ...\n",
       "    ```\n",
       "\n",
       "    Note that you can use `wandb.init()` as a context manager to automatically\n",
       "    call `wandb.finish()` at the end of the block.\n",
       "\n",
       "Args:\n",
       "    entity: The username or team name under which the runs will be logged.\n",
       "        The entity must already exist, so ensure you’ve created your account\n",
       "        or team in the UI before starting to log runs. If not specified, the\n",
       "        run will default your default entity. To change the default entity,\n",
       "        go to [your settings](https://wandb.ai/settings) and update the\n",
       "        \"Default location to create new projects\" under \"Default team\".\n",
       "    project: The name of the project under which this run will be logged.\n",
       "        If not specified, we use a heuristic to infer the project name based\n",
       "        on the system, such as checking the git root or the current program\n",
       "        file. If we can't infer the project name, the project will default to\n",
       "        `\"uncategorized\"`.\n",
       "    dir: The absolute path to the directory where experiment logs and\n",
       "        metadata files are stored. If not specified, this defaults\n",
       "        to the `./wandb` directory. Note that this does not affect the\n",
       "        location where artifacts are stored when calling `download()`.\n",
       "    id: A unique identifier for this run, used for resuming. It must be unique\n",
       "        within the project and cannot be reused once a run is deleted. The\n",
       "        identifier must not contain any of the following special characters:\n",
       "        `/ \\ # ? % :`. For a short descriptive name, use the `name` field,\n",
       "        or for saving hyperparameters to compare across runs, use `config`.\n",
       "    name: A short display name for this run, which appears in the UI to help\n",
       "        you identify it. By default, we generate a random two-word name\n",
       "        allowing easy cross-reference runs from table to charts. Keeping these\n",
       "        run names brief enhances readability in chart legends and tables. For\n",
       "        saving hyperparameters, we recommend using the `config` field.\n",
       "    notes: A detailed description of the run, similar to a commit message in\n",
       "        Git. Use this argument to capture any context or details that may\n",
       "        help you recall the purpose or setup of this run in the future.\n",
       "    tags: A list of tags to label this run in the UI. Tags are helpful for\n",
       "        organizing runs or adding temporary identifiers like \"baseline\" or\n",
       "        \"production.\" You can easily add, remove tags, or filter by tags in\n",
       "        the UI.\n",
       "        If resuming a run, the tags provided here will replace any existing\n",
       "        tags. To add tags to a resumed run without overwriting the current\n",
       "        tags, use `run.tags += [\"new_tag\"]` after calling `run = wandb.init()`.\n",
       "    config: Sets `wandb.config`, a dictionary-like object for storing input\n",
       "        parameters to your run, such as model hyperparameters or data\n",
       "        preprocessing settings.\n",
       "        The config appears in the UI in an overview page, allowing you to\n",
       "        group, filter, and sort runs based on these parameters.\n",
       "        Keys should not contain periods (`.`), and values should be\n",
       "        smaller than 10 MB.\n",
       "        If a dictionary, `argparse.Namespace`, or `absl.flags.FLAGS` is\n",
       "        provided, the key-value pairs will be loaded directly into\n",
       "        `wandb.config`.\n",
       "        If a string is provided, it is interpreted as a path to a YAML file,\n",
       "        from which configuration values will be loaded into `wandb.config`.\n",
       "    config_exclude_keys: A list of specific keys to exclude from `wandb.config`.\n",
       "    config_include_keys: A list of specific keys to include in `wandb.config`.\n",
       "    allow_val_change: Controls whether config values can be modified after their\n",
       "        initial set. By default, an exception is raised if a config value is\n",
       "        overwritten. For tracking variables that change during training, such as\n",
       "        a learning rate, consider using `wandb.log()` instead. By default, this\n",
       "        is `False` in scripts and `True` in Notebook environments.\n",
       "    group: Specify a group name to organize individual runs as part of a larger\n",
       "        experiment. This is useful for cases like cross-validation or running\n",
       "        multiple jobs that train and evaluate a model on different test sets.\n",
       "        Grouping allows you to manage related runs collectively in the UI,\n",
       "        making it easy to toggle and review results as a unified experiment.\n",
       "        For more information, refer to our\n",
       "        [guide to grouping runs](https://docs.wandb.com/guides/runs/grouping).\n",
       "    job_type: Specify the type of run, especially helpful when organizing runs\n",
       "        within a group as part of a larger experiment. For example, in a group,\n",
       "        you might label runs with job types such as \"train\" and \"eval\".\n",
       "        Defining job types enables you to easily filter and group similar runs\n",
       "        in the UI, facilitating direct comparisons.\n",
       "    mode: Specifies how run data is managed, with the following options:\n",
       "        - `\"online\"` (default): Enables live syncing with W&B when a network\n",
       "            connection is available, with real-time updates to visualizations.\n",
       "        - `\"offline\"`: Suitable for air-gapped or offline environments; data\n",
       "            is saved locally and can be synced later. Ensure the run folder\n",
       "            is preserved to enable future syncing.\n",
       "        - `\"disabled\"`: Disables all W&B functionality, making the run’s methods\n",
       "            no-ops. Typically used in testing to bypass W&B operations.\n",
       "    force: Determines if a W&B login is required to run the script. If `True`,\n",
       "        the user must be logged in to W&B; otherwise, the script will not\n",
       "        proceed. If `False` (default), the script can proceed without a login,\n",
       "        switching to offline mode if the user is not logged in.\n",
       "    anonymous: Specifies the level of control over anonymous data logging.\n",
       "        Available options are:\n",
       "        - `\"never\"` (default): Requires you to link your W&B account before\n",
       "            tracking the run. This prevents unintentional creation of anonymous\n",
       "            runs by ensuring each run is associated with an account.\n",
       "        - `\"allow\"`: Enables a logged-in user to track runs with their account,\n",
       "            but also allows someone running the script without a W&B account\n",
       "            to view the charts and data in the UI.\n",
       "        - `\"must\"`: Forces the run to be logged to an anonymous account, even\n",
       "            if the user is logged in.\n",
       "    reinit: Shorthand for the \"reinit\" setting. Determines the behavior of\n",
       "        `wandb.init()` when a run is active.\n",
       "    resume: Controls the behavior when resuming a run with the specified `id`.\n",
       "        Available options are:\n",
       "        - `\"allow\"`: If a run with the specified `id` exists, it will resume\n",
       "            from the last step; otherwise, a new run will be created.\n",
       "        - `\"never\"`: If a run with the specified `id` exists, an error will\n",
       "            be raised. If no such run is found, a new run will be created.\n",
       "        - `\"must\"`: If a run with the specified `id` exists, it will resume\n",
       "            from the last step. If no run is found, an error will be raised.\n",
       "        - `\"auto\"`: Automatically resumes the previous run if it crashed on\n",
       "            this machine; otherwise, starts a new run.\n",
       "        - `True`: Deprecated. Use `\"auto\"` instead.\n",
       "        - `False`: Deprecated. Use the default behavior (leaving `resume`\n",
       "            unset) to always start a new run.\n",
       "        Note: If `resume` is set, `fork_from` and `resume_from` cannot be\n",
       "        used. When `resume` is unset, the system will always start a new run.\n",
       "        For more details, see our\n",
       "        [guide to resuming runs](https://docs.wandb.com/guides/runs/resuming).\n",
       "    resume_from: Specifies a moment in a previous run to resume a run from,\n",
       "        using the format `{run_id}?_step={step}`. This allows users to truncate\n",
       "        the history logged to a run at an intermediate step and resume logging\n",
       "        from that step. The target run must be in the same project.\n",
       "        If an `id` argument is also provided, the `resume_from` argument will\n",
       "        take precedence.\n",
       "        `resume`, `resume_from` and `fork_from` cannot be used together, only\n",
       "        one of them can be used at a time.\n",
       "        Note: This feature is in beta and may change in the future.\n",
       "    fork_from: Specifies a point in a previous run from which to fork a new\n",
       "        run, using the format `{id}?_step={step}`. This creates a new run that\n",
       "        resumes logging from the specified step in the target run’s history.\n",
       "        The target run must be part of the current project.\n",
       "        If an `id` argument is also provided, it must be different from the\n",
       "        `fork_from` argument, an error will be raised if they are the same.\n",
       "        `resume`, `resume_from` and `fork_from` cannot be used together, only\n",
       "        one of them can be used at a time.\n",
       "        Note: This feature is in beta and may change in the future.\n",
       "    save_code: Enables saving the main script or notebook to W&B, aiding in\n",
       "        experiment reproducibility and allowing code comparisons across runs in\n",
       "        the UI. By default, this is disabled, but you can change the default to\n",
       "        enable on your [settings page](https://wandb.ai/settings).\n",
       "    tensorboard: Deprecated. Use `sync_tensorboard` instead.\n",
       "    sync_tensorboard: Enables automatic syncing of W&B logs from TensorBoard\n",
       "        or TensorBoardX, saving relevant event files for viewing in the W&B UI.\n",
       "        saving relevant event files for viewing in the W&B UI. (Default: `False`)\n",
       "    monitor_gym: Enables automatic logging of videos of the environment when\n",
       "        using OpenAI Gym. For additional details, see our\n",
       "        [guide for gym integration](https://docs.wandb.com/guides/integrations/openai-gym).\n",
       "    settings: Specifies a dictionary or `wandb.Settings` object with advanced\n",
       "        settings for the run.\n",
       "\n",
       "Returns:\n",
       "    A `Run` object, which is a handle to the current run. Use this object\n",
       "    to perform operations like logging data, saving files, and finishing\n",
       "    the run. See the [Run API](https://docs.wandb.ai/ref/python/run) for\n",
       "    more details.\n",
       "\n",
       "Raises:\n",
       "    Error: If some unknown or internal error happened during the run\n",
       "        initialization.\n",
       "    AuthenticationError: If the user failed to provide valid credentials.\n",
       "    CommError: If there was a problem communicating with the W&B server.\n",
       "    UsageError: If the user provided invalid arguments to the function.\n",
       "    KeyboardInterrupt: If the user interrupts the run initialization process.\n",
       "        If the user interrupts the run initialization process.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/hasindu/myenv/lib/python3.10/site-packages/wandb/sdk/wandb_init.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b12ae009-404d-4ea5-953f-33721f74f2dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/super_admin/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhasindumadushan325\u001b[0m (\u001b[33mhasindumadushan325-university-of-peradeniya\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/super_admin/hasindu/src/wandb/run-20250510_092445-t3va4d1v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/phi3-pubmed-pretrain/runs/t3va4d1v' target=\"_blank\">attempt-1</a></strong> to <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/phi3-pubmed-pretrain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/phi3-pubmed-pretrain' target=\"_blank\">https://wandb.ai/hasindumadushan325-university-of-peradeniya/phi3-pubmed-pretrain</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/phi3-pubmed-pretrain/runs/t3va4d1v' target=\"_blank\">https://wandb.ai/hasindumadushan325-university-of-peradeniya/phi3-pubmed-pretrain/runs/t3va4d1v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hasindumadushan325-university-of-peradeniya/phi3-pubmed-pretrain/runs/t3va4d1v?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x75a959adbe80>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"phi3-pubmed-pretrain\",\n",
    "    name=\"attempt-1\",\n",
    "    config={\n",
    "        \"lora_r\": lora_r,\n",
    "        \"lora_alpha\": lora_alpha,\n",
    "        \"epochs\": epochs,\n",
    "        \"quantization\": quantization,\n",
    "        \"lora_target_modules\": lora_target_modules\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b74b4-f93e-41c8-936a-1b5564a650b8",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8286896c-1fa0-43ea-a43c-0c4c862d3267",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=quantization==\"4bit\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "017d9825-af10-4ad0-bb79-77c3c2a2de80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Fetching 2 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:27<00:00, 13.71s/it]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb27353b-0289-4c08-a6bf-708622c7d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c6552-5082-447b-b93d-2749d752ec1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
