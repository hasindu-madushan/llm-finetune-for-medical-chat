{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4643151f-8b64-40cb-9b36-758e369e6407",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd5ae7-e718-4db9-9b4a-ff81be0e2457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install absl-py rouge-score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034bab24-dc8a-4b83-a570-29fcf3bdeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd47c21-5017-47b8-ac58-df9338ef163a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20bc9df7-5328-4945-a1bd-30f823fc8b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super_admin/hasindu/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import wandb\n",
    "import evaluate  # Hugging Face's evaluate library\n",
    "import numpy as np\n",
    "import torch\n",
    "from bert_score import BERTScorer, score as bert_score\n",
    "\n",
    "from utils import tokenize_dataset_for_qna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458060f-cfcf-4f51-a6aa-506cef1dc7c1",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb057781-55b7-494f-bcb4-dd7d00ba4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/phi_qna_finetuned_attempt_3/final\"\n",
    "\n",
    "data_path = \"../data/qna/\"\n",
    "test_data_path = data_path + \"test.csv\"\n",
    "\n",
    "model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "base_model_path = \"../models/phi_pubmed_pretrained_attempt_3/final_pretrained\"\n",
    "\n",
    "max_len = 512\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38a3746-3c13-42cc-920a-2614eeede918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhasindumadushan325\u001b[0m (\u001b[33mhasindumadushan325-university-of-peradeniya\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/super_admin/hasindu/src/wandb/run-20250516_032009-mhwijxkt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/qna_finetune-evaluation/runs/mhwijxkt' target=\"_blank\">attempt_3</a></strong> to <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/qna_finetune-evaluation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/qna_finetune-evaluation' target=\"_blank\">https://wandb.ai/hasindumadushan325-university-of-peradeniya/qna_finetune-evaluation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hasindumadushan325-university-of-peradeniya/qna_finetune-evaluation/runs/mhwijxkt' target=\"_blank\">https://wandb.ai/hasindumadushan325-university-of-peradeniya/qna_finetune-evaluation/runs/mhwijxkt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hasindumadushan325-university-of-peradeniya/qna_finetune-evaluation/runs/mhwijxkt?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x713f07f54a30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"qna_finetune-evaluation\", name=\"attempt_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f573ca-73a8-4ef6-af46-ecd2d84e6bad",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15035ec-6729-4cf4-a946-1b1f2a700e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Instruction:\n",
      "Assume you are an excellent doctor. Using your knowledge, answer the question given below.\n",
      "\n",
      "# Question: {question}\n",
      "\n",
      "# Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "# Instruction:\n",
    "Assume you are an excellent doctor. Using your knowledge, answer the question given below.\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Answer: \"\"\"\n",
    "prompt_template = prompt_template.strip()\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8443f97-858d-4220-b3d2-8f728fa48020",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030d192a-2157-43c3-8529-3f9fd4663faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "\n",
    "def tokenize_dataset_for_qna(tokenizer, data_df, prompt_template, max_len):\n",
    "    dataset = Dataset.from_pandas(data_df)\n",
    "    dataset = dataset.map(lambda sample: tokenize_for_qna(sample, tokenizer, prompt_template, max_len), batched=False)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenize_for_qna(example, tokenizer, prompt_template, max_len):\n",
    "    prompt = prompt_template.format(question=example['question'])\n",
    "    answer = example[\"answer\"] + tokenizer.eos_token\n",
    "    full_text = prompt + answer\n",
    "\n",
    "    # Tokenize answer to get the length of answer tokens\n",
    "    prompt_len = len(tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    )[\"input_ids\"])\n",
    "\n",
    "    # Tokenize answer to get the length of answer tokens\n",
    "    full_len = len(tokenizer(\n",
    "        full_text,\n",
    "        truncation=False\n",
    "    )[\"input_ids\"])\n",
    "    \n",
    "    # Tokenize full sequence once\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True    \n",
    "    )\n",
    "    \n",
    "    # Convert to numpy arrays for faster operations\n",
    "    input_ids = np.array(tokenized[\"input_ids\"])\n",
    "    attention_mask = np.array(tokenized[\"attention_mask\"])\n",
    "    \n",
    "    # Create labels array and mask prompt portion efficiently\n",
    "    labels = input_ids.copy()\n",
    "    padding_len = max_len - full_len\n",
    "    # Mask the prompt tokens\n",
    "    labels[padding_len:padding_len + prompt_len] = -100\n",
    "    \n",
    "    # Update the tokenized dict with numpy arrays\n",
    "    tokenized[\"input_ids\"] = input_ids.tolist()\n",
    "    tokenized[\"attention_mask\"] = attention_mask.tolist()\n",
    "    tokenized[\"labels\"] = labels.tolist()\n",
    "    \n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac602a77-0a3a-4d89-915a-b102625ebce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1500/1500 [00:01<00:00, 806.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_data_path)\n",
    "test_set = tokenize_dataset_for_qna(tokenizer, test_df.iloc[:1500, :], prompt_template, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a851ebd-aff1-4e44-ad09-93151119bcb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Ive had sleep apnea for approximately 8 years now. I also have polycythemia vera which started around the same time. I was constantly having to get plebotomies every six months when I wasnt wearing my cpap all of the time. But for the last few years now, Ive been real diligent about wearing it and I havent had to get any phlebotomies. Im now thinking that my polycythemia vera was because I was not getting enough oxygen? Am I on the right track? I would appreciate any help you could offer. Thanks.',\n",
       " 'answer': 'Hello, Thank you for contacting ChatDoctorI understand your concern, I am Chat Doctor, Infectious Disease Specialist answering your query. Yes you are on right track. This can happen to you for low oxygen tension. As apnea occurs during sleep time there is adaptation by mean of increase RBC which leads you towards the poly Bohemia. I advise you to use the CPAP during sleep otherwise the polycythemia will increase. I advise you to take the phlebotomies done and once the hemoglobin level is under control you can start using he CPAP. You can ask for further queries here on bit.ly/ Chat Doctor. Thank you, ChatDoctorInfectious Disease specialist wish you the best health at Chat Doctor.',\n",
       " 'input_ids': [32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  396,\n",
       "  2799,\n",
       "  4080,\n",
       "  29901,\n",
       "  13,\n",
       "  7900,\n",
       "  2017,\n",
       "  366,\n",
       "  526,\n",
       "  385,\n",
       "  15129,\n",
       "  11619,\n",
       "  29889,\n",
       "  5293,\n",
       "  596,\n",
       "  7134,\n",
       "  29892,\n",
       "  1234,\n",
       "  278,\n",
       "  1139,\n",
       "  2183,\n",
       "  2400,\n",
       "  29889,\n",
       "  13,\n",
       "  13,\n",
       "  29937,\n",
       "  894,\n",
       "  29901,\n",
       "  306,\n",
       "  345,\n",
       "  750,\n",
       "  8709,\n",
       "  3095,\n",
       "  14011,\n",
       "  363,\n",
       "  14235,\n",
       "  29871,\n",
       "  29947,\n",
       "  2440,\n",
       "  1286,\n",
       "  29889,\n",
       "  306,\n",
       "  884,\n",
       "  505,\n",
       "  15680,\n",
       "  1270,\n",
       "  386,\n",
       "  29747,\n",
       "  1147,\n",
       "  29874,\n",
       "  607,\n",
       "  4687,\n",
       "  2820,\n",
       "  278,\n",
       "  1021,\n",
       "  931,\n",
       "  29889,\n",
       "  306,\n",
       "  471,\n",
       "  21003,\n",
       "  2534,\n",
       "  304,\n",
       "  679,\n",
       "  5644,\n",
       "  7451,\n",
       "  290,\n",
       "  583,\n",
       "  1432,\n",
       "  4832,\n",
       "  7378,\n",
       "  746,\n",
       "  306,\n",
       "  471,\n",
       "  593,\n",
       "  591,\n",
       "  4362,\n",
       "  590,\n",
       "  21447,\n",
       "  481,\n",
       "  599,\n",
       "  310,\n",
       "  278,\n",
       "  931,\n",
       "  29889,\n",
       "  1205,\n",
       "  363,\n",
       "  278,\n",
       "  1833,\n",
       "  2846,\n",
       "  2440,\n",
       "  1286,\n",
       "  29892,\n",
       "  306,\n",
       "  345,\n",
       "  1063,\n",
       "  1855,\n",
       "  21749,\n",
       "  25692,\n",
       "  1048,\n",
       "  591,\n",
       "  4362,\n",
       "  372,\n",
       "  322,\n",
       "  306,\n",
       "  447,\n",
       "  794,\n",
       "  750,\n",
       "  304,\n",
       "  679,\n",
       "  738,\n",
       "  1374,\n",
       "  280,\n",
       "  7451,\n",
       "  290,\n",
       "  583,\n",
       "  29889,\n",
       "  1954,\n",
       "  1286,\n",
       "  7291,\n",
       "  393,\n",
       "  590,\n",
       "  15680,\n",
       "  1270,\n",
       "  386,\n",
       "  29747,\n",
       "  1147,\n",
       "  29874,\n",
       "  471,\n",
       "  1363,\n",
       "  306,\n",
       "  471,\n",
       "  451,\n",
       "  2805,\n",
       "  3307,\n",
       "  288,\n",
       "  28596,\n",
       "  29973,\n",
       "  1913,\n",
       "  306,\n",
       "  373,\n",
       "  278,\n",
       "  1492,\n",
       "  5702,\n",
       "  29973,\n",
       "  306,\n",
       "  723,\n",
       "  11188,\n",
       "  738,\n",
       "  1371,\n",
       "  366,\n",
       "  1033,\n",
       "  5957,\n",
       "  29889,\n",
       "  1834,\n",
       "  29889,\n",
       "  13,\n",
       "  13,\n",
       "  29937,\n",
       "  673,\n",
       "  29901,\n",
       "  10994,\n",
       "  29892,\n",
       "  3374,\n",
       "  366,\n",
       "  363,\n",
       "  6958,\n",
       "  292,\n",
       "  678,\n",
       "  271,\n",
       "  6132,\n",
       "  2801,\n",
       "  29902,\n",
       "  2274,\n",
       "  596,\n",
       "  5932,\n",
       "  29892,\n",
       "  306,\n",
       "  626,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  29892,\n",
       "  512,\n",
       "  3647,\n",
       "  2738,\n",
       "  360,\n",
       "  895,\n",
       "  559,\n",
       "  12630,\n",
       "  391,\n",
       "  22862,\n",
       "  596,\n",
       "  2346,\n",
       "  29889,\n",
       "  3869,\n",
       "  366,\n",
       "  526,\n",
       "  373,\n",
       "  1492,\n",
       "  5702,\n",
       "  29889,\n",
       "  910,\n",
       "  508,\n",
       "  3799,\n",
       "  304,\n",
       "  366,\n",
       "  363,\n",
       "  4482,\n",
       "  288,\n",
       "  28596,\n",
       "  260,\n",
       "  2673,\n",
       "  29889,\n",
       "  1094,\n",
       "  3095,\n",
       "  14011,\n",
       "  10008,\n",
       "  2645,\n",
       "  8709,\n",
       "  931,\n",
       "  727,\n",
       "  338,\n",
       "  28206,\n",
       "  491,\n",
       "  2099,\n",
       "  310,\n",
       "  7910,\n",
       "  390,\n",
       "  5371,\n",
       "  607,\n",
       "  11981,\n",
       "  366,\n",
       "  7113,\n",
       "  278,\n",
       "  15680,\n",
       "  17966,\n",
       "  29747,\n",
       "  29889,\n",
       "  306,\n",
       "  22939,\n",
       "  366,\n",
       "  304,\n",
       "  671,\n",
       "  278,\n",
       "  28505,\n",
       "  3301,\n",
       "  2645,\n",
       "  8709,\n",
       "  6467,\n",
       "  278,\n",
       "  15680,\n",
       "  1270,\n",
       "  386,\n",
       "  29747,\n",
       "  674,\n",
       "  7910,\n",
       "  29889,\n",
       "  306,\n",
       "  22939,\n",
       "  366,\n",
       "  304,\n",
       "  2125,\n",
       "  278,\n",
       "  1374,\n",
       "  280,\n",
       "  7451,\n",
       "  290,\n",
       "  583,\n",
       "  2309,\n",
       "  322,\n",
       "  2748,\n",
       "  278,\n",
       "  9736,\n",
       "  468,\n",
       "  417,\n",
       "  2109,\n",
       "  3233,\n",
       "  338,\n",
       "  1090,\n",
       "  2761,\n",
       "  366,\n",
       "  508,\n",
       "  1369,\n",
       "  773,\n",
       "  540,\n",
       "  28505,\n",
       "  3301,\n",
       "  29889,\n",
       "  887,\n",
       "  508,\n",
       "  2244,\n",
       "  363,\n",
       "  4340,\n",
       "  9365,\n",
       "  1244,\n",
       "  373,\n",
       "  2586,\n",
       "  29889,\n",
       "  368,\n",
       "  29914,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  29889,\n",
       "  3374,\n",
       "  366,\n",
       "  29892,\n",
       "  678,\n",
       "  271,\n",
       "  6132,\n",
       "  2801,\n",
       "  797,\n",
       "  3647,\n",
       "  2738,\n",
       "  360,\n",
       "  895,\n",
       "  559,\n",
       "  4266,\n",
       "  391,\n",
       "  6398,\n",
       "  366,\n",
       "  278,\n",
       "  1900,\n",
       "  9045,\n",
       "  472,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  29889,\n",
       "  32000],\n",
       " 'attention_mask': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  10994,\n",
       "  29892,\n",
       "  3374,\n",
       "  366,\n",
       "  363,\n",
       "  6958,\n",
       "  292,\n",
       "  678,\n",
       "  271,\n",
       "  6132,\n",
       "  2801,\n",
       "  29902,\n",
       "  2274,\n",
       "  596,\n",
       "  5932,\n",
       "  29892,\n",
       "  306,\n",
       "  626,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  29892,\n",
       "  512,\n",
       "  3647,\n",
       "  2738,\n",
       "  360,\n",
       "  895,\n",
       "  559,\n",
       "  12630,\n",
       "  391,\n",
       "  22862,\n",
       "  596,\n",
       "  2346,\n",
       "  29889,\n",
       "  3869,\n",
       "  366,\n",
       "  526,\n",
       "  373,\n",
       "  1492,\n",
       "  5702,\n",
       "  29889,\n",
       "  910,\n",
       "  508,\n",
       "  3799,\n",
       "  304,\n",
       "  366,\n",
       "  363,\n",
       "  4482,\n",
       "  288,\n",
       "  28596,\n",
       "  260,\n",
       "  2673,\n",
       "  29889,\n",
       "  1094,\n",
       "  3095,\n",
       "  14011,\n",
       "  10008,\n",
       "  2645,\n",
       "  8709,\n",
       "  931,\n",
       "  727,\n",
       "  338,\n",
       "  28206,\n",
       "  491,\n",
       "  2099,\n",
       "  310,\n",
       "  7910,\n",
       "  390,\n",
       "  5371,\n",
       "  607,\n",
       "  11981,\n",
       "  366,\n",
       "  7113,\n",
       "  278,\n",
       "  15680,\n",
       "  17966,\n",
       "  29747,\n",
       "  29889,\n",
       "  306,\n",
       "  22939,\n",
       "  366,\n",
       "  304,\n",
       "  671,\n",
       "  278,\n",
       "  28505,\n",
       "  3301,\n",
       "  2645,\n",
       "  8709,\n",
       "  6467,\n",
       "  278,\n",
       "  15680,\n",
       "  1270,\n",
       "  386,\n",
       "  29747,\n",
       "  674,\n",
       "  7910,\n",
       "  29889,\n",
       "  306,\n",
       "  22939,\n",
       "  366,\n",
       "  304,\n",
       "  2125,\n",
       "  278,\n",
       "  1374,\n",
       "  280,\n",
       "  7451,\n",
       "  290,\n",
       "  583,\n",
       "  2309,\n",
       "  322,\n",
       "  2748,\n",
       "  278,\n",
       "  9736,\n",
       "  468,\n",
       "  417,\n",
       "  2109,\n",
       "  3233,\n",
       "  338,\n",
       "  1090,\n",
       "  2761,\n",
       "  366,\n",
       "  508,\n",
       "  1369,\n",
       "  773,\n",
       "  540,\n",
       "  28505,\n",
       "  3301,\n",
       "  29889,\n",
       "  887,\n",
       "  508,\n",
       "  2244,\n",
       "  363,\n",
       "  4340,\n",
       "  9365,\n",
       "  1244,\n",
       "  373,\n",
       "  2586,\n",
       "  29889,\n",
       "  368,\n",
       "  29914,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  29889,\n",
       "  3374,\n",
       "  366,\n",
       "  29892,\n",
       "  678,\n",
       "  271,\n",
       "  6132,\n",
       "  2801,\n",
       "  797,\n",
       "  3647,\n",
       "  2738,\n",
       "  360,\n",
       "  895,\n",
       "  559,\n",
       "  4266,\n",
       "  391,\n",
       "  6398,\n",
       "  366,\n",
       "  278,\n",
       "  1900,\n",
       "  9045,\n",
       "  472,\n",
       "  678,\n",
       "  271,\n",
       "  15460,\n",
       "  29889,\n",
       "  32000]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60d571-b664-464b-a3b5-7ba96d0e8e22",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e73e1b6-b9b7-47b2-ab53-25f21c24d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = PeftConfig.from_pretrained(model_path)\n",
    "base_model_name = peft_config.base_model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da75b58-9eeb-4020-936f-f1cf43d25c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65cb96b6-5bd7-44d1-bd84-c6a107afddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# === Quantized model loading ===\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0b63bf7-fe1b-401f-b8c1-a394a65ed24d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3Attention(\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=24, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=24, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "              (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "              (activation_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm()\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_attention_layernorm): Phi3RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(model, model_path)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0fd68d-392c-49c4-bc85-81c05bb4a07b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ce6b4-018f-422f-a85e-285f9038dbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1b39ed-ea6c-426c-a76c-63ac9283a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded167b3-4fc9-439c-8c3c-238d193c5a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a3294-c17a-403d-9fe7-626608f6f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./eval_output_base\",\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     do_eval=True,\n",
    "#     report_to=\"none\"\n",
    "# )\n",
    "\n",
    "# base_model_trainer = Trainer(\n",
    "#     model=base_model,\n",
    "#     args=training_args,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "# )\n",
    "\n",
    "# base_model_eval_result = base_model_trainer.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6480b4-2485-422f-a289-898f0ba4dbe9",
   "metadata": {},
   "source": [
    "# Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f852223-0fa4-49c8-9f81-e600cef32e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./eval_output\",\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",\n",
    "    eval_accumulation_steps=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d93a44e-cf37-4dc0-8171-3bde13eaca09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63728/875759355.py:97: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize metrics ONCE (reuse them)\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bert_scorer = BERTScorer(\n",
    "    lang=\"en\",\n",
    "    model_type=\"bert-base-uncased\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    idf=False,  # Disable IDF to save memory\n",
    "    rescale_with_baseline=True  # Better score normalization\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\" Metric computation \"\"\"\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_preds\n",
    "        \n",
    "        # Convert to numpy (move to CPU first if needed)\n",
    "        if torch.is_tensor(logits):\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "        if torch.is_tensor(labels):\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "        \n",
    "        # Get predicted tokens (shape: [batch_size, seq_length])\n",
    "        pred_ids = np.argmax(logits, axis=-1)\n",
    "        \n",
    "        # Decode in batches to avoid memory spikes\n",
    "        batch_size = 8  # Adjust based on your GPU memory\n",
    "        pred_str, label_str = [], []\n",
    "        \n",
    "        for i in range(0, len(pred_ids), batch_size):\n",
    "            # Decode predictions\n",
    "            batch_preds = pred_ids[i:i+batch_size]\n",
    "            pred_str.extend(tokenizer.batch_decode(\n",
    "                batch_preds, \n",
    "                skip_special_tokens=True\n",
    "            ))\n",
    "            \n",
    "            # Decode labels (replace -100 with pad_token_id)\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "            batch_labels = np.where(\n",
    "                batch_labels != -100, \n",
    "                batch_labels, \n",
    "                tokenizer.pad_token_id\n",
    "            )\n",
    "            label_str.extend(tokenizer.batch_decode(\n",
    "                batch_labels, \n",
    "                skip_special_tokens=True\n",
    "            ))\n",
    "        \n",
    "        # Skip if empty (avoid errors)\n",
    "        if not pred_str or not label_str:\n",
    "            return {\n",
    "                'bleu': 0.0,\n",
    "                'rouge1': 0.0,\n",
    "                'rouge2': 0.0,\n",
    "                'rougeL': 0.0,\n",
    "                'bertscore_f1': 0.0\n",
    "            }\n",
    "        \n",
    "        # Compute BLEU (handle edge cases)\n",
    "        try:\n",
    "            bleu_score = bleu_metric.compute(\n",
    "                predictions=pred_str,\n",
    "                references=[[ref] for ref in label_str]\n",
    "            )['bleu']\n",
    "        except:\n",
    "            bleu_score = 0.0\n",
    "        \n",
    "        # Compute ROUGE\n",
    "        rouge_scores = rouge_metric.compute(\n",
    "            predictions=pred_str,\n",
    "            references=label_str,\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        \n",
    "        # Compute BERTScore in batches\n",
    "        P, R, F1 = bert_scorer.score(\n",
    "            pred_str, \n",
    "            label_str,\n",
    "            batch_size=4  # Small batch for BERTScore\n",
    "        )\n",
    "        \n",
    "        metrics = {\n",
    "            'bleu': bleu_score,\n",
    "            'rouge1': rouge_scores['rouge1'],\n",
    "            'rouge2': rouge_scores['rouge2'],\n",
    "            'rougeL': rouge_scores['rougeL'],\n",
    "            'bertscore_precision': P.mean().item(),\n",
    "            'bertscore_recall': R.mean().item(),\n",
    "            'bertscore_f1': F1.mean().item(),\n",
    "        }\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        return metrics\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer, padding=False),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9d3989f-7cbb-47aa-8197-c2dab6692fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188/188 15:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Evaluate perplexity ===\n",
    "eval_result = trainer.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ddcdbf5-5d13-49ac-b3a3-86e2986139ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics:\n",
      "Loss: 1.6187\n",
      "Perplexity: 5.05\n",
      "BLEU: 0.1806\n",
      "ROUGE-1: 0.4767\n",
      "ROUGE-2: 0.2357\n",
      "ROUGE-L: 0.3843\n",
      "BERTscore precision: 0.3302\n",
      "BERTscore recall: 0.5072\n",
      "BERTscore f1: 0.4110\n"
     ]
    }
   ],
   "source": [
    "# Print all results\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Loss: {eval_result['eval_loss']:.4f}\")\n",
    "print(f\"Perplexity: {torch.exp(torch.tensor(eval_result['eval_loss'])):.2f}\")\n",
    "print(f\"BLEU: {eval_result['eval_bleu']:.4f}\")\n",
    "print(f\"ROUGE-1: {eval_result['eval_rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {eval_result['eval_rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {eval_result['eval_rougeL']:.4f}\")\n",
    "print(f\"BERTscore precision: {eval_result['eval_bertscore_precision']:.4f}\")\n",
    "print(f\"BERTscore recall: {eval_result['eval_bertscore_recall']:.4f}\")\n",
    "print(f\"BERTscore f1: {eval_result['eval_bertscore_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92be58f3-423a-42b3-9bc4-ae8a46dc9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\n",
    "    \"eval_loss\": eval_result['eval_loss'], \n",
    "    \"perplexity\": torch.exp(torch.tensor(eval_result['eval_loss'])),\n",
    "    \"BLUE\": eval_result['eval_bleu'],\n",
    "    \"ROUGE_1\": eval_result['eval_rouge1'],\n",
    "    \"ROUGE_2\": eval_result['eval_rouge2'],\n",
    "    \"ROUGE_L\": eval_result['eval_rougeL'],\n",
    "    \"BERTscore_precision\": eval_result['eval_bertscore_precision'],\n",
    "    \"BERTscore recall\": eval_result['eval_bertscore_recall'],\n",
    "    \"BERTscore f1\": eval_result['eval_bertscore_f1']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b6324-721a-40fd-94e8-260ae1f37420",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74594108-635c-4a63-ac8f-074d13a890cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = test_set.select(range(433, 439))  # First 5 examples\n",
    "input_ids = torch.tensor(samples[\"input_ids\"]).to(model.device)\n",
    "attention_mask = torch.tensor(samples[\"attention_mask\"]).to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "# Log predictions to W&B\n",
    "wandb_table = wandb.Table(columns=[\"Title\", \"Actual Abstract\", \"Generated Text\"])\n",
    "for i, gen in enumerate(generated_texts):\n",
    "    title = samples[i][\"title\"]\n",
    "    actual = samples[i][\"abstract\"]\n",
    "    print(f\"\\nActual: {title}\\n{actual}\\n---\\nGenerated: {gen}\\n\")\n",
    "    wandb_table.add_data(title, actual, gen)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035ed31-1dea-4631-8d66-797ce87c0a74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f60d22-2210-4463-bee0-814c84f988dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"generated_examples\": wandb_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14185bf3-943b-4a05-b9e0-649841a4723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, text, max_new_tokens=128):\n",
    "    sample = tokenizer(text + tokenizer.eos_token, truncation=True, padding=\"max_length\", max_length=max_len, return_attention_mask=True)\n",
    "    input_ids = torch.tensor([sample[\"input_ids\"]]).to(model.device)\n",
    "    attention_mask = torch.tensor([sample[\"attention_mask\"]]).to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "        use_cache=False\n",
    "    )\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return generated_texts[0]\n",
    "    # # Log predictions to W&B\n",
    "    # for i, gen in enumerate(generated_texts):\n",
    "    #     title = samples[i][\"title\"]\n",
    "    #     actual = samples[i][\"abstract\"]\n",
    "    #     print(f\"\\nTitle: {title}\\n---\\nActual Abstract: {actual}\\n---\\nGenerated: {gen}\\n\")\n",
    "    #     wandb_table.add_data(title, actual, gen)\n",
    "    \n",
    "    \n",
    "    # wandb.log({\"generated_examples\": wandb_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c1b43f-b1c8-4f27-80ec-76cd90ce3be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f748a1-4da5-439e-911d-0c76f22bbedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate(model, prompt_template.format(question=\"What are the symptoms of Glaucoma ??\"), max_new_tokens=200)\t\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23655995-0192-4149-9a70-6bfd50eada85",
   "metadata": {},
   "source": [
    "# Instruction:\n",
    "Assume you are an excellent doctor. Using your knowledge, answer the quesion given below.\n",
    "\n",
    "# Question: What are the symptoms of Glaucoma ??\n",
    "\n",
    "# Answer:ains of Glaucoma can vary depending on the type and severity of the condition. The most common symptom of glaucoma is loss of vision, which may begin with a loss of peripheral (side) vision. This is often described as tunnel vision.  Glaucoma can also cause blurred vision, halos around lights, eye pain, redness, and vision loss.  Glaucoma is often called the \"silent thief of sight\" because it usually has no symptoms until significant vision loss has occurred.  If you have glaucoma, you may not notice any changes in your vision until the damage is severe.  Glaucoma is a progressive disease, which means it gets worse over time.  If you have glaucoma, it's important to have regular eye exams to monitor your vision and eye pressure.  If you have any of the following symptoms, you should see\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867378a-e7b3-4674-ba2a-0adccf82ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"example_2\": generated_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf3492-8ff3-4ad8-93a7-7dc58381f852",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726ad10-f44e-4381-92ea-1705858dab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(base_model, \"# The relationship between diabetes and blood pressure\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47b38672-f3a9-481e-a6e3-b5fdb059ff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_generate(model, tokenizer, text, max_new_tokens=300):\n",
    "    model.eval()\n",
    "    sample = tokenizer(\n",
    "        text + tokenizer.eos_token,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    ).to(model.device)\n",
    "\n",
    "    input_ids = sample[\"input_ids\"]\n",
    "    generated = input_ids.clone()\n",
    "    past_key_values = None\n",
    "    position_ids = torch.arange(0, input_ids.shape[1], device=model.device).unsqueeze(0)\n",
    "\n",
    "    prev_decoded = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "    for i in range(max_new_tokens):\n",
    "        if i == 0:\n",
    "            input_token = input_ids\n",
    "        else:\n",
    "            input_token = next_token_id\n",
    "            position_ids = torch.tensor([[generated.shape[1] - 1]], device=model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_token,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "                position_ids=position_ids\n",
    "            )\n",
    "\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        next_token_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        generated = torch.cat((generated, next_token_id), dim=1)\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # Decode full sequence and compute the diff\n",
    "        decoded = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        new_text = decoded[len(prev_decoded):]\n",
    "        prev_decoded = decoded\n",
    "\n",
    "        yield new_text\n",
    "\n",
    "        if next_token_id.squeeze().item() == tokenizer.eos_token_id:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c46300-d4ee-41f4-9690-ffb0b7d0d086",
   "metadata": {},
   "source": [
    "\"What are the symptoms of Glaucoma ??\"\n",
    "2. \"Explain Management of advanced and extragonadal germ-cell tumors.\"\n",
    "3. \"what is the role of hns in the virulence phenotype of pathogenic salmonellae?\"\n",
    "4. \"what are drinking patterns of high-risk drivers?\"\n",
    "5. \"My sister is on Xanax, feyntnol patch and a pain medicine for cancer.  She has been on 25 of fentynol and within 6 days she has been bumped up to 100 now she is almost lethargic and breathing is really labored and right arm is twitching.. She was carrying on conversation Sunday and Monday patch was put on Tuesday and now cant even sit up..no one seems worried but me.. Just wondering what I could do\"\n",
    "6. \"I was playing basketball the other night and went up to block a shot and flipped over the guy and landed on my side/back. Since then the lower left side of back/side have been sore, hurts when I take deep breaths and when I lay on my back, any chance of a bruised kidney or any serious injury I could have?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e5e997f-875b-4518-b4d0-5da482dd35e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The urinary tract is a system of organs that make up the body's waste disposal system. It includes the kidneys, ureters, bladder, and urethra. The kidneys filter waste from the blood and produce urine. Urine flows from the kidneys through the ureters to the bladder. The bladder stores urine until it is time to urinate. Urine leaves the body through the urethra.    Urinary tract infections (UTIs) are infections of the urinary tract. They are caused by bacteria. UTIs are more common in women than in men.    The signs and symptoms of a UTI include       - Pain or burning when urinating    - Frequent urination    - Urgent need to urinate    - Cloudy urine    - Strong-smelling urine    - Fever    - Chills    - Nausea    - Vomiting    - Pain in the lower abdomen    - Blood in the urine    - Urine that is dark or red    - Urine that is cloudy    - Urine that smells bad    - Urine that is foamy    - Urine that is pink or red    - Urine that is brown    - Urine that is yellow    - Urine that is clear    - Urine that is green"
     ]
    }
   ],
   "source": [
    "for token in stream_generate(model, tokenizer, prompt_template.format(question=\"What is (are) Urinary Tract Infections ?\")):\n",
    "    print(token, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51c50739-9375-48b4-b882-d90e32d26f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"What are the symptoms of Glaucoma ??\",\n",
    "    \"My sister is on Xanax, feyntnol patch and a pain medicine for cancer.  She has been on 25 of fentynol and within 6 days she has been bumped up to 100 now she is almost lethargic and breathing is really labored and right arm is twitching.. She was carrying on conversation Sunday and Monday patch was put on Tuesday and now cant even sit up..no one seems worried but me.. Just wondering what I could do\",\n",
    "    \"I was playing basketball the other night and went up to block a shot and flipped over the guy and landed on my side/back. Since then the lower left side of back/side have been sore, hurts when I take deep breaths and when I lay on my back, any chance of a bruised kidney or any serious injury I could have?\",\n",
    "    \"What are the treatments for High Blood Pressure ?\",\n",
    "    \"What is (are) Urinary Tract Infections ?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9063faa6-b569-4dfc-ba82-27f879bd23a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the symptoms of Glaucoma ?? \n",
      "  The symptoms of glaucoma are usually painless and may not be noticed until the disease has progressed. The first sign of glaucoma is often a loss of peripheral (side) vision. This is often described as tunnel vision. As glaucoma progresses, the blind spot in the visual field gets larger and the field of vision narrows. Eventually, the blind spot may reach the center of vision. This is called tunnel vision. If glaucoma is not treated, it can lead to blindness.\n",
      "My sister is on Xanax, feyntnol patch and a pain medicine for cancer.  She has been on 25 of fentynol and within 6 days she has been bumped up to 100 now she is almost lethargic and breathing is really labored and right arm is twitching.. She was carrying on conversation Sunday and Monday patch was put on Tuesday and now cant even sit up..no one seems worried but me.. Just wondering what I could do \n",
      "  The symptoms of your sister are suggestive of a possible stroke. She needs to be evaluated by a neurologist as soon as possible. The symptoms of a stroke are sudden onset of weakness or numbness of the face, arm or leg, especially on one side of the body, sudden onset of confusion, difficulty speaking, difficulty seeing in one or both eyes, sudden onset of severe headache, difficulty walking, dizziness, loss of balance or coordination, and sudden onset of severe dizziness or vertigo. If she is having any of these symptoms, she needs to be evaluated by a neurologist as soon as possible.\n",
      "I was playing basketball the other night and went up to block a shot and flipped over the guy and landed on my side/back. Since then the lower left side of back/side have been sore, hurts when I take deep breaths and when I lay on my back, any chance of a bruised kidney or any serious injury I could have? \n",
      "  The pain you are experiencing is most likely due to a muscle strain. This is a common injury that occurs when a muscle is overstretched or torn. It is important to rest the muscle and apply ice to the area to reduce swelling. You can also take over-the-counter pain medications such as ibuprofen or acetaminophen. If the pain does not subside, you should see a doctor to rule out a more serious injury.\n",
      "What are the treatments for High Blood Pressure ? \n",
      "  The American Heart Association recommends that people with high blood pressure should limit their sodium intake to 1,500 mg per day. The average American consumes about 3,400 mg of sodium per day. The American Heart Association recommends that people with high blood pressure should limit their sodium intake to 1,500 mg per day. The average American consumes about 3,400 mg of sodium per day. The American Heart Association recommends that people with high blood pressure should limit their sodium intake to 1,500 mg per day. The average American consumes about 3,400 mg of sodium per day. The American Heart Association recommends that people with high blood pressure should limit their sodium intake to 1,500 mg per day. The average American consumes about 3,400 mg of sodium per day. The American Heart Association recommends that people with high blood pressure should limit their sodium intake to 1,500 mg per day. The average American consumes about 3,400 mg of sodium per day. The American Heart Association recommends that people with high blood pressure should limit their sodium intake to 1,500 mg per day. The average American consumes about 3,400 mg\n",
      "What is (are) Urinary Tract Infections ? \n",
      "  The urinary tract is a system of organs that make up the body's waste disposal system. It includes the kidneys, ureters, bladder, and urethra. The kidneys filter waste from the blood and produce urine. Urine flows from the kidneys through the ureters to the bladder. The bladder stores urine until it is time to urinate. Urine leaves the body through the urethra.    Urinary tract infections (UTIs) are infections of the urinary tract. They are caused by bacteria. UTIs are more common in women than in men.    The signs and symptoms of a UTI include       - Pain or burning when urinating    - Frequent urination    - Urgent need to urinate    - Cloudy urine    - Strong-smelling urine    - Fever    - Chills    - Nausea    - Vomiting    - Pain in the lower abdomen    - Blood in the urine    - Urine that is dark or red    - Urine that is cloudy    - Urine that smells bad    - Urine that is foamy    - Urine that is pink or red    - Urine that is brown    - Urine that is yellow    - Urine that is clear    - Urine that is green\n"
     ]
    }
   ],
   "source": [
    "wandb_table = wandb.Table(columns=[\"Question\", \"Generated answer\"])\n",
    "\n",
    "for example in examples:\n",
    "    generated_answer = \"\"\n",
    "    for token in stream_generate(model, tokenizer, prompt_template.format(question=example)):\n",
    "        generated_answer += token\n",
    "    wandb_table.add_data(example, generated_answer)\n",
    "    print(example, \"\\n\", generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c63b455a-4561-4c4a-aa26-95a3f8d8c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"generated_examples\": wandb_table})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
