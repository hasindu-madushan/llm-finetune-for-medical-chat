{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4643151f-8b64-40cb-9b36-758e369e6407",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd5ae7-e718-4db9-9b4a-ff81be0e2457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install absl-py rouge-score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034bab24-dc8a-4b83-a570-29fcf3bdeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc9df7-5328-4945-a1bd-30f823fc8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import wandb\n",
    "import evaluate  # Hugging Face's evaluate library\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from utils import tokenize_dataset_for_qna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458060f-cfcf-4f51-a6aa-506cef1dc7c1",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb057781-55b7-494f-bcb4-dd7d00ba4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/phi_qna_finetuned_attempt_1/final\"\n",
    "\n",
    "data_path = \"../data/qna/\"\n",
    "test_data_path = data_path + \"test.csv\"\n",
    "\n",
    "model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "max_len = 512\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a3746-3c13-42cc-920a-2614eeede918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"qna_finetune-evaluation\", name=\"attempt_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f573ca-73a8-4ef6-af46-ecd2d84e6bad",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8443f97-858d-4220-b3d2-8f728fa48020",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac602a77-0a3a-4d89-915a-b102625ebce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_data_path)\n",
    "test_set = tokenize_dataset_for_qna(tokenizer, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60d571-b664-464b-a3b5-7ba96d0e8e22",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e73e1b6-b9b7-47b2-ab53-25f21c24d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = PeftConfig.from_pretrained(model_path)\n",
    "base_model_name = peft_config.base_model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da75b58-9eeb-4020-936f-f1cf43d25c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb96b6-5bd7-44d1-bd84-c6a107afddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Quantized model loading ===\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b63bf7-fe1b-401f-b8c1-a394a65ed24d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, model_path)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0fd68d-392c-49c4-bc85-81c05bb4a07b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded167b3-4fc9-439c-8c3c-238d193c5a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a3294-c17a-403d-9fe7-626608f6f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./eval_output_base\",\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     do_eval=True,\n",
    "#     report_to=\"none\"\n",
    "# )\n",
    "\n",
    "# base_model_trainer = Trainer(\n",
    "#     model=base_model,\n",
    "#     args=training_args,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "# )\n",
    "\n",
    "# base_model_eval_result = base_model_trainer.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6480b4-2485-422f-a289-898f0ba4dbe9",
   "metadata": {},
   "source": [
    "# Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f852223-0fa4-49c8-9f81-e600cef32e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./eval_output\",\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",\n",
    "    eval_accumulation_steps=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93a44e-cf37-4dc0-8171-3bde13eaca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load metrics\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\" Calculate bleu and rouge scores \"\"\"\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_preds\n",
    "        \n",
    "        logits = logits.cpu().numpy() if torch.is_tensor(logits) else logits\n",
    "        \n",
    "        labels = labels.cpu().numpy() if torch.is_tensor(labels) else labels\n",
    "        \n",
    "        # Get predicted token IDs (argmax of logits)\n",
    "        pred_ids = np.argmax(logits, axis=-1)  # Shape: (batch_size, seq_length)\n",
    "        \n",
    "        # Decode predictions and labels\n",
    "        pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Replace -100 with pad_token_id in labels\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        references = [[ref] for ref in label_str]\n",
    "\n",
    "        \n",
    "        # Compute BLEU\n",
    "        bleu_result = bleu_metric.compute(\n",
    "            predictions=pred_str,\n",
    "            references=references\n",
    "        )\n",
    "        \n",
    "        # Compute ROUGE\n",
    "        rouge_result = rouge_metric.compute(\n",
    "            predictions=pred_str,\n",
    "            references=label_str,\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        \n",
    "        # Extract main scores\n",
    "        metrics = {\n",
    "            'bleu': bleu_result['bleu'],\n",
    "            'rouge1': rouge_result['rouge1'],\n",
    "            'rouge2': rouge_result['rouge2'],\n",
    "            'rougeL': rouge_result['rougeL'],\n",
    "        }\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Then when you evaluate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d3989f-7cbb-47aa-8197-c2dab6692fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluate perplexity ===\n",
    "eval_result = trainer.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcdbf5-5d13-49ac-b3a3-86e2986139ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all results\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Loss: {eval_result['eval_loss']:.4f}\")\n",
    "print(f\"Perplexity: {torch.exp(torch.tensor(eval_result['eval_loss'])):.2f}\")\n",
    "print(f\"BLEU: {eval_result['eval_bleu']:.4f}\")\n",
    "print(f\"ROUGE-1: {eval_result['eval_rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {eval_result['eval_rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {eval_result['eval_rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be58f3-423a-42b3-9bc4-ae8a46dc9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\n",
    "    \"eval_loss\": eval_result['eval_loss'], \n",
    "    \"perplexity\": torch.exp(torch.tensor(eval_result['eval_loss'])),\n",
    "    \"BLUE\": eval_result['eval_bleu'],\n",
    "    \"ROUGE_1\": eval_result['eval_rouge1'],\n",
    "    \"ROUGE_2\": eval_result['eval_rouge2'],\n",
    "    \"ROUGE_L\": eval_result['eval_rougeL']    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b6324-721a-40fd-94e8-260ae1f37420",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74594108-635c-4a63-ac8f-074d13a890cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = test_set.select(range(433, 439))  # First 5 examples\n",
    "input_ids = torch.tensor(samples[\"input_ids\"]).to(model.device)\n",
    "attention_mask = torch.tensor(samples[\"attention_mask\"]).to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "# Log predictions to W&B\n",
    "wandb_table = wandb.Table(columns=[\"Title\", \"Actual Abstract\", \"Generated Text\"])\n",
    "for i, gen in enumerate(generated_texts):\n",
    "    title = samples[i][\"title\"]\n",
    "    actual = samples[i][\"abstract\"]\n",
    "    print(f\"\\nActual: {title}\\n{actual}\\n---\\nGenerated: {gen}\\n\")\n",
    "    wandb_table.add_data(title, actual, gen)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035ed31-1dea-4631-8d66-797ce87c0a74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f60d22-2210-4463-bee0-814c84f988dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"generated_examples\": wandb_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14185bf3-943b-4a05-b9e0-649841a4723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, text, max_new_tokens=128):\n",
    "    sample = tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_len, return_attention_mask=True)\n",
    "    input_ids = torch.tensor([sample[\"input_ids\"]]).to(model.device)\n",
    "    attention_mask = torch.tensor([sample[\"attention_mask\"]]).to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        use_cache=False\n",
    "    )\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return generated_texts[0]\n",
    "    # # Log predictions to W&B\n",
    "    # for i, gen in enumerate(generated_texts):\n",
    "    #     title = samples[i][\"title\"]\n",
    "    #     actual = samples[i][\"abstract\"]\n",
    "    #     print(f\"\\nTitle: {title}\\n---\\nActual Abstract: {actual}\\n---\\nGenerated: {gen}\\n\")\n",
    "    #     wandb_table.add_data(title, actual, gen)\n",
    "    \n",
    "    \n",
    "    # wandb.log({\"generated_examples\": wandb_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f748a1-4da5-439e-911d-0c76f22bbedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate(model, \"The relationship between diabetes and blood pressure\\n\")\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867378a-e7b3-4674-ba2a-0adccf82ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"example_1\": generated_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726ad10-f44e-4381-92ea-1705858dab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(base_model, \"# The relationship between diabetes and blood pressure\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
