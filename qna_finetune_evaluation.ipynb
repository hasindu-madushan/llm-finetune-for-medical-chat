{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4643151f-8b64-40cb-9b36-758e369e6407",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedd5ae7-e718-4db9-9b4a-ff81be0e2457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install absl-py rouge-score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034bab24-dc8a-4b83-a570-29fcf3bdeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd47c21-5017-47b8-ac58-df9338ef163a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20bc9df7-5328-4945-a1bd-30f823fc8b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super_admin/hasindu/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import wandb\n",
    "import evaluate  # Hugging Face's evaluate library\n",
    "import numpy as np\n",
    "import torch\n",
    "from bert_score import BERTScorer, score as bert_score\n",
    "\n",
    "from utils import tokenize_dataset_for_qna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458060f-cfcf-4f51-a6aa-506cef1dc7c1",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb057781-55b7-494f-bcb4-dd7d00ba4ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/phi_qna_finetuned_attempt_3/final\"\n",
    "\n",
    "data_path = \"../data/qna/\"\n",
    "test_data_path = data_path + \"test.csv\"\n",
    "\n",
    "model_id = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "base_model_path = \"../models/phi_pubmed_pretrained_attempt_3/final_pretrained\"\n",
    "\n",
    "max_len = 512\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a3746-3c13-42cc-920a-2614eeede918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"qna_finetune-evaluation\", name=\"attempt_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f573ca-73a8-4ef6-af46-ecd2d84e6bad",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a15035ec-6729-4cf4-a946-1b1f2a700e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Instruction:\n",
      "Assume you are an excellent doctor. Using your knowledge, answer the question given below.\n",
      "\n",
      "# Question: {question}\n",
      "\n",
      "# Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "# Instruction:\n",
    "Assume you are an excellent doctor. Using your knowledge, answer the question given below.\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Answer: \"\"\"\n",
    "prompt_template = prompt_template.strip()\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8443f97-858d-4220-b3d2-8f728fa48020",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030d192a-2157-43c3-8529-3f9fd4663faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "\n",
    "def tokenize_dataset_for_qna(tokenizer, data_df, prompt_template, max_len):\n",
    "    dataset = Dataset.from_pandas(data_df)\n",
    "    dataset = dataset.map(lambda sample: tokenize_for_qna(sample, tokenizer, prompt_template, max_len), batched=False)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def tokenize_for_qna(example, tokenizer, prompt_template, max_len):\n",
    "    prompt = prompt_template.format(question=example['question'])\n",
    "    answer = example[\"answer\"] + tokenizer.eos_token\n",
    "    full_text = prompt + answer\n",
    "\n",
    "    # Tokenize answer to get the length of answer tokens\n",
    "    prompt_len = len(tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    )[\"input_ids\"])\n",
    "\n",
    "    # Tokenize answer to get the length of answer tokens\n",
    "    full_len = len(tokenizer(\n",
    "        full_text,\n",
    "        truncation=False\n",
    "    )[\"input_ids\"])\n",
    "    \n",
    "    # Tokenize full sequence once\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True    \n",
    "    )\n",
    "    \n",
    "    # Convert to numpy arrays for faster operations\n",
    "    input_ids = np.array(tokenized[\"input_ids\"])\n",
    "    attention_mask = np.array(tokenized[\"attention_mask\"])\n",
    "    \n",
    "    # Create labels array and mask prompt portion efficiently\n",
    "    labels = input_ids.copy()\n",
    "    padding_len = max_len - full_len\n",
    "    # Mask the prompt tokens\n",
    "    labels[padding_len:padding_len + prompt_len] = -100\n",
    "    \n",
    "    # Update the tokenized dict with numpy arrays\n",
    "    tokenized[\"input_ids\"] = input_ids.tolist()\n",
    "    tokenized[\"attention_mask\"] = attention_mask.tolist()\n",
    "    tokenized[\"labels\"] = labels.tolist()\n",
    "    \n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac602a77-0a3a-4d89-915a-b102625ebce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_data_path)\n",
    "test_set = tokenize_dataset_for_qna(tokenizer, test_df.iloc[:1500, :], prompt_template, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a851ebd-aff1-4e44-ad09-93151119bcb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60d571-b664-464b-a3b5-7ba96d0e8e22",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e73e1b6-b9b7-47b2-ab53-25f21c24d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = PeftConfig.from_pretrained(model_path)\n",
    "base_model_name = peft_config.base_model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da75b58-9eeb-4020-936f-f1cf43d25c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65cb96b6-5bd7-44d1-bd84-c6a107afddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# === Quantized model loading ===\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b63bf7-fe1b-401f-b8c1-a394a65ed24d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3Attention(\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=24, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=24, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "              (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "              (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "              (activation_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm()\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_attention_layernorm): Phi3RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(model, model_path)\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0fd68d-392c-49c4-bc85-81c05bb4a07b",
   "metadata": {},
   "source": [
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ce6b4-018f-422f-a85e-285f9038dbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb1b39ed-ea6c-426c-a76c-63ac9283a909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'microsoft/Phi-3.5-mini-instruct'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ded167b3-4fc9-439c-8c3c-238d193c5a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Phi3ForCausalLM(\n",
       "  (model): Phi3Model(\n",
       "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x Phi3DecoderLayer(\n",
       "        (self_attn): Phi3Attention(\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Phi3MLP(\n",
       "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (activation_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Phi3RMSNorm()\n",
       "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_attention_layernorm): Phi3RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Phi3RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a3294-c17a-403d-9fe7-626608f6f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./eval_output_base\",\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     do_eval=True,\n",
    "#     report_to=\"none\"\n",
    "# )\n",
    "\n",
    "# base_model_trainer = Trainer(\n",
    "#     model=base_model,\n",
    "#     args=training_args,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "# )\n",
    "\n",
    "# base_model_eval_result = base_model_trainer.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6480b4-2485-422f-a289-898f0ba4dbe9",
   "metadata": {},
   "source": [
    "# Test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f852223-0fa4-49c8-9f81-e600cef32e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./eval_output\",\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",\n",
    "    eval_accumulation_steps=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93a44e-cf37-4dc0-8171-3bde13eaca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize metrics ONCE (reuse them)\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bert_scorer = BERTScorer(\n",
    "    lang=\"en\",\n",
    "    model_type=\"bert-base-uncased\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    idf=False,  # Disable IDF to save memory\n",
    "    rescale_with_baseline=True  # Better score normalization\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\" Metric computation \"\"\"\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_preds\n",
    "        \n",
    "        # Convert to numpy (move to CPU first if needed)\n",
    "        if torch.is_tensor(logits):\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "        if torch.is_tensor(labels):\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "        \n",
    "        # Get predicted tokens (shape: [batch_size, seq_length])\n",
    "        pred_ids = np.argmax(logits, axis=-1)\n",
    "        \n",
    "        # Decode in batches to avoid memory spikes\n",
    "        batch_size = 8  # Adjust based on your GPU memory\n",
    "        pred_str, label_str = [], []\n",
    "        \n",
    "        for i in range(0, len(pred_ids), batch_size):\n",
    "            # Decode predictions\n",
    "            batch_preds = pred_ids[i:i+batch_size]\n",
    "            pred_str.extend(tokenizer.batch_decode(\n",
    "                batch_preds, \n",
    "                skip_special_tokens=True\n",
    "            ))\n",
    "            \n",
    "            # Decode labels (replace -100 with pad_token_id)\n",
    "            batch_labels = labels[i:i+batch_size]\n",
    "            batch_labels = np.where(\n",
    "                batch_labels != -100, \n",
    "                batch_labels, \n",
    "                tokenizer.pad_token_id\n",
    "            )\n",
    "            label_str.extend(tokenizer.batch_decode(\n",
    "                batch_labels, \n",
    "                skip_special_tokens=True\n",
    "            ))\n",
    "        \n",
    "        # Skip if empty (avoid errors)\n",
    "        if not pred_str or not label_str:\n",
    "            return {\n",
    "                'bleu': 0.0,\n",
    "                'rouge1': 0.0,\n",
    "                'rouge2': 0.0,\n",
    "                'rougeL': 0.0,\n",
    "                'bertscore_f1': 0.0\n",
    "            }\n",
    "        \n",
    "        # Compute BLEU (handle edge cases)\n",
    "        try:\n",
    "            bleu_score = bleu_metric.compute(\n",
    "                predictions=pred_str,\n",
    "                references=[[ref] for ref in label_str]\n",
    "            )['bleu']\n",
    "        except:\n",
    "            bleu_score = 0.0\n",
    "        \n",
    "        # Compute ROUGE\n",
    "        rouge_scores = rouge_metric.compute(\n",
    "            predictions=pred_str,\n",
    "            references=label_str,\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        \n",
    "        # Compute BERTScore in batches\n",
    "        P, R, F1 = bert_scorer.score(\n",
    "            pred_str, \n",
    "            label_str,\n",
    "            batch_size=4  # Small batch for BERTScore\n",
    "        )\n",
    "        \n",
    "        metrics = {\n",
    "            'bleu': bleu_score,\n",
    "            'rouge1': rouge_scores['rouge1'],\n",
    "            'rouge2': rouge_scores['rouge2'],\n",
    "            'rougeL': rouge_scores['rougeL'],\n",
    "            'bertscore_precision': P.mean().item(),\n",
    "            'bertscore_recall': R.mean().item(),\n",
    "            'bertscore_f1': F1.mean().item(),\n",
    "        }\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        return metrics\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer, padding=False),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d3989f-7cbb-47aa-8197-c2dab6692fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluate perplexity ===\n",
    "eval_result = trainer.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddcdbf5-5d13-49ac-b3a3-86e2986139ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print all results\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(f\"Loss: {eval_result['eval_loss']:.4f}\")\n",
    "print(f\"Perplexity: {torch.exp(torch.tensor(eval_result['eval_loss'])):.2f}\")\n",
    "print(f\"BLEU: {eval_result['eval_bleu']:.4f}\")\n",
    "print(f\"ROUGE-1: {eval_result['eval_rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {eval_result['eval_rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {eval_result['eval_rougeL']:.4f}\")\n",
    "print(f\"BERTscore precision: {eval_result['eval_bertscore_precision']:.4f}\")\n",
    "print(f\"BERTscore recall: {eval_result['eval_bertscore_recall']:.4f}\")\n",
    "print(f\"BERTscore f1: {eval_result['eval_bertscore_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be58f3-423a-42b3-9bc4-ae8a46dc9351",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\n",
    "    \"eval_loss\": eval_result['eval_loss'], \n",
    "    \"perplexity\": torch.exp(torch.tensor(eval_result['eval_loss'])),\n",
    "    \"BLUE\": eval_result['eval_bleu'],\n",
    "    \"ROUGE_1\": eval_result['eval_rouge1'],\n",
    "    \"ROUGE_2\": eval_result['eval_rouge2'],\n",
    "    \"ROUGE_L\": eval_result['eval_rougeL'],\n",
    "    \"BERTscore_precision\": eval_result['eval_bertscore_precision'],\n",
    "    \"BERTscore recall\": eval_result['eval_bertscore_recall'],\n",
    "    \"BERTscore f1\": eval_result['eval_bertscore_f1']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b6324-721a-40fd-94e8-260ae1f37420",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74594108-635c-4a63-ac8f-074d13a890cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = test_set.select(range(433, 439))  # First 5 examples\n",
    "input_ids = torch.tensor(samples[\"input_ids\"]).to(model.device)\n",
    "attention_mask = torch.tensor(samples[\"attention_mask\"]).to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "# Log predictions to W&B\n",
    "wandb_table = wandb.Table(columns=[\"Title\", \"Actual Abstract\", \"Generated Text\"])\n",
    "for i, gen in enumerate(generated_texts):\n",
    "    title = samples[i][\"title\"]\n",
    "    actual = samples[i][\"abstract\"]\n",
    "    print(f\"\\nActual: {title}\\n{actual}\\n---\\nGenerated: {gen}\\n\")\n",
    "    wandb_table.add_data(title, actual, gen)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035ed31-1dea-4631-8d66-797ce87c0a74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f60d22-2210-4463-bee0-814c84f988dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"generated_examples\": wandb_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14185bf3-943b-4a05-b9e0-649841a4723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, text, max_new_tokens=128):\n",
    "    sample = tokenizer(text + tokenizer.eos_token, truncation=True, padding=\"max_length\", max_length=max_len, return_attention_mask=True)\n",
    "    input_ids = torch.tensor([sample[\"input_ids\"]]).to(model.device)\n",
    "    attention_mask = torch.tensor([sample[\"attention_mask\"]]).to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "        use_cache=False\n",
    "    )\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return generated_texts[0]\n",
    "    # # Log predictions to W&B\n",
    "    # for i, gen in enumerate(generated_texts):\n",
    "    #     title = samples[i][\"title\"]\n",
    "    #     actual = samples[i][\"abstract\"]\n",
    "    #     print(f\"\\nTitle: {title}\\n---\\nActual Abstract: {actual}\\n---\\nGenerated: {gen}\\n\")\n",
    "    #     wandb_table.add_data(title, actual, gen)\n",
    "    \n",
    "    \n",
    "    # wandb.log({\"generated_examples\": wandb_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c1b43f-b1c8-4f27-80ec-76cd90ce3be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f748a1-4da5-439e-911d-0c76f22bbedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate(model, prompt_template.format(question=\"What are the symptoms of Glaucoma ??\"), max_new_tokens=200)\t\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23655995-0192-4149-9a70-6bfd50eada85",
   "metadata": {},
   "source": [
    "# Instruction:\n",
    "Assume you are an excellent doctor. Using your knowledge, answer the quesion given below.\n",
    "\n",
    "# Question: What are the symptoms of Glaucoma ??\n",
    "\n",
    "# Answer:ains of Glaucoma can vary depending on the type and severity of the condition. The most common symptom of glaucoma is loss of vision, which may begin with a loss of peripheral (side) vision. This is often described as tunnel vision.  Glaucoma can also cause blurred vision, halos around lights, eye pain, redness, and vision loss.  Glaucoma is often called the \"silent thief of sight\" because it usually has no symptoms until significant vision loss has occurred.  If you have glaucoma, you may not notice any changes in your vision until the damage is severe.  Glaucoma is a progressive disease, which means it gets worse over time.  If you have glaucoma, it's important to have regular eye exams to monitor your vision and eye pressure.  If you have any of the following symptoms, you should see\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e867378a-e7b3-4674-ba2a-0adccf82ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"example_2\": generated_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf3492-8ff3-4ad8-93a7-7dc58381f852",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726ad10-f44e-4381-92ea-1705858dab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(base_model, \"# The relationship between diabetes and blood pressure\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47b38672-f3a9-481e-a6e3-b5fdb059ff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_generate(model, tokenizer, text, max_new_tokens=128):\n",
    "    model.eval()\n",
    "    sample = tokenizer(\n",
    "        text + tokenizer.eos_token,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_len\n",
    "    ).to(model.device)\n",
    "\n",
    "    input_ids = sample[\"input_ids\"]\n",
    "    generated = input_ids.clone()\n",
    "    past_key_values = None\n",
    "    position_ids = torch.arange(0, input_ids.shape[1], device=model.device).unsqueeze(0)\n",
    "\n",
    "    prev_decoded = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "    for i in range(max_new_tokens):\n",
    "        if i == 0:\n",
    "            input_token = input_ids\n",
    "        else:\n",
    "            input_token = next_token_id\n",
    "            position_ids = torch.tensor([[generated.shape[1] - 1]], device=model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=input_token,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "                position_ids=position_ids\n",
    "            )\n",
    "\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        next_token_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        generated = torch.cat((generated, next_token_id), dim=1)\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        # Decode full sequence and compute the diff\n",
    "        decoded = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        new_text = decoded[len(prev_decoded):]\n",
    "        prev_decoded = decoded\n",
    "\n",
    "        yield new_text\n",
    "\n",
    "        if next_token_id.squeeze().item() == tokenizer.eos_token_id:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e5e997f-875b-4518-b4d0-5da482dd35e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What is the value of x in the equation 3x + 5 = 14? Please show your work.\n",
      "\n",
      "# Answer\n",
      "To find the value of x in the equation 3x + 5 = 14, follow these steps:\n",
      "\n",
      "1. Subtract 5 from both sides of the equation to isolate the term with x on one side:\n",
      "   3x + 5 - 5 = 14 - 5\n",
      "   3x = 9\n",
      "\n",
      "2. Divide both sides of the equation by 3 to solve for x:\n",
      "   3x /"
     ]
    }
   ],
   "source": [
    "for token in stream_generate(model, tokenizer, prompt_template.format(question=\"What are the symptoms of Glaucoma ??\")):\n",
    "    print(token, end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
